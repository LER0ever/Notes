<!doctype html><head lang=en><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Computer Science - L.E.R Wiz</title><meta name=format-detection content="telephone=no"><script>if(self!=top){top.location=this.location.href;}</script><meta name=keywords content="LER0ever,L.E.R,Yi Rong,blog,L.E.R Space,Lumos,Wiz"><meta name=description content="L.E.R Space, a magical personal website by Yi Rong"><link rel=icon href=/favicon.ico><link rel="shortcut icon" href=/favicon.ico><link rel=stylesheet id=font-style-css href=/css/fonts-google.css><link rel=stylesheet id=fancybox-css href=/css/jquery.fancybox-1.3.4.css><link rel=stylesheet id=cr-style-css href=/css/style.css><link rel=stylesheet href=/css/hljs/monokai-sublime.css><script src=/js/jquery.min.js></script><script src=/js/jquery-migrate.min.js></script><script type=text/x-mathjax-config>
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script><script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script><body class="home post-tag"><div id=wrapper><header id=branding><div class="wrapper clearfix"><div class="hgroup-wrap clearfix"><div class=hgroup-right><div class="social-profiles clearfix"><ul></ul></div><form class="searchform clearfix"><label class=assistive-text for=s>Search</label>
<input id=searchbox placeholder=Search class="s field" name=s></form></div><div id=site-logo class=clearfix><h3 id=site-title><a href=https://notes.rongyi.blog title="L.E.R Wiz" rel=home>L.E.R Wiz</a></h3><h4 id=site-description>All Ways Forward @ UW-Madison</h4></div></div><nav id=access class=clearfix><ul class=root><li class="menu-item menu-item-type-post_type menu-item-object-page"><a href=/>Home</a><li class="menu-item menu-item-type-post_type menu-item-object-page"><a href=/academics>My Academics</a><li class="menu-item menu-item-type-post_type menu-item-object-page"><a href=/sn>Contact Me</a><li class="menu-item menu-item-type-post_type menu-item-object-page"><a href=/archive>All Notes</a><li class="menu-item menu-item-type-post_type menu-item-object-page"><a href=https://rongyi.blog/about target=_blank>L.E.R Space</a><li class=default-menu><a href=https://rongyi.blog title=Navigation>Navigation</a></ul></nav></div></header><div id=main class="site-main wrapper clearfix"><div class=container><div id=primary class=no-margin-left><div id=content><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540 Final Review</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/12/15/cs540-fr.html title="December 15, 2017">December 15, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p>This article is a copy-paste from <a href=https://github.com/vivin/cse598/tree/master/final>https://github.com/vivin/cse598/tree/master/final</a><h1 id=agents>Agents</h1><ul><li>Agent perceives its <strong>environment</strong> therough <strong>sensors</strong> and acts upon that environment through <strong>actuators</strong>.<li>A <strong>percept</strong> refers to the agent&rsquo;s perceptual inputs at any given instant.<li>An agent&rsquo;s <strong>percept sequence</strong> is the complete history of everything the agent has ever perceived.<li>A <strong>performance measure</strong> evaluates any given sequence of environment states. It captures the notion of desirability.</ul><h2 id=rationality>Rationality</h2><p>What is rational at any given time depends on four things:<ul><li>The performance measure that defines the criterion of success.<li>The agent&rsquo;s prior knowledge of the environment.<li>The actions that the agent can perform.<li>The agent&rsquo;s percept sequence to date.</ul><p>Hence, the definition of a <strong>rational agent</strong>: <em>For each possible percept sequence, a rational agent should select an action that is expected to maximize its performance measure, given the evidence provided by the percept sequence and whatever built-in knowledge the agent has.</em><p>There is a distinction between <strong>omniscience</strong> and <strong>rationality</strong>. An <strong>omniscient</strong> agent knows the actual outcome of its actions and can act accordingly; but omiscience is impossible in reality. Hence a <strong>rational</strong> agent is not necessarily <strong>omniscient</strong>. It can only act based on what it knows and what it has done so far.<p>A rational agent should also be <strong>autonomous</strong>. It should <strong>learn</strong> what it can to compensate for partial or incorrect prior-knowledge. It should also involve itself in <strong>information gathering</strong> and <strong>exploration</strong> with the aim to maximize performance and in order to modify future percepts.<p><em>As a general rule, it is better to design performance measures according to what one actually wants in the environment, rather than according to how one things the agent should behave</em>.<h2 id=peas>PEAS</h2><p><strong>P</strong>erformance, <strong>E</strong>nvironment, <strong>A</strong>ctuators, <strong>S</strong>ensors. This helps us specify the task environment. For example, consider a taxi-driver agent:<ul><li><strong>Performance Measure</strong>: Safe, fast, legal, comfortable trip, maximize profits.<li><strong>Environment</strong>: Roads, other traffic, pedestrians, customers.<li><strong>Actuators</strong>: Steering, accelerator, brake, signal, horn, display.<li><strong>Sensors</strong>: Cameras, sonar, speedometer, GPS, odometer, accelerometer, engine sensors, keyboard.</ul><h2 id=properties-of-task-environments>Properties of Task Environments</h2><ul><li><strong>Fully observable</strong> vs. <strong>partially observable</strong><li><strong>Single agent</strong> vs. <strong>multi-agent</strong><li><strong>Deterministic</strong> vs. <strong>stochastic</strong> (randomly determined; having random probability distribution).<li><strong>Episodic</strong> vs. <strong>sequential</strong> - In episodic, agent&rsquo;s experience is divided into atomic episoodes. The next episode does not depend on actions taken in previous episodes (e.g. NN classifier for hand-written digits). In sequential, current decision depends on previous actions and can affect future decisions.<li><strong>Static</strong> vs. <strong>dynamic</strong><li><strong>Discrete</strong> vs. <strong>continuous</strong><li><strong>Known</strong> vs. <strong>unknown</strong></ul><h2 id=turing-test>Turing Test</h2><p>Introduced by Alan Turing in 1950. It is a test of a machine&rsquo;s ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. A human judge engages in natural-language conversations with a human and a machine. The machine is designed to generate performance indistinguishable from that of a human being. Conversation is limited to a text-only channel. If the judge cannot reliably tell a machine from a human, the machine is said to have passed the test.<h1 id=uninformed-search>Uninformed Search</h1><p><strong>Problem formulation</strong> is the process of deciding what actions and states to consider, given a goal.<p>A <strong>problem</strong> can be defined formally by five components:<ul><li>The <strong>initial state</strong> that the agent starts in.<li>A description of the possible <strong>actions</strong> available to the agent. Given a particular state $s$, $ACTIONS(s)$ returns the set of actions that can be executed in $s$, i.e., each of these actions is <strong>applicable</strong> in $s$.<li>A <strong>transition model</strong>, which is a description of what each action does. This is specified by a function $RESULT(s, a)$ that returns the state that results from performing action $a$ in state $s$. The term <strong>successor</strong> is used to refer to any state reachable from a given state by a single action. Together, the initial state, actions, and transition model implicitly define the <strong>state space</strong> of the problem.<li>The <strong>goal test</strong>, which determines whether a given state is a goal state.</ul><p>In general, <em>an agent with several immediate options of unknown value can decide what to do by first examining</em> future <em>actions that eventually lead to states of known value.</em><h2 id=framing-a-search-problem>Framing a search problem</h2><p>There are <strong>six</strong> components: <strong>states</strong>, <strong>initial state</strong>, <strong>actions</strong>, <strong>transition model</strong>, <strong>goal test</strong>, and <strong>path cost</strong>. Provide a mathematical description of the <strong>transition model</strong>, and <strong>goal test</strong> if possible.<h2 id=infrastructure-for-search-algorithms>Infrastructure for search algorithms</h2><p>For each node $n$, we have a structure that contains four components:<ul><li>$n.STATE$: the state in the state space to which the node corresponds.<li>$n.PARENT$: the node in the search tree that generated this node.<li>$n.ACTION$: the action that was applied to the parent that generated this node (i.e., what action from the parent got me here?)<li>$n.PATH-COST$: the cost, traditionally denoted by $g(n)$, of the path from the initial state to the node, as indicated by the parent pointers. This would be the sum of the individual step costs in the path. The <strong>step cost</strong> of taking action $a$ in state $s$ to reach state $s&rsquo;$ is denoted by $c(s, a, s&rsquo;)$.</ul><h2 id=measuring-problem-solving-performance>Measuring problem-solving performance</h2><ul><li><strong>Completeness</strong>: Is the algorithm guaranteed to find a solution when there is one?<li><strong>Optimality</strong>: Does the strategy find the optimal solution? (<strong>optimal solution</strong>: the <strong>solution</strong> that has the lowest path-cost among all solutions).<li><strong>Time complexity</strong>: How long does it take to find a solution?<li><strong>Space complexity</strong>: How much memory is needed to perform the search?</ul><p>Complexity is represented in terms of three quantities:<ul><li>$b$: The <strong>branching factor</strong>, or maximum number of successors of any node.<li>$d$: The <strong>depth</strong> of the <em>shallowest</em> goal node.<li>$m$: The <strong>maximum length</strong> of any path in the state space.</ul><h2 id=comparing-uninformed-search-strategies>Comparing uninformed-search strategies</h2><table><thead><tr><th><strong>Criterion</strong><th><strong>BFS</strong><th><strong>Uniform Cost</strong><th><strong>DFS</strong><th><strong>Depth Limited</strong><th><strong>ID-DFS</strong><th><strong>Bidirectional</strong><tbody><tr><td>Complete?<td>$\text{Yes}^{a}$<td>$\text{Yes}^{a,b}$<td>No<td>No<td>$\text{Yes}^{a}$<td>$\text{Yes}^{a,d}$<tr><td>Time<td>$O(b^{d})$<td>$O\big(b^{1+\left\lfloor\frac{C^{*}}{\epsilon}\right\rfloor}\big)$<td>$O(b^{m})$<td>$O(b^{l})$<td>$O(b^{d})$<td>$O(b^{\frac{d}{2}})$<tr><td>Space<td>$O(b^{d})$<td>$O\big(b^{1+\left\lfloor\frac{C^{*}}{\epsilon}\right\rfloor}\big)$<td>$O(bm)$<td>$O(bl)$<td>$O(bd)$<td>$O(b^{\frac{d}{2}})$<tr><td>Optimal?<td>$\text{Yes}^{c}$<td>Yes<td>No<td>No<td>$\text{Yes}^{c}$<td>$\text{Yes}^{c,d}$</table><p>Evaluation of tree-search strategies. $b$ is the branching factor; $d$ is the depth of the shallowest solution; <em>m</em> is the maximum depth of the search tree; <em>l</em> is the depth limit. Superscript caveats are as follows: <sup>a</sup> complete if <em>b</em> is finite; <sup>b</sup> complete if step costs &gt;= ϵ for positive ϵ. <sup>c</sup> optimal if step costs are all identical; <sup>d</sup> if both directions use breadth-first search.<h2 id=bfs-breadth-first-search>BFS (Breadth-first search)</h2><p>A simple strategy in which root node is expanded first and then all successors of the root node are expanded next, then <em>their</em> successors, and so on. In general, all nodes are expanded at a given depth in the search tree, before any nodes at the next level are expanded. This means that the <em>shallowest</em> unexpanded node is chosen for expansion. To do this we use a FIFO queue (i.e., regular queue). <strong>The goal test is applied to each node when it is generated rather than when it is selected for expansion</strong>; this is because if we applied the test when it is selected for expansion, we would have to expand the whole layer of nodes at depth $d$ before the goal was detected, which makes the runtime complexity $O(b^{d + 1})$. The algorithm discards any new path to a state already in the frontier or explored set (any such state must be <em>at least as deep</em> as the one already found). Hence BFS always has the <em>shallowest</em> path to every node on the frontier.<ul><li><strong>Completeness</strong>: BFS is complete. If the shallowest goal-node is at some finite-depth $d$, BFS will eventually find it after generating all shallower-nodes (provided branching-factor $b$ is finite).<li><strong>Optimality</strong>: BFS is optimal, assuming that the path-cost is a non-decreasing function of the depth of the node. This is easily seen if all actions have the same cost.<li><strong>Time</strong>: We generate $b^h$ nodes at each level $h$. So the root generates $b$, and then each of those generate $b$, which leads to $b^2$ at the second level, and so on. Hence in general, we have $O(b^d)$.<li><strong>Space</strong>: We store every expanded node in the <em>explored</em> set. This means that every node remains in memory, which gives us $O(b^{d - 1})$ in the <em>explored</em> set. The <em>frontier</em> set then has $O(b^d)$ nodes. This means that the size of the <em>frontier</em> dominates, which gives us a space complexity of $O(b^d)$.</ul><p><strong>Memory requirements are a bigger problem for BFS than execution time.</strong> That is, we will face the issue of running out of memory, long before the face the issue of waiting way too long for a solution.<h2 id=uniform-cost-search>Uniform-cost search</h2><p>When all step costs are equal, BFS is optimal because it always expands the <em>shallowest</em> unexpanded node. How about an algorithm that is optimal with any step-cost function?<ul><li>Instead of expanding the shallowest node, <strong>uniform-cost search</strong> expands the node $n$ with the <strong>lowest path-cost</strong> $\mathbf{g(n)}$<li>This is done by storing the frontier <strong>as a priority queue</strong> ordered by $g$.<li>The goal test is applied to a node when it is <em>selected for expansion</em> rather than when it is first <em>generated</em> (i.e., opposite of BFS). This is because the first goal-node that is <em>generated</em> may not necessarily be on the most-optimal path.<li>Another test is also added to check for the case where a better path is found to a node currently on the frontier.</ul><p>An example:<pre><code>                  99
[Sibiu] -----------------------[Fagaras]
   \                               |
    \                              |
     \ 80                          |
      \                            |
 [Rimnicu Vilcea]                  |
        \                          |
         \                         |
          \                        |
           \ 97                    |
            \                      |
             \                     |
          [Pitesti]                |
               \                   |
                \                  |
                 \ 101             |
                  \                | 211
                   \               |
                    \              |
                     \             |
                      \            |
                       \           |
                        \          |
                         \         |
                          \        |
                           \       |
                            \      |
                             \     |
                              \    |
                               \   |
                                \  |
                                 \ |
                                  \|
                             [Bucharest]
</code></pre><p>The problem is to get from <strong>Sibiu</strong> to <strong>Bucharest</strong>. The successors of <strong>Sibiu</strong> are <strong>Riminicu Vilcea</strong> and <strong>Fagaras</strong> with path-costs $80$ and $99$ respectively. Since <strong>Riminicu Vilcea</strong> is the least-cost node, it is expanded next, which gives us <strong>Pitesti</strong> with a total path-cost of $80 + 97 = 177$. Now <strong>Fagaras</strong> is the least-cost node, and so it is expanded, giving us <strong>Bucharest</strong> with a cost of $99 + 211 = 310$. Now although <strong>Bucharest</strong> is the goal node, we keep going since we don&rsquo;t perform the goal test on <em>generated</em> nodes. So we will next choose <strong>Pitesti</strong> for expansion which adds a second path to <strong>Bucharest</strong> with the cost $80 + 97 + 101 = 278$. The algorithm now checks to see if this new path is better than the old one; it is and so the old one is discarded. <strong>Bucharest</strong> with $g$-cost $278$ is now selected for expansion and then returned as a solution (because we perform the goal test when a node is selected for expansion).<ul><li><strong>Optimality</strong>: Uniform-cost search is optimal in general. Whenever it selects a node $n$ for expansion, the optimal path to that node as been found. Why is this? Assume there exists another frontier node $n&rsquo;$ on the optimal path from the start node to $n$. By definition, $n&rsquo;$ would have a lower $g$-cost than $n$ and would have been selected first. Since step-costs are non-negative, paths will never get shorter as nodes are added.<li><strong>Completeness</strong>: Uniform-cost search is complete, assuming that the branching-factor $b$ is finite, and that the step costs are $\ge \epsilon$ where $\epsilon$ is some small positive-number. If the second assumption does not hold, it can get stuck in an infinite loop if there is a path with an infinite sequence of zero-cost actions (i.e., a sequence of <em>NoOp</em> actions). Completeness is therefore guaranteed provided the cost of every step exceeds some small positive-constant $\epsilon$.<li><strong>Time</strong> and <strong>Space</strong>: The algorithm is guided by path costs rather than depth and so the runtime and space complexity cannot really be expressed in terms of $b$ and $d$. Instead, let $C^*$ be the cost of the optimal solution, and assume that every action costs <em>at least</em> $\epsilon$. Then the worst-case time and space complexity is $O\big(b^{1+\left\lfloor\frac{C^*}{\epsilon}\right\rfloor}\big)$, which can be much greater than $b^d$. Here we&rsquo;re first looking at the cost per step, and then the branching-factor is raised to that power. We add $1$ because we apply the goal test when we <em>select nodes for expansion</em>. The reason we can have runtime and space complexity greater than $b^d$ is because the algorithm can explore large trees of small steps before exploring paths that involve large, and perhaps useful steps. When all step costs are equal, uniform-cost search is similar to BFS except that BFS stops as soon as it generates a goal whereas uniform-cost examines all nodes at the goal&rsquo;s depth to see if any have a lower cost. Hence, uniform-cost search does more work by expanding nodes at depth $d$ unnecessarily.</ul><h2 id=dfs>DFS</h2><p>This search algorithm always expands the <em>deepest</em> node in the current frontier of the search tree. The search goes to the deepest level of the search tree, where the nodes have no successors. As those nodes are expanded, they are removed from the frontier, so then the search &ldquo;backs up&rdquo; to the next deepest node that still has unexplored successors. Uses a stack (LIFO queue). This means the most-recently-generated node is selected for expansion.<ul><li><strong>Completeness</strong>: The graph-search version (which avoids repeated states and redundant paths) is complete in finite state-spaces because it will eventually expand every node. The tree-search version is <strong>not complete</strong>; it can keep following a loop forever. The DFS tree-search algorithm can be modified at no extra memory-cost so that it checks new states against those on the path from the root to the current node. This avoids infinite loops in finite state-spaces but does not avoid the issue of redundant paths. In infinite state-spaces, both versions fail if an infinite non-goal path is encountered (e.g., Knuth&rsquo;s 4 problem; DFS will keep applying the same operator over and over again).<li><strong>Optimality</strong>: Both versions are non-optimal for similar reasons. Assuming we have a search space where we have a goal node $C$ on the right-subtree at some depth $d$, and a goal node $J$ on the left subtree at some depth $d&rsquo;$ ($d&rsquo; &gt; d$). Then DFS will start by exploring the left subtree even though $C$ is a goal node. Furthermore, it would end up returning $J$ as a solution even though $C$ is a better solution. Hence DFS is not optimal.<li><strong>Time</strong>: The time complexity for DFS graph-search is bounded by the size of the state-space (which could be infinite). A DFS tree-search however, may end up generating all of the $O(b^m)$ nodes in the search tree, where $m$ is the maximum depth of any node (so this can be much bigger than the size of the state space). Also, $m$ itself can be much larger than $d$, and is infinite if the tree is unbounded. For an example, consider a binary tree where the goal node is the deepest and right-most node. In this case, DFS will generate <em>all</em> nodes before it gets to the goal node. Even if the goal node is at depth <em>d</em> which is much smaller than <em>m</em>, the time-complexity is still dominated by the fact that it is still exploring all the other nodes, and hence we still end up with $O(b^m)$.<li><strong>Space</strong>: The space complexity is the reason we consider DFS. There is no advantage for a graph search, but in a DFS tree search, we only need to store a single path from the root to a leaf node, along with any remaining, unexpanded sibling-nodes for each node in the path. Once a node has been fully expanded, it can be removed from memory as soon as all of its descendants have been fully explored. Hence, the storage is only $O(bm)$ for a state space with branching-factor $b$ and maximum depth $m$.</ul><h2 id=depth-limited-dfs>Depth-limited DFS</h2><p>DFS fails in infinite search-spaces. This failure can be alleviated by using a variation called depth-limited DFS. In this algorithm, DFS is supplied with a predetermined depth-limit $l$. This means that nodes at depth $l$ are treated as if they have no successors. This solves the infinite path problem. However, we have an additional source of incompleteness if we choose $l &lt; d$ (i.e., the shallowest goal is beyond the depth limit. This usually happens when $d$ is unknown). Depth-limited DFS is also nonoptimal if we chose $l &gt; d$ (for reasons of nonoptimality in DFS in general).<ul><li><strong>Completeness</strong>: Incomplete if $l &lt; d$ (but also incomplete in general).<li><strong>Optimality</strong>: Nonoptimal when $l &gt; d$.<li><strong>Time</strong>: $O(b^l)$.<li><strong>Space</strong>: $O(bl)$.</ul><h2 id=id-dfs>ID-DFS</h2><p>This is a general strategy used in combination with DFS tree search that finds the best depth limit. The algorithm does this by gradually increasing the depth (first 0, then 1, then 2, and so on) until a node is found. This occurs when the depth limit reaches $d$, the depth of the shallowest goal-node. Iterative deepening combines the benefits of DFS and BFS.<ul><li><strong>Completeness</strong>: It is complete if $b$ is finite.<li><strong>Optimal</strong>: It is optimal if all step-costs are identical.<li><strong>Time</strong>: $O(b^d)$.<li><strong>Space</strong>: $O(bd)$.</ul><p>This can seem wasteful because states are generated multiple times. But it turns out this is not too costly, this is because in a search tree with the same (or nearly the same) branching factor at each level, most of the nodes are in the bottom level and so it does not matter that the upper levels are generated multiple times. In an ID-DFS, the nodes at depth $d$ are generated once, the ones are depth $d - 1$ are generated twice, and so on. Hence we have:<p>$$
N(IDS) = (d)b + (d - 1)b^2 + \dotso + (1)b^d\text{, which is }O(b^d)
$$<p>There is an extra cost of generating the upper levels multiple times, but it is not too large. For example, with <em>b</em> set to $10$ and <em>d</em> set to $5$, we have:<p>$$
N(IDS) = 50 + 400 + 3,000 + 20,000 + 100,000 = 123,450
$$
$$
N(BFS) = 10 + 100 + 1,000 + 10,000 + 100,000 = 111,110
$$<p><strong>In general, ID-DFS is the preferred uninformed-search method when the search space is large and the depth of the solution is not known.</strong><h1 id=informed-search>Informed Search</h1><p>An informed-search strategy is one that uses problem-specific knowledge beyond the defintion of the problem itself. It can find solutions more efficiently than can an uninformed strategy.<p>The general approach that we consider is called <strong>best-first search</strong>. This is an instance of the general $TREE-SEARCH$ or $GRAPH-SEARCH$ algorithm in which a node is selected for expansion based on an <em>evaluation function</em> $f(n)$. The evaluation function is construed as a cost estimate, and a node with the <em>lowest</em> evaluation is expanded first. The implementation of best-first search is similar to uniform-cost search except we use $f$ instead of $g$ to order the priority queue.<p><strong>The choice of $f$ determines the search strategy.</strong> Most best-first algorithms include as a component of $f$, a <strong>heuristic function</strong>, denoted as $h(n)$:<ul><li>$h(n)$: estimated cost of the cheapest path from the state at node $n$ to a goal state.</ul><p>Although $h(n)$ takes a node as input, it depends on the <em>state</em> at that node. This is unlike $g(n)$, which returns the path cost from the root to node $n$. If $n$ is a goal node, $h(n) = 0$.<h2 id=greedy-best-first-search>Greedy Best-First Search</h2><p>Here $f(n) = h(n)$.<ul><li><strong>Optimality</strong>: It is not optimal, and to see why, consider a path A-B-C and A-C. The heuristic cost from A-B is 50, the cost from B-C is 90, and the cost from A-C is 100. Greedy best-first search will choose B for expansion because 50 is lesser than 100. After that, it will choose to go from B to C because the cost is 90, which is lesser than 100. However, the path from A to C via B is 40 more than the path from A to C directly. Hence it is not optimal.<li><strong>Completeness</strong>: It is incomplete in a finite state-space like DFS. Consider the same situation above, but with the difference that there is no path from B-C. Then in the tree-search version, B is repeatedly expanded the path from A to C will never be taken (infinite loop). The graph-search version <em>is</em> complete in finite spaces, but not in infinite ones.<li><strong>Time</strong> and <strong>space</strong>: The worst-case complexity for tree-search version is $O(b^m)$, where $m$ is the maximum depth of the search space.</ul><h2 id=a-search>A* search</h2><p>** A* ** search is both <strong>complete</strong> and <strong>optimal</strong>. It is also <strong>optimally efficient</strong> for any given consistent-heuristic.<p>Here $f(n) = g(n) + h(n)$.<p>Since $g(n)$ is the path cost from start node to node $n$, and $h(n)$ is the estimated cost of the cheapest path from $n$ to the goal, we have:<p>$f(n)$ = estimated cost of the cheapest solution <strong>through</strong> $n$.<p>The algorithm is identical to uniform-cost search except it uses $g + h$ instead of $g$.<p>A* has conditions for optimality. These are <strong>admissibility</strong> and <strong>consistency</strong>.<p>The first condition required for optimality is that $h(n)$ is an <strong>admissible heuristic</strong>:<p><strong>Admissible Heuristic</strong>: A heuristic that <em>never overestimates</em> the cost to reach the goal. This means that $f(n)$ also never overestimates the true cost of a solution along the current path through $n$. Admissible heuristics are optimistic by nature because they think the cost of solving the problem is less than it actually is (e.g.: straight-line distance).<p><strong>Consistent Heuristic</strong>: This is a stronger condition that is required only for applications of A* to graph search. A heuristic $h(n)$ is consistent if, for every node $n$ and every successor $n&rsquo;$ of $n$ generated by any action $a$, the esimated cost of reaching the goal from $n$ (i.e., $h(n)$) is no greater than the step cost of getting to $n&rsquo;$ ($c(n, a, n&rsquo;)$) plus the estimated cost of reaching the goal from $n&rsquo;$ ($h(n&rsquo;)$):<p>$h(n) \le c(n, a, n&rsquo;) + h(n&rsquo;)$<p>This is a form of the general <strong>triangle inequality</strong>.<p><strong>Optimality of A</strong>*: The tree-search version of A* is optimal if $h(n)$ is admissible, while the graph-search version is optimal if $h(n)$ is consistent.<p>How is A* optimal? First, <em>if $h(n)$ is consistent, then the values of $f(n)$ along any path are nondecreasing.</em> The proof follows directly from the definition of consistency. Supposed $n&rsquo;$ is a successor of $n$; then $g(n&rsquo;) = g(n) + c(n, a, n&rsquo;)$ for some action $a$ and we have:<p>$$
f(n&rsquo;) = g(n&rsquo;) + h(n&rsquo;)
= g(n) + c(n, a, n&rsquo;) + h(n&rsquo;) \ge g(n) + h(n)
= f(n)
$$<p>The next thing we have to prove is that <em>whenever A</em>* <em>selects a node $n$ for expansion, the optimal path to that node has been found.</em> If this was not the case, it means that there would have to be another frontier node $n&rsquo;$ on the optimal path from the start node to $n$. Since $f$ is nondecreasing along any path, $n&rsquo;$ would have a lower $f$-cost than $n$ and would have been selected first. Hence a node like $n&rsquo;$ cannot exist.<h2 id=heuristic-development>Heuristic Development</h2><p>Heuristic accuracy has an effect on performance. The quality of a heuristic is measured by the <strong>effective branching factor</strong> $b^*$. If the total number of nodes generated by A* for any particular problem is $N$ and the solution depth is $d$, then $b^*$ is the branching factor that a uniform tree of depth $d$ would have to have in order to contain $N + 1$ nodes. Hence:<p>$$
N + 1 = 1 + b^* + (b^*)^2 + \dotso + (b^*)^d
$$<p>For example, if A* finds a solution at depth 5 using 52 nodes, the effective branching-factor is 1.92.<p>It is therefore desirable to generate good heuristics. There are multiple ways to do it:<ul><li><strong>Relaxed problems</strong>: The heuristics $h_1$ (misplaced tiles) and $h_2$ (Manhattan distance) are fairly-good heuristics for the 8-puzzle problem. Looking at them closely, they are perfectly-accurate path-lengths for <em>simplified</em> versions of the puzzle. For the first one, the rule is changed so that a tile could move anywhere instead of just to the adjacent empty square. Then, $h_1$ gives us the exact number of steps in the shortest solution. If we changed the rule further such that we could move the tile one square in any direction, even to an occupied square, then $h_2$ gives us the exact number of steps in the shortest solution. A problem with fewer restrictions on its actions is called a <strong>relaxed problem</strong>. This makes the state-space for the relaxed problem a <em>supergraph</em> of the original state-space, because the removal of restrictions creates additional edges (i.e., there are more ways we can transition from one state to another since restrictions are removed). Any optimal solution in the original problem is, by definition, also a solution in the relaxed problem; but the relaxed problem may have <em>better</em> solutions if the added edges provide short-cuts. Hence, <em>the cost of an optimal solution to a relaxed problem, is an admissible heuristic for the original problem</em> (since it can <em>never</em> overestimate). Also, since the derived heuristic is an exact cost for the relaxed problem, it must obey the triangle inequality and is therefore <strong>consistent</strong>.<li><strong>$ABSOLVER$</strong>: It can generate heuristics automatically from problem definitions, using the &ldquo;relaxed problem&rdquo; method and various other techniques (Prieditis, 1993).<li><strong>Pattern databases</strong>: Admissible heuristics can be derived from the solution cost of a <strong>subproblem</strong> of a given problem. The cost of an optimal solution of a subproblem is definitely a lower-bound on the cost of the complete problem. Hence it is an admissible heuristic. The idea behind <strong>pattern databases</strong> is to store these exact solution-costs for every-possible subproblem-instance. We can compute an admissible heuristic $h_DB$ for each complete state encountered during a search, simply by looking up the corresponding subproblem configuration in the database. The database itself is constructed by searching back from the goal and recording the cost of each new pattern encountered; the expense of this search is amortized over mnay subsequent problem instances.<li><strong>Learning heuristics from experience</strong>: We can solve lots of problems and each optimal solution provides examples from which we can learn $h(n)$. Another way is use <strong>features</strong> of a state that are relevant to predicting the state&rsquo;s value rather than the raw state-description. For example, we can generate 100 random 8-puzzle configurations and gather statistics on their actual solution-costs. For example, assume we have a feature called &ldquo;number of misplaced tiles&rdquo;, that is $x_1(n)$. We might find that when this value is 5, the average solution cost is around 14 and so on. A second feature $x_2(n)$ would be &ldquo;number of pairs of adjacent tiles that are not adjacent in the goal state&rdquo;. These features can then be combined to generate an $h(n)$. A common approach is to use linear combination: $h(n) = c_1x_(n) + c_2x_2(n)$. The constants are adjusted to get the best fit to the actual data on solution costs. The heuristic will satisfy the condition that $h(n) = 0$ for goal states, but <strong>is not necessarily admissible or consistent</strong>.</ul><p>What happens if one fails to get a single, &ldquo;clearly-best&rdquo; heuristic from the ones generated? What if a collection of admissible heuristics is available such that none of them dominates any of the others? We can have the best of both worlds by doing:<p>$h(n) = \max{h_1(n), \dotso, h_m(n)}$<h1 id=local-search>Local Search</h1><p>In many problems, path to the goal is irrelevant (e.g., 8 queens problem). What we usually care about is the final solution. Hence, we don&rsquo;t need to store the path to the solution, but can simply explore the solution space to either maximize/minimize the objective function (depending on whether we are maximizing payoff or minimizing cost).<h2 id=hill-climbing-search>Hill-climbing search</h2><p>This is a simple search algorithm that simply loops and moves in the direction of increasing value (uphill). It will terminate when it reaches a peak (i.e., a point where none of the neighbors have a higher value). This algorithm is also known as <strong>greedy local search</strong>. Unfortunately, hill climbing will get stuck for the following reasons:<ul><li><strong>Local maxima</strong><li><strong>Ridges</strong> (sequence of local maxima that is very difficult for greedy algorithms to navigate)<li><strong>Plateaux</strong> (flat area).</ul><p>There are variations to get around these difficulties:<ul><li><strong>Stochastic hill climbing</strong> chooses at random from among the uphill moves; the probability of selection can vary with the steepness of the uphill move. This usually converges more slowly than steepest ascent, but in some state landscapes, it finds better solutions.<li><strong>First-choice hill climbing</strong> implements stochastic hill-climbing by generating successors randomly until one is generated that is better than the current state. This is a good strategy when when a state has many (e.g., thousands) of successors.<li><strong>Random-restart hill climbing</strong>: The above algorithms are incomplete; they often fail to find a goal when one exists because they can get stuck on a local maxima. In random-restart, we conduct a series of hill-climbing searches from randomly-generated initial states until a goal is found.</ul><h2 id=simulated-annealing>Simulated annealing</h2><p>The regular hill-climbing algorithm <em>never</em> makes &ldquo;downhill&rdquo; moves towards states with a lower value (or higher cost). Hence, it is guaranteed to be incomplete. A purely-random walk would allow us to move both uphill and downhill, but is very inefficient. How could we combine the two? We can do this with <strong>simulated annealing</strong>. The innermost loop of the simulated-annealing algorithm is quite similar to hill climbing, except instead of choosing the <em>best</em> move, it chooses a <em>random</em> move. If the move improves the situation, it is always accepted. Otherwise (i.e., if the move is &ldquo;bad&rdquo;), the algorithm accepts the move with some probability less than 1. The probability decreases exponentially with the &ldquo;badness&rdquo; of the move (i.e., the amount of $\Delta E$ by which the evaluation is worsened), and also decreases as the &ldquo;temperature&rdquo; $T$ goes down. This means that &ldquo;bad&rdquo; moves are more likely to be allowed at the start when $T$ is high, and they become more unlikely as $T$ decreases. If the <em>schedule</em> lowers $T$ slowly enough, the algorithm will find a global optimum with probability approaching 1.<h2 id=local-beam-search>Local beam search</h2><p>The <strong>local beam search</strong> algorithm keeps track of $k$ states rather than just one state. It begins with $k$ randomly-generated states. At each step, all successors of all $k$ states are generated. If any one of those is a goal, the algorithm halts. Otherwise, it selects the $k$ best successors from the complete list and repeats. This may look like we are simply running $k$ random restarts in parallel. However, in random-restart each search process runs independently of the others. <em>In a local beam-search, useful information is passed among the parallel search-threads</em>. This means the algorithm abandons unfruitful searches and moves towards the area where most progress is being made.<h1 id=adversarial-search>Adversarial Search</h1><p>Algorithms that cover <strong>competitive</strong> environments in which the agents&rsquo; goals are in conflict.<h2 id=minimax-algorithm>MINIMAX algorithm</h2><p>Assume there are two players $MAX$ and $MIN$. $MAX$ always chooses a move that maximizes the payoff, whereas $MIN$ will always choose a move that will minimize $MAX$&rsquo;s payoff. So here the strategy is built up assuming that each player plays optimally. Given a game tree, the optimal strategy can be determined from the <strong>minimax value</strong> of each node, which is written as $MINIMAX(n)$.<p>$$
MINIMAX(s)
$$
$$
= UTILITY(s)\text{ if }TERMINAL-TEST(s)
$$
$$
= \max_{a \in Actions(s)}\ MINIMAX(RESULT(s, a))\text{ if }PLAYER(s) = MAX
$$
$$
= \min_{a \in Actions(s)}\ MINIMAX(RESULT(s, a))\text{ if }PLAYER(s) = MIN
$$<p>The <strong>minimax algorithm</strong> computes the minimax decision from the current state. It uses a simple, recursive computation of minimax values of each successor state, directly implementing the defining equations. The recursion proceeds all the way down to the leaves of the tree, and then the minimax values are <strong>backed up</strong> through the tree as the recursion unwinds.<p>Assume the following tree:<p><img src=http://imgur.com/sXrSXr5.png alt=minimax><p>The recursion proceeds all the way to three bottom-left nodes, and here it uses the $UTILITY$ function on them to discover that their values are 3, 12, and 8. Since the level above is where $MIN$ would play, it takes the minimum of the three values and returns it as the backed-up value of node B. A similar process gives the backed-up values for nodes C and D. Since the root is where $MAX$ plays, we take the maximum of the values, which gives us 3, which also ends up being the backed-up value of the root node.<p>The minimax algorithm performs a complete depth-first exploration of the game tree. If the maximum depth of the tree is <em>m</em> and there are <em>b</em> legal moves at each point, then the time complexity of the minimax algorithm is $O(b^m)$. The space complexity is $O(bm)$ for an algorithm that generates all actions at once, or $O(m)$ for an algorithm that generates them one at a time.<h2 id=alpha-beta-pruning>Alpha-Beta Pruning</h2><p>With minimax search, the number of game states it has to examine is exponential in the depth of the tree. We cannot eliminate the exponent, but we can cut it in half by <strong>pruning</strong>. The idea is that we can compute the correct minimax decision without looking at every node in the game tree. The general principle is this: consider a node $n$ somewhere in the tree, such that the player has a chance of moving to that node. If the player has a better choice $m$ either at the parent of node $n$, or at any choice point further up, then $n$ will never be reached in actual play. So once we have found out enough about $n$ (by examining some of its descendants) to reach this conclusion, we can prune it.<p>The algorithm gets its name from the following two parameters that describe the bounds on the backed-up values that appear anywhere along the path:<ul><li>$\alpha$ = the value of the best (i.e., highest-value/upper-bound) choice we have found so far at any choice point along the path for $MAX$.<li>$\beta$ = the value of the best (i.e., lowest-value/lower-bound) choice we have found so far at any choice point along the path for $MIN$.</ul><p>To see it in action, consider this tree:<pre><code>                     A
                   / | \
                  /  |  \
                 /   |   \
                /    |    \
               /     |     \
              /      |      \
             /       |       \
            B        C        D
           /|\      /|\      /|\
          / | \    / | \    / | \
         /  |  \  /  |  \  /  |  \        
        11  9 13 7   5 14 2  10   3
</code></pre><p>First we will go all the way down the left sub-tree to find 11, which is initially the lowest-value at B (MIN). On examining other nodes, we find that 9 is actually the lowest value. So B goes through the following value changes:<ul><li>B: [$-\infty$, 11] -&gt; [$-\infty$, 9] -&gt; [9, 9]</ul><p>Since the MIN value at B is 9, we know that A (MAX) can currently choose a value that is at least 9. So we end up with the following values:<ul><li>A: [9, $+\infty$]<li>B: [9, 9]</ul><p>Now we will walk through the next subtree. Here, we first find 7. So the values are:<ul><li>A: [9, $+\infty$]<li>B: [9, 9]<li>C: [$-\infty$, 7]</ul><p>If we explore any more of C&rsquo;s children, they can only have values that are lesser than equal to 7, because MIN at C will choose the smallest value. This means that we will never get a value that is higher than 7. We also know that MAX chooses the highest value. The option that MAX has at A is 9, so it will <strong>never</strong> choose C! This means that we don&rsquo;t have to explore any other nodes under C.<p>Now we will explore D. Here, we first find 2. So the values are:<ul><li>A: [9, $+\infty$]<li>B: [9, 9]<li>C: [$-\infty$, 7]<li>D: [$-\infty$, 2]</ul><p>For the same reasons as in the case of C, MAX will never pick D because 2 is smaller than its best choice of 9. This means that we don&rsquo;t have to explore any of D&rsquo;s other children either. Hence, we finally end up with A: [9, 9] which means we play 9.<p>Let&rsquo;s also see it in action with the tree rotated:<pre><code>                     A
                   / | \
                  /  |  \
                 /   |   \
                /    |    \
               /     |     \
              /      |      \
             /       |       \
            B        C        D
           /|\      /|\      /|\
          / | \    / | \    / | \
         /  |  \  /  |  \  /  |  \        
        3  10  2 14  5  7 13  11  9
</code></pre><p>First we will go all the way down the left sub-tree. B ends up going through the following value changes:<ul><li>B: [$-\infty$, 3] -&gt; [$-\infty$, 2] -&gt; [2, 2]</ul><p>Since the MIN value at B is 2, we know that A (MAX) can only choose a value that is at least 2. So we end up with the following values:<ul><li>A: [2, $+\infty$]<li>B: [2, 2]</ul><p>Now we will walk through C&rsquo;s subtree. Here we first find 14. This is better than the current best-choice of 2, so we have the following values:<ul><li>A: [2, 14]<li>B: [2, 2]<li>C: [$-\infty$, 14]</ul><p>We then find 5 and 7 under C, which means we end up with the following:<ul><li>A: [2, 14] -&gt; [2, 5]<li>B: [2, 2]<li>C: [$-\infty$, 14] -&gt; [$-\infty$, 5] -&gt; [5, 5]</ul><p>Now we look at D:<ul><li>A: [2, 5] -&gt; [2, 13] -&gt; [2, 11] -&gt; [2, 9] -&gt; [9, 9]<li>B: [2, 2]<li>C: [5, 5]<li>D: [$-\infty$, 13] -&gt; [$-\infty$, 11] -&gt; [$-\infty$, 9] -&gt; [9, 9]</ul><p>So we play 9. What is interesting is that the rotated tree forced us to examine every-single node, which means we pretty much ended up with minimax. This will happen if we examine the children of a node in order of decreasing utility.<h1 id=introduction-and-decision-trees>Introduction and Decision Trees</h1><ul><li><strong>Unsupervised Learning</strong>: The agent learns patterns in the input even though no explicit feedback is supplied. Most common unsupervised learning-task is clustering: detecting potentially useful clusters of input examples. For example, a taxi agent might gradually develop a concept of &ldquo;good traffic days&rdquo; and &ldquo;bad traffic days&rdquo; without ever being given labeled examples.<li><strong>Reinforcement learning</strong>: The agent learns from a series of reinforcements - rewards or punishments.<li><strong>Supervised learning</strong>: The agent observes some example input - output pairs and learns a function that maps from input to output. The outputs can come from a teacher who gives the agent information about what the output is. The output can also come from the agent&rsquo;s percepts and the environment ends up being the teacher.</ul><p>Noise and lack of labels create a continuum between supervised and unsupervised learning.<h2 id=supervised-learning>Supervised Learning</h2><p>The task of supervised learning is this:<p>Given a <strong>training set</strong> of $N$ example input-output pairs $(x_1, y_1), (x_2, y_2), \dotso, (x_N, y_N)$, where each $y_j$ was generated by an unknown function $y = f(x)$, discover a function $h$ that approximates the true function $f$.<p>$x$ and $y$ can be any value; they need not be numbers. The function $h$ is a <strong>hypothesis</strong>. Learning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set. To measure the accuracy of a hhypothesis we give it a <strong>test set</strong> of examples that are distinct from the training set. We say a hypothesis <strong>generalizes</strong> well if it correctly predicts the value of <em>y</em> for novel examples. Sometimes the function $f$ is <strong>stochastic</strong> - it is not strictly a function of $x$, and what we have to learn is a <strong>conditional probability</strong> distribution $P(Y | x)$.<p><strong>Classification</strong>: When the output $y$ is one of a finite set of values (such as $sunny$, $cloudy$, or $rainy$), the learning problem is called <strong>classification</strong>, and is called boolean or binary classification if there are only two values.<p><strong>Regression</strong>: When $y$ is a number (such as tomorrow&rsquo;s temperature), the learning problem is called <strong>regression</strong>. (Technically, solving a regression problem is finding a conditional expectation or average value of $y$, because the probability that we have found <em>exactly</em> the right real-valued number for $y$ is 0.)<p>How do we choose from among multiple, consistent hypotheses? One answer is to prefer the <em>simplest</em> hypothesis consistent with the data. This principle is called <strong>Ockham&rsquo;s razor</strong>. In general there is a tradeoff between complex hypotheses that fit the training data well, and simpler hypotheses that may generalize better (i.e., the question of overfitting).<p>We say a learning problem is <strong>realizable</strong> if the hypothesis space contains the true function. Unfortunately we cannot always tell whether a given learning problem is realizable.<h2 id=learning-decision-trees>Learning Decision Trees</h2><p>A <strong>decision tree</strong> represents a function that takes as input a vector of attribute values and returns a &ldquo;decision&rdquo; - a single output value. The input and output values can be discrete or continuous. A decision tree reaches its decision by performing a sequence of tests. Each internal node in the tree corresponds to a test of the value of one of the input attributes, $Ai$, and the branches from the node are labled with the possible values of the attribute, $A_i = v_{ik}$. Each leaf node in the tree specifies a value to be returned by the function.<p>Some functions cannot be represented concisely. For example, the majority function, which returns true if and only if more than half of the inputs are true, requires an exponentially large decision tree. Decision trees are therefore good for some kinds of functions and bad for others. There is no <em>one</em> representation that is efficient for all kinds of funtions. For example, consider the set of all boolean functions on $n$ attributes. How many different functions are in this set? This is the number of different truth tables we can write down. A truth table over $n$ attributes has $2^n$ rows, one for each combination of values of the attributes. We can consider the answer column of the table as a $2^n$-bit number that defines the function. Therefore there are $2^(2^n)$ different functions.<p>Finding a minimal decision tree consistent with the training set is NP-hard. Constructing a minimal binary tree with respect to the expected number of tests required for classifying an unseen instance is NP-complete. Even finding the minimal equivalent decision tree for a given decision tree, or building the optimal decision tree from decision tables is known to be NP-hard. (pg 699)<p><strong>Inducing decision trees from examples</strong><p>An example for a decision tree consists of an $(x, y)$ pair where $\mathbf{x}$ is a vector of values for the input attributes, and $y$ is a single Boolean output value.<p>It is guided by four cases:<ol><li>If the remaining examples are all positive (or all negative), then we are done: we can answer <em>Yes</em> or <em>No</em>. (e.g., see None and Some branches on pg 701).<li>If there are some positive and negative examples, then choose the best attribute to split them. (see Hungry being used to split on pg 701).<li>If there are no examples left, it means that no example has been observed for this combination of attribute values, and we return a default value calculated from the plurality classification of all examples that we used in contructing the node&rsquo;s parent. (passed along in parent_examples).<li>If there are no attributes left, but both positive and negative examples, it means that these examples have exactly the same description, but different classifications. This can happen because there is an error or noise in the data.</ol><p>The set of examples is crucial for <em>constructing</em> the tree, but do not appear anywhere in the tree itself.<p>We can <strong>evaluate</strong> the accuracy of a learning algorithms with a <strong>learning curve</strong>. We split the examples into a training set and test set. We learn a hypothesis <em>h</em> with the training set and measure its accuracy with the test set. For example, if we have 100 examples, we start with a training set of size 1 and increase one at a time up to size 99. For each size, we repeat the process of randomly splitting 20 times, and average the results of the 20 trials.<p><strong>Choosing attribute tests</strong><p>We need a formal measure of &ldquo;fairly good&rdquo; and &ldquo;really useless&rdquo; to figure out which attribute we should base our test on. We can do this with the notion of information gain, which is defined in terms of <strong>entropy</strong>:<p>$$
H(V) = \sum\limits_kP(v_k)\log_2\dfrac{1}{P(v_k)} = -\sum\limits_kP(v_k)\log_2P(v_k)
$$<p>Hence for a boolean variable, it is just $B(q) = -(q\log_2q + (1 - q)log_2(1 - q))$. Since a boolean variable can only have two values. The probability of one value is always 1 minus probability of the other. So in the case of the decision tree, we can look at the entropy of the goal attribute on the whole set. We can simply look at one example, the positive example, since the formula accounts for the negative example as well $(1 - q)$. So we can simply calculate $H(Goal)$, which is just $B\big(\frac{p}{p + n}\big)$ (i.e., we see the percentage of how many times the positive example happens in the example set).<p>Now testing on a single attribute will only give us part of the bits of $B\big(\frac{p}{p + n}\big)$. In the restaurant example, testing one attribute only gives us part of the information of 1 bit (of entropy; p = n = 6 so B = 1). We can measure exactly how much by looking at the entropy remaining <em>after</em> the attribute test.<p>If you have an attribute $A$ with $d$ distinct values, it divides the training set of examples $E$ into subsets $E_1, \dotso, E_d$. Each subset $E_k$ has $p_k$ positive examples and $n_k$ negative examples. If we go along that particular branch, we will need an additional $B\big(\frac{p_k}{p_k + n_k}\big)$ bits of information. A randomly chosen example from the training set has the $k^{th}$ value for the attribute with probability $\frac{p_k + n_k}{p + n}$, and so the expected entropy remaining after testing attribute A is:<p>$$
Remainder(A) = \sum\limits_{k = 1}^{d}\dfrac{p_k + n_k}{p + n}B\bigg(\dfrac{p_k}{p_k + n_k}\bigg)
$$<p>The <strong>information gain</strong> from the attribute test on $A$ is the expected reduction in entropy:<p>$$
Gain(A) = B\bigg(\dfrac{p}{p + n}\bigg) - Remainder(A)
$$<p><strong>Generalization and overfitting</strong><p>An algorithm may accurately predict every case according to the test data, but it may not be general. This is called overfitting. For decision trees, we can use <strong>decision tree pruning</strong> to combat overfitting. Pruning works by eliminating nodes in the decision tree that are not clearly relevant. We start with a full tree, and then look at a test node that has only leaf nodes as descendants. If the test appears to be irrelevant (detecting only noise in the data), we eliminate the test, replacing it with a leaf node. This process is repeated, considering each test with only leaf descendants, until each one has either been pruned or accepted as is.<p>How do we detect irrelevance? Suppose we are at a node consisting of $p$ positive and $n$ negative examples. If the attribute is irrelevant, we would expect that it would split the examples into subsets that each have roughly the same proprtion of positive examples as the whole set, $\frac{p}{p + n}$, and so the information gain will be close to zero. Hence information gain is a good clue to irrelevance. How large a gain should we require in order to split on a particular attribute?<p>To answer that we do a statistical <strong>significance test</strong>. Such a test begins by assuming there is no underlying pattern (the so-called <strong>null hypothesis</strong>). Then the actual data are analyzed to calculate to the extent to which they deviate from a perfect absence of pattern. If the degree of deviation is statistically unlikely (usually taken to mean a 5% probability or less), then that is considered to be good evidence for the presence of a significant pattern in the data. This is basically chi-squared.<p>So we need observed and expected values for each subset (i.e., for every $E_k$ from $1$ to $d$). The expected values would then just be $\hat{p_k} = p\times\big(\dfrac{p_k + n_k}{p + n}\big)$ (for positive) and $\hat{n_k} = n\times\big(\dfrac{p_k + n_k}{p + n}\big)$ (for negative). The total deviation would be the overall sum for each $k$ of the sum of the positive and negative chi-squared values. (pg 706):<p>$$
\Delta = \sum\limits_{k = 1}^{d}\dfrac{(p_k - \hat{p_k})^2}{\hat{p_k}} + \dfrac{(n_k - \hat{n_k})^2}{\hat{n_k}}
$$<p>This value delta is distributed according to chi-squared with $v - 1$ degress of freedom (where $v$ is the number of values for the attribute).<p><strong>Minimum depth decision tree</strong><p>We want an easy, simple, elegant decision tree that minimizes misclassification errors. The problem is NP-hard (find min decision tree with min error).<p><strong>Broadening the applicability of decision trees</strong><p>See pg 707<ul><li><strong>Missing data</strong> Given a complete decision tree, how should one classify an example that is missing one of the test attributes? Second, how should one modify the information-gain formula, when some examples have unknown values for the attribute?<li><strong>Multivalued attributes</strong> How do you split on something like <em>ExactTime</em> that has a bunch of different values that are effectively singletons?<li><strong>Continuous and integer-valued input attributes</strong> Something like <em>Height</em> and <em>Weight</em> would have an infinite set of possible values. Decision trees usually try to find a <strong>split point</strong> like, <em>Weight</em> &gt; 160.<li><strong>Continuous-valued output attributes</strong> If we are trying to predict a numerical output value, we need a <strong>regression tree</strong> rather than a classification tree. A regression tree has at each leaf a linear function of some subset of numerical attributes rather than a single value. Learning algorithm must decide when to stop splitting and begin applying linear regression over the attributes.</ul><h2 id=evaluating-and-choosing-the-best-hypothesis>Evaluating and Choosing the Best Hypothesis</h2><p>We want to learn a hypothesis that fits the future data best. We make the <strong>stationary assumption</strong>: that there is a probability distribution over examples that remains stationary over time (pg 708). Examples that satisfy these assumptions are called <em>independent and identically distributed</em> or <strong>i.i.d.</strong>. This is a definition for our &ldquo;future data&rdquo;.<p>What is &ldquo;best fit&rdquo;? First we define the <strong>error rate</strong> of a hypothesis as the proportion of mistakes it makes. That is the number of times $h(x) \ne y$ for an $(x, y)$ example. It does not mean, however, that because the hypothesis has a low error rate on the training set, it will generalize well. To get an accurate evaluation of a hypothesis, we need to test it on a set of examples it has not seen yet.<p>The simplest approach is <strong>holdout cross-validation</strong>. Randomly split the available data into a training set from which the learning algorithm produces $h$ and a test set on which the accuracy of $h$ is evaluated. This has the disadvantage that it fails to use all of the available data (pg 708).<p>Another technique is <em>k</em>-<strong>fold cross-validation</strong>. Each example serves double duty - as training data and test data. First we split the data into <em>k</em> equal subsets. We then perform <em>k</em> rounds of learning; on each round, <em>1/k</em> of the data is held out as a test set and the remaining examples are used as training data. The average test set score of the <em>k</em> rounds should then be a better estimate than a single score. Popular values for <em>k</em> are 5 and 10. This takes us 5-10 times longer to run but is statistically more-accurate. The extreme is <em>k = n</em> or <strong>leave-one-out cross-validation</strong> or <strong>LOOCV</strong>.<p><strong>Peeking</strong> causes the test results to be invalidated. Users can inadvertently <strong>peek</strong> at the test data. If you select the hypothesis <em>on the basis of its test set error rate</em>, you have peeked.<p>The best way is to lock your test set away and only use it to validate.<p><strong>Model selection: Complexity versus goodness of fit</strong><p>(pg 709, 710)<p>Higher-degree polynomials can fit the training data better, but when the degree is too high, they will overfit and perform poorly on validation data. Choosing the degree of the polynomial is an instance of the problem of <strong>model selection</strong>. You can think of the task of finding the best hypothesis as two tasks: model selection defines the hypothesis space and then <strong>optimization</strong> finds the best hypothesis within that space.<p>This section talks about selecting models that are parameterizing by <em>size</em>.<h1 id=regression-and-classification-with-linear-models>Regression and Classification with Linear Models</h1><p>Applies to the class of linear functions of continuous-valued inputs.<h2 id=loss-functions>Loss functions</h2><p>In the following, $o$ is output, $e$ is expected.<ul><li>$L_1$ is absolute value loss: $L_1(e, o) = |e - o|$<li>$L_2$ is squared error loss: $L_2(e, o) = (e - o)^2$ (good for regression - this is a convex function, so no local minimae)<li>$L_{0/1}$ is 0/1 loss: $L_{0/1}(e, o) = 0$ if $o = e$ else $1$. (good for binary classification)</ul><h2 id=univariate-linear-regression>Univariate linear regression</h2><p>pg(718, 719)<p>A univariate linear function with input $x$ and output $y$ has the form:<p>$$
y = w_1x + w_0
$$<p>Where $w_0$ and $w_1$ are real-valued coefficients to be learned. We use the letter $w$ because we can think of the coefficients as <strong>weights</strong>; the value of $y$ is changed by changing the relative weight of one term to another. We can define $\mathbf{w}$ to be the vector $[w_0, w_1]$ and define the hypothesis function:<p>$$
h_{\mathbf{w}}(x) = w_1x + w_0
$$<p>The task of finding the $h_{\mathbf{w}}$ that best fits the data is called <strong>linear regression</strong>. To fit a line to the data, all we have to do is find the values of the weights that minimize the empirical loss. We use the $L_2$ loss function for this. Many forms of learning involve adjusting weights to minimize a loss, so it helps to think about a <strong>weight space</strong> - the space defined by all possible settings of the weights. For univariate regression, the weight space is defined by $w_0$ and $w_1$ is two-dimensional. Hence we can graph loss as a function of $w_0$ and $w_1$ in a three-dimensional plot. We can see that the loss function is <strong>convex</strong> and this is true for <em>every</em> linear-regression problem with an $L_2$ loss function and thus implies that there are no local minima.<h2 id=gradient-descent>Gradient descent</h2><p>To go beyond linear models, we need to face the fact that equations defining minimum loss will often have no closed-form solution. Instead, we will face a general optimization search problem in a continuous weight space. We can address these by a hill-climbing algorithm that follows the <strong>gradient</strong> of the function to be optimizied. In this case, because we are trying to minimize the loss, we will use <strong>gradient descent</strong>. We choose any starting point in weight space (a point in the $(w_0, w_1)$ plane) and then move to a neighboring point that is downhill, repeating until we converge on the minimum possible loss.<p>The parameter $\alpha$ is usually called the <strong>learning rate</strong> when we are trying to minimze loss in a learning problem. It can be a fixed constant, or it can decay over time as the learning process proceeds. For univariate regression, the loss function is a quadratic function and so the partial derivative will be a linear function. (pg 719, 720)<p>Learning rule for the weights:<p>$$
w_0 = w_0 + \alpha(y - h_{\mathbf{w}}(x))
$$
$$
w_1 = w_1 + \alpha(y - h_{\mathbf{w}}(x)) \times x
$$<p>This covers only one training example. For N training examples, we want to minimize the sum of the individual losses for each example. The derivative of a sum is the sum of the derivatives and so:<p>$$
w_0 = w_0 + \alpha\sum\limits_j(y_j - h_{\mathbf{w}}(x_j))
$$
$$
w_1 = w_1 + \alpha\sum\limits_j(y_j - h_{\mathbf{w}}(x_j)) \times x_j
$$<p>These updates constitute the <strong>batch gradient descent</strong> learning rule for univariate linear regression. <strong>Convergence to a unique global minimum is guaranteed</strong> (as long as we pick $\alpha$ small enough) but may be very slow. We have to cycle through all the training data for every step, and there may be many steps.<p>Another possibility is <strong>stochastic gradient descent</strong>, where we consider only a single training point at a time, taking a step after each one using the learning rule (i.e., not the $N$ training examples; the single-example one). It can be used in an online setting, where new data are coming one at a time, or offline, where we cycle through the same data as many times as is necessary, taking a step after considering each single example. It is often faster than batch gradient descent. With a fixed learning rate $\alpha$, however, it <strong>does not guarantee convergence</strong>; it can oscillate around the minimum without settling down. In some cases, as we see later, a schedule of decreasing learning rates (as in simulated annealing) does guarantee convergence.<h2 id=multivariate-linear-regression>Multivariate linear regression</h2><p>We can extend the univariate linear-regression to solve <strong>multivariate linear-regression</strong> problems. In the multivariate case, each example $\mathbf{x}$j is an n-element vector. Hence:<p>$$
h_{sw}(\mathbf{x}_j) = w_0 + w_1x_{j, 1} + \dotso + w_nx_{j, n} = w_0 + \sum\limits_iw_ix_{j, i}
$$<p>The $w_0$ term (intercept) stands out as different. We can fold it into this equation by inventing a dummy input-attribute, $x_{j, 0}$ which is always $1$. Then $h$ is just the dot-product of the weights and the input vector, or the matrix product of the transpose of the weights and the input vector:<p>$$
h_{sw}(\mathbf{x}_j) = \mathbf{w}\cdot\mathbf{x}_j = \mathbf{w}^T\mathbf{x}_j = \sum\limits_iw_ix_{j, i}
$$
The best vector of weights, $\mathbf{w}^*$ minimizes the squared-error loss over the examples:<p>$$
\mathbf{w}^* = \underset{\mathbf{w}}{\operatorname{argmin}}\sum\limits_jL_2(y_j, \mathbf{w}\cdot\mathbf{x}_j)
$$<p>Gradient descent will reach the (unique) minimum of the loss function. The update function for each weight is<p>$$
w_i = w_i + \alpha\sum\limits_jx_{j,i}(y_j - h_{\mathbf{w}}(\mathbf{x}_j))
$$<p>It is possible to solve this analytically using linear algebra for the $\mathbf{w}$ that minimizes loss. If $\mathbf{y}$ is the output vector for training examples, and $\mathbf{X} is the <strong>data matrix</strong> (i.e., a matrix of inputs with one $n$-dimensional example per row). Then the solution that minimizes the mean squared error is:<p>$$
\mathbf{w}^* = (\mathbf{X}^{\mathrm{T}}\mathbf{X})^{-1}\mathbf{X}^{\mathrm{T}}\mathbf{y}
$$<p>With univariate we don&rsquo;t have to worry about overfitting. But with multivariate linear regression in high-dimensional spaces, it is possible that some dimension that is actually irrelevant, appears by chance to be useful, resulting in <strong>overfitting</strong>. Hence, we can use <strong>regularization</strong> on multivariate linear functions to avoid overfitting. The process of explicitly penalizing complex hypothesis is called <strong>regularization</strong>. This is because it looks for a function that is more regular, or less complex. We do this via a cost function (pg 721):<p>$$
Cost(h) = EmpLoss(h) + \lambda Complexity(h)
$$<p>Hence the best hypothesis is obtained by finding the one with the minimum cost in the entire hypothesis-space.<p>The choice of regularization function depends on the hypothesis space. For polynomials, a good regularization function is the sum of the squares of the coefficients. Keeping the sum small keeps us away from wiggly polynomials. Another way to simplify models is to reduce the dimensions that models work with. A process of <strong>feature selection</strong> can be performed to discard attributes that appear to be irrelevant. chi-squared pruning is a kind of feature selection.<p>For linear functions, the complexity can be specified as a function of weights. We can consider a family of regularization functions:<p>$$
Complexity(h_\mathbf{w}) = L_q(\mathbf{w}) = \sum\limits_i{\left\lvert w_i \right\rvert}^q
$$<p>As with loss functions, with $q = 1$ we have $L_1$ regularization, which minimizes the sum of the absolute values. With $q = 2$, we have $L_2$ regularization which minimizes the sum of the squares. When one should you pick? It depends on the problem, but $L_1$ regularization tends to produce a <strong>sparse model</strong>. That is, it often sets many weights to zero, effectively declaring the corresponding attributes to be irrelevant (like how DECISION-TREE-LEARNING does, but DTL does it by a different mechanism). Hypotheses that discard attributes can be easier for a human to understand, and may be less likely to overfit.<p>(pg 722) Explanation as to why $L_1$ regularization leads to weights of zero, while $L_2$ regularization does not. $L_1$ prioritizes axes, where values of the weights can be zero. We are looking for an intersection between the $L_q$ regularization space and the contours of the minimal achievable loss. At the intersection is where we have a hypothesis that minimizes cost (i.e., is not very complex and minimizes loss). $L_2$ treats dimensional axes as arbitrary. $L_2$ is spherical, which makes it rotationally invariant. $L_1$ is appropriate when the axes are not interchangeable.<h2 id=linear-classifiers-with-a-hard-threshold>Linear classifiers with a hard threshold</h2><p>Linear functions can be used to do classification as well as regression. (pg 723)<p>A <strong>decision boundary</strong> is a line (or a surface, in higher dimensions) that separates the two classes. A linear decision boundary is called a <strong>linear separator</strong> and data that admit such a separator are called <strong>linearly separable</strong>.<p>For the earthquake vs. nuke classification problem, we have the linear classifier $-4.9 + 1.7x_1 - x_2$. Where when it is greater than 0, we have nuclear explosions and otherwise we have earthquakes. If we use the convention of a dummy input such that $x_0 = 1$, we can write the classification hypothesis as follows:<p>$$
h_{\mathbf{w}}(x) = 1\text{ if }\mathbf{w}\cdot\mathbf{x} \ge 0\text{ and }0\text{ otherwise}
$$<p>Alternatively, we can think of $h$ as the result of passing the linear function $\mathbf{w}\cdot\mathbf{x}$ through a <strong>threshold function</strong>:<p>$$
h_\mathbf{w}(x) = Threshold(\mathbf{w}\cdot\mathbf{x})\text{ where }Threshold(z) = 1\text{ if }z \ge 0\text{ and }0\text{ otherwise}
$$<p>We don&rsquo;t have a closed-form solution that we can solve, and we cannot use gradient-descent either since the slope is 0 everywhere except at the point of discontinuity, where it does not exist at all (not differentiable). However, we can use a simple weight-update rule that converges to a solution - that is, a linear separator that classifies the data perfectly - <strong>provided the data are linearly separable</strong>. For a single example $(x, y)$ we have:<p>$$
w_i = w_i + \alpha(y - h_\mathbf{w}(x))x_i
$$<p>This is basically the update rule for linear regression, and is also called the <strong>perceptron learning rule</strong>. Since we are considering a 0/1 classification problem, the behavior is somewhat different. Both the true value (i.e., expected actual value) $y$ and the hypothesis output $h_\mathbf{w}(x)$ can be $0$ or $1$. So there are three cases:<ul><li>If the output is correct, i.e., $y = h_\mathbf{w}(x)$, then the weights are not changed.<li>If $y$ is 1 but $h_\mathbf{w}(x)$ is 0, then $w_i$ is <strong>increased</strong> since we want to make $\mathbf{w}\cdot\mathbf{x}$ bigger so that $h_\mathbf{w}(x)$ outputs 1.<li>If $y$ is 0 but $h_\mathbf{w}(x)$ is 1, then $w_i$ is <strong>decreased</strong> when the corresponding input $x_i$ is <strong>positive</strong> and <strong>increased</strong> when $x_i$ is <strong>negative</strong>. This makes sense because we want to make $\mathbf{w}\cdot\mathbf{x}$ smaller so that $h_\mathbf{w}(x)$ outputs 0.</ul><p>learning curve etc: (pg 724)<p>What if the data points are not linearly separable? This happens commonly in the real world. In general <strong>the perceptron rule may not converge</strong> to a stable solution for a <strong>fixed learning rate</strong>. But if the learning rate $\alpha$ decays as $O\left(\frac{1}{t}\right)$ where $t$ is the iteration number, then the rule can be shown to converge to a minimum-error solution when examples are presented in a random sequence. It can also be shown that finding the minimum-error solution is NP-hard, so one expects that many presentations of the examples will be requred for convergence to be achieved.<h2 id=linear-classification-with-logistic-regression>Linear classification with logistic regression</h2><p>In linear classifiers, we saw that passing the output of the linear function through the threshold function creates a linear classifier. Unfortunately the hard nature of the threshold causes some problems. Mainly $hw(x)$ is no-longer differentiable since it is discontinuous. Therefore, learning the perceptron rule is very unpredictable. Another problem is that the linear classifier outputs a completely-confident prediction of $1$ or $0$, even for examples that are quite close to the boundary. In many situations, we need more gradated predictions.<p>We can solve this by softening the threshold function, i.e., approximating the hard threshold with a continuous, differentiable function. The function we will use is called the logistic function:<p>$$
Logistic(z) = \dfrac{1}{1 + \mathrm{e}^{-z}}
$$<p>Therefore, we now have:<p>$$
h_\mathbf{w}(x) = Logistic(\mathbf{w}\cdot\mathbf{x}) = \dfrac{1}{1 + \mathrm{e}^{-\mathbf{w}\cdot\mathbf{x}}}
$$<p>This means the output is a number between 0 and 1 and we can interpret it as a <em>probability</em> of belonging to the class labeled 1. The hypothesis forms a soft boundary in the input space and gives a probability of 0.5 for any input at the center of the region, and approaches 0 or 1 as we move away from the boundary.<p>The process of fitting the weights of this model to minimize loss on a data set is called <strong>logistic regression</strong>. There is no easy closed-form solution to find the optimal value of $\mathbf{w}$ with this model, but we can still use gradient descent. Our hypothesis no-longer outputs just 0 or 1, so we can use the $L_2$ loss function. (pg 726 for full derivation)<p>This eventually gives us the weight update rule for minimizing the loss as:<p>$$
w_i = w_i + \alpha(y - h_\mathbf{w}(x)) \times h_\mathbf{w}(x)(1 - h_\mathbf{w}(x)) \times x_i
$$<p>In the linearly-separable case, logistic regression is slower to convert, but is also more predictable. When data are noisy and nonseparable, logistic regression converges far more quickly and reliably.<h1 id=nonparametric-models>Nonparametric models</h1><p>Linear regression and neural networks use the training data to estimate a <strong>fixed</strong> set of parameters $\mathbf{w}$, which defines our hypothesis $hw(x)$. At that point we can throw away the training data, because they are all summarized by $\mathbf{w}$. A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples, i.e., does not depend on size of knowledge base) is called a <strong>parametric model</strong>. (pg 737).<p>It doesn&rsquo;t matter how much data you throw at a parameteric model; it won&rsquo;t change its mind about how many parameters it needs. When data sets are small, it makes sense to have a strong restriction on the allowable hypotheses, to avoid overfitting. But when there are thousands or millions or billions of examples to learn from, it seems like a better idea to let the data speak for themselves rather than forcing them to speak through a tiny vector of parameters. If the data say that the correct answer is a very wiggly function, we shouldn&rsquo;t restrict ourselfs to linear or slightly wiggly functions.<p>A <strong>nonparameteric model</strong> is one that cannot be characterized by a bounded set of parameters. For example, suppose that each hypothesis we generate simply retains within itself all of the training examples and uses all of them to predict the next example. Such a hypothesis family would be nonparametric because the effective number of parameters is unbounded - it grows with the number of examples. This approach is called <strong>instance-based learning</strong> or <strong>memory-based learning</strong>. The simplest instance-based learning method is <strong>table lookup</strong>. That is, when asked for $h(x)$, see if $x$ is in the table; if it is,return the corresponding $y$. The problem is that it does not generalize well. If $x$ is not in the table, it can only return some default value of $y$.<h1 id=support-vector-machines>Support Vector Machines</h1><p>The <strong>support vector machine</strong> or SVM framework is currently the most-popular approach for &ldquo;off-the-shelf&rdquo; supervised learning: if you don&rsquo;t have any specialized prior-knowledge about a domain, then the SVM is an excellent method to try first. SVMs have three properties that make them attractive:<ol><li><p>SVMs construct a <strong>maximum margin separator</strong> - a decision boundary with the largest-possible distance to example points. This helps them generalize well.<li><p>SVMs create a linear separating hyperplane, but they have the ability to embed the data into a higher-dimensional space, using the so-called <strong>kernel trick</strong>. Often, data that are not linearly separable in the original input space are easily separable in the higher-dimensional space. The high-dimensional linear separator is actually nonlinear in the original space. This means the hypothesis space is greatly expanded over methods that use strictly-linear representations.<li><p>SVMs are a nonparametric method - they retain training examples and potentially need to store them all. On the other hand, in practice they often end up retaining <strong>only a small fraction</strong> of the number of examples - sometimes as few as a small constant times the number of dimensions. Thus SVMs combine the advantages of nonparametric and parametric models: they have the flexibility to represent complex functions, but they are <strong>resistant to overfitting</strong>.</ol><p>SVMs are successful because of one key insight and one neat trick. Assume a binary classification problem with three candidate decision boundaries, each a linear seprator. Each of them is consistent with all the examples, so from the point of view of 0/1 loss, each would be good (pg 745 has examples in figure 18.30). Logistic regression would find some separating line, and its location depends on <em>all</em> the example points. <strong>The key insight</strong> of SVMs is that that examples are <em>more important</em> than others, and that paying attention to them can lead to better generalization.<p>If you consider a separating line which comes close to some number of examples, it certainly minimizes loss and classifies all the examples correctly. But it should make us nervous because so many examples are close to the line; it is entirely possible that other examples will turn out to fall on the other side. SVMs address this issue: instead of minimizing expected <em>empirical loss</em> on the training data, SVMs attempt to minimize expected <em>generalization</em> loss. We don&rsquo;t know where the as-yet-unseen points may fall, but under the <em>probabilistic assumption</em> that they are drawn from the same distribution as the previously-seen examples, there are some arguments from computational learning theory suggesting that we minimize generalization loss by choosing the separator that is farthest away from the examples we have seen so far. We call this separator, the <strong>maximum margin separator</strong>. The <strong>margin</strong> is the area bounded by lines. The distance between these lines is twice the distance from the separator to the nearest example point.<p>Traditionally, SVMs use the convention that class labels are +1 and -1, instead of the +1 and 0 we have seen before. Also, where we put the intercept into the weight vector $\mathbf{w}$ (and a corresponding dummy 1 value into $x_{j, 0}$), SVMs do not do that; they keep the intercept as a separate parameter, $b$. Hence the separator is defined as the set of points:<p>$$
\{ \mathbf{x}\text{ }:\text{ }\mathbf{w}\cdot\mathbf{x} + b = 0 \}
$$<p>It is possible to search the space of $\mathbf{w}$ and $b$ with gradient descent to find the parameters that maximize the margin while correctly classifying all the examples. However, it turns out there is another approach to solving this problem. The alternate representation is called the dual representation, where the optimal solution is found by solving:<p>$$
\underset{\mathbf{\alpha}}{\operatorname{argmax}}\sum\limits_j\alpha_j - \dfrac{1}{2}\sum\limits_{j, k}\alpha_j\alpha_ky_jy_k(\mathbf{x}_j\cdot\mathbf{x}_k)
$$<p>Subject to the constraints $\alpha_j \ge = 0$ and $\sum\nolimits_j\alpha_jy_j = 0$. This is a <strong>quadratic programming</strong> optimization problem that can be solved by many good software packages.<p>Once we have found the vector $\mathbf{\alpha}$, we can get back to $\mathbf{w}$ with the equation $\mathbf{w} = \sum\nolimits_j\alpha_j\mathbf{x}_j$. Or we can stay in the dual representation.<p>The dual representation has some important properties:<ul><li>The expression is convex; it has a single global maximum that can be found efficiently.<li>The data enter the expression only in the form of dot products of pairs of points. This is true of the equation for the separator itself; once the optimal $\alpha_j$ have been calculated, it is:
$$
h(\mathbf{x}) = \operatorname{sign}\left(\sum\limits_j\alpha_jy_j(\mathbf{x}\cdot\mathbf{x}_j) - b\right)
$$<li>The weights $\alpha_j$ associated with each data point are <em>zero</em> except for the <strong>suport vectors</strong> - the points closest to the separator. They are called support vectors because they &ldquo;hold up&rdquo; the separating plane. Because there are usually many fewer support vectors than examples, SVMs gain some of the advantages of parametric models.</ul><p>What if the examples are <em>not</em> linearly separable (pg 746, 747)? We can re-express the data - i.e., we map each input-vector $\mathbf{x}$ to a new vector of feature values, $F(\mathbf{x})$. An example:<p>$$
f_1 = {x_1}^2
$$
$$
f_2 = {x_2}^2
$$
$$
f_3 = \sqrt{2}x_1x_2
$$<p>It turns out now that the data in the new, three-dimensional space defined by the three features is <em>linearly separable</em> by a plane. If the data are mapped into a space of sufficiently high dimension, then they will almost always be linearly separable. For example, four dimensions suffice for linearly separating a circle anywhere in the plane, and five dimensions suffice to linearly separate any ellipse. In general (with some special cases excepted), if we have $N$ data points, then they will always be separable in spaces of $N - 1$ dimensions or more.<p>Now we would not expect to find a linear separator in the input space $\mathbf{x}$, but we can find linear separators in the high-dimensional feature space F($\mathbf{x}$) simply by replacing $\mathbf{x}_j\cdot\mathbf{x}_k$ by $F(\mathbf{x}_j)\cdot F(\mathbf{x}_k)$. This is very straightforward since we can replace $\mathbf{x}$ by $F(\mathbf{x})$ in any learning algorithm. But the dot product has special properties. We can often compute $F(\mathbf{x}_j)\cdot F(\mathbf{x}_k)$ without first computing $F$ for each point. That is through some simple algebra we can show:<p>$$
F(\mathbf{x}_j)\cdot F(\mathbf{x}_k) = {(\mathbf{x}_j\cdot \mathbf{x}_k)}^2
$$<p>That&rsquo;s why the $\sqrt{2}$ is in $f_3$. The expression ${(\mathbf{x}_j\cdot\mathbf{x}_k)}^2$ is called a <strong>kernel function</strong> and is usually written as $K(\mathbf{x}_j, \mathbf{x}_k)$. Hence, we can learn in the higher dimensional space, but we compute only kernel functions rather than the full list of features for each data point. (pg 747 for additional stuff on kernel functions).<p>This is the clever <strong>kernel trick</strong>. Plugging these kernels into the equation (the giant equation for SVM), <em>optimal linear separators can be found efficiently in feature spaces with billions of (or, in some cases, infinitely many) dimensions.</em> The resulting linear separators, when mapped back to the original input space, can correspond to arbitrarily wiggly, nonlinear decision boundaries between the positive and negative examples. A general kernel function is the <strong>polynomial kernel</strong> $K(\mathbf{x}_j, \mathbf{x}_k) = {(1 + \mathbf{x}_j\cdot\mathbf{x}_k)}^d$ and corresponds to a feature space whose dimension is exponential in $d$.<p>What to do with noisy data? If the data are inherently noisy, we may not want a linear separator in some high-dimensional space. Rather, we&rsquo;d like a decision surface in lower-dimensional space that does not cleanly separate the classes, but reflects the reality of the noisy data. That is possible with the <strong>soft margin</strong> classifier, which allows examples to fall on the wrong side of the decision boundary, but assigns them a penalty proportional to the distance required to move them back on the correct side. (pg 748 last para of SVM for other places where kernel function can be applied)<h1 id=ensemble-learning>Ensemble Learning</h1><p>Previous learning methods employ a single hypothesis, chosen from a hypothesis space, to make predictions. The idea of <strong>ensemble learning</strong> methods is to select a collection, or <strong>ensemble</strong> of hypotheses from the hypothesis space and combine their predictions. As an example, during cross-validation we might generate twenty different decision trees, and have them vote on the best classification for a new example.<p>The motivation for ensemble learning is simple. Consider an ensemble of $K = 5$ hypotheses and suppose that we combine their predictions using simple majority voting. For the ensemble to misclassify a new example, <em>at least three of the five hypotheses have to misclassify it</em>. The hope is that this is much less likely than a misclassification by a single hypothesis. The assumption here is that the errors are independent.<p>Another way to view the ensemble idea is to think of it as a generic way of enlarging the hypothesis space. Think of the ensemble itself as a hypothesis, and the new hypothesis space as the set of all possible ensembles constructable from hypotheses in the original space.<h2 id=boosting>Boosting</h2><p>The most widely used ensemble method is called <strong>boosting</strong>. First we need to understand what a <strong>weighted training set</strong> is. In such a training set, each example has an associated weight $w_j \ge 0$. The higher the weight of an example, the higher is the importance attached to it during the learning of a hypothesis. We can easily modify existing algorithms to take the weight into account (where this is not possible, you can create a <strong>replicated training set</strong> where the $j^{th}$ example appears $w_j$ times, using randomization to handle fractional weights).<ol><li>Boosting starts with $w_j = 1$ for all the examples (normal training set).<li>From this, it generates the first hypothesis $h_1$. This hypothesis will classify some of the training examples correctly and some incorrectly.<li>We want the next hypothesis to do better on the misclassified examples, so we <strong>increase</strong> their weights while <strong>decreasing</strong> the weights of the correctly classified examples.<li>From this weighted training-set, we generate the hypothesis $h_2$.<li>The process continues until we have generated $K$ hypotheses. The final ensemble hypothesis is a weighted-majority combination of all $K$ hypothesis, each weighted according to how well it performed on the training set (pg 750 for a visual example).</ol><p>A specific algorithm called ADABOOST has a very important property: if the input learning algorithm $L$ is a <strong>weak learning</strong> algorithm - which means that $L$ always returns a hypothesis with accuracy on the training set that is slightly better than random guessing (i.e., $.5 + \epsilon$ for Boolean classification) - then ADABOOST will return a hypothesis that <em>classifies the training data perfectly</em> for <strong>large enough</strong> $K$. Thus, the algorithm <em>boosts</em> the accuracy of the original learning algorithm on the training data. This result holds no matter how inexpressive the original hypothesis space and no matter how complex the function being learned. For decision trees, we can employ boosting on the <strong>decision stumps</strong>, which are decision trees with just one test, at the root. (pg 750 for details).<p>The finding of the performance of boosting is a surprise since Ockham&rsquo;s razor tells us not to make hypotheses any more complex than necessary. But here we can see that the prediction improves as the ensemble hypothesis gets more complex. There are many explanations. One view is that boosting approximates <strong>Bayesian learning</strong>, which can be shown to be an optimal learning algorithm, and the approximation improves as more hypotheses are added. Another possible explanation is that the addition of further hypotheses enables the ensemble to be <em>more definite</em> in its disctinction between positive and negative examples, which helps it when it comes to classifying new examples.<h2 id=bagging>Bagging</h2><p><strong>Bagging</strong> is the first effective method with ensemble learning for improving the performance of learning algorithms. It combines hypotheses learned from multiple <strong>bootstrap</strong> data sets, each generated by subsampling (random) the original data set.<p>Here we are given a training set of $N$ examples, and a class of learning models (e.g. decision trees, NN, etc.). We train multiple ($K$) models on different samples (data splits) and average their predictions. Prediction is done by averaging the results of $K$ models. The goal is to improve the accuracy of one model by using its multiple copies. The average of misclassification errors on different data splits gives a better estimate of the predictive ability of a learning method. For regression we average, for classification we use a majority vote. (see <a href=http://people.cs.pitt.edu/~milos/courses/cs2750-Spring04/lectures/class23.pdf>here</a> for more information)<h2 id=bagging-vs-boosting>Bagging vs. Boosting</h2><p>Bagging has the advantage of being parallelizable. We can train multiple models at the same time on different samples, since no model depends on the result of the other. In contrast, Boosting and only be done sequentially, because each new hypothesis is generated by training on a data-set, whose weighted data-points have their weights set based on the performance of the <em>previous</em> hypothesis.<h2 id=precision-and-recall>Precision and Recall</h2><ul><li>Precision is the number of correctly classified instances for class X divided by the total number of instances the algorithm classified for class X.<li>Recall is the number of correctly classified instances for class X divided by the total number of instances of class X in the <em>test data</em>.</ul><p>Another way: recall is a measure of how many of the relevant documents were retrieved, while precision is a measure of how many of the retrieved documents were relevant.<p>Given,<ul><li>D = number of documents retrieved.<li>R = number of relevant documents retrieved<li><p>N = number of relevant documents in the collection.<li><p>Recall = R / N<li><p>Precision = R / D</ul><p>In other words:<ul><li>If I have high <strong>recall</strong>, it means out of all the documents <em>that are actually relevant</em>, a high percentage exists in my result set. However, the result set may also contain a large number of irrelevant documents. The focus here is to return <em>as many</em> of the relevant things as necessary, at the cost of also including irrelevant/misclassified things. In other words, the idea here is to <em>minimize false negatives</em> at the cost of <em>maximizing false positives</em>.<li>If I have high <strong>precision</strong>, it means out of all the documents <em>retrieved by my query</em>, a high percentage are actually relevant to my query. However, it does not necessarily mean that <strong>all</strong> relevant documents were retrieved. The focus here is to have <em>as many</em> of the relevant things as necessary <em>within the search result</em>, at the cost of missing out on actually-relevant things (i.e., we may not return all the relevant things). In other words, the idea here is to <em>minimize false positives</em> at the cost of <em>maximizing false negatives</em> (i.e., the things that are left out).</ul><p>Let&rsquo;s have an example where we have 100 documents, 30 of which are relevant to our query.<ul><li>Algorithm #1 retrieves all 100. So Recall = 100%, and Precision = 30%. So it did retrieve all the documents, but also a bunch of irrelevant ones. A person has to manually inspect these documents.<li>Algorithm #2 retrieves 70 documents, including all 30 revelant documents. Recall = 100%, Precision = 70%. This is a little better.<li>Algorithm #3 retrieves 60 documents, including 20 relevant. Recall = 20 / 30 = 67%, Precision = 20 / 50 = 40%.</ul><p>Another one:<ul><li>P = N(relevant items retrieved) / N(total retrieved) = P(relevant | retrieved)<li>R = N(relevant items retrieved) / N(total relevant) = P(retrieved | relevant)</ul><p>Which one is important depends on the circumstances. A typical web surfer looking for documents would like every result on the first page to be relevant (high precision) but not have the interest in knowing, let alone looking at every document that is relevant.<p>In contrast, various professional searchers (paralegals, intelligence analysts) are concerned with trying to get as high recall as possible, and will tolerate fairly low precision results in order to get it.<ul><li>You can always get a recall of 1 (but very low precision) by retrieving all documents for all queries. <strong>Recall</strong> is a <strong>non-decreasing function</strong> of the <strong>number of documents retrieved</strong>.<li>On the other hand, <strong>precision</strong> usually <strong>decreases</strong> as the <strong>number of documents retrieved</strong> is <strong>increased</strong>.</ul><h2 id=learning-terms-and-statements>Learning: Terms and Statements</h2><p><strong>Decision Trees</strong><ul><li><strong>Pruning</strong> is the process of removing irrelevant attribute tests based on their statistical significance (use chi-squared). (pg 706)<li><strong>Pruning</strong> helps combat <strong>overfitting</strong> because the decision-tree algorithms seizes on any pattern it sees. Overfitting is more likely as the hypothesis space and number of input attributes grows.</ul><p><strong>Linear regression</strong><ul><li><strong>Univariate linear regression</strong> has a single variable $x$ and has two weights $w_0$ and $w_1$. The function has the form $y = w_1x + w_0$. There is no risk of <strong>overfitting</strong>.<li>Finding the weights that make the hypothesis function best fit the data is called <strong>linear regression</strong>.<li>We can graph the $L_2$ loss function with respect to the weights and this gives us a <strong>convex</strong> loss function, which means we can use <strong>gradient descent</strong>.<li><strong>Gradient descent</strong> is a hill-climbing algorithm that follows the <strong>gradient</strong> of the function to be optimized to a downhill point where we minimize the loss.<li><strong>Learning rate</strong> is the parameter $\alpha$ in gradient descent. It is also known as <strong>step size</strong>.<li><strong>Batch gradient descent</strong> is when we update the weights after cycling through all the training data for every step. It can be pretty slow, but is guaranteed to converge.<li><strong>Stochastic gradient descent</strong> updates the weight after considering a single training point. We can use this in an online setting. It is faster than the batch version. With a fixed-learning rate however, it <em>does not guarantee convergence</em> and can oscillate about the minimum. We will need a decreasing learning rate or simulated annealing.<li><strong>Multivariate linear regression</strong> has multiple variables, where we simply represent them as a vector $\mathbf{x}_j$.<li>Multivariate linear regression has a risk of <strong>overfitting</strong>.<li>To combat <strong>overfitting</strong>, we use <strong>regularization</strong> which is a cost based on empirical loss and the complexity of the hypothesis.<li>$L_1$ regularization tends to produce a <strong>spare model</strong> where many weights are set to zero (corresponding weights are irrelevant). This is because the intersection between the minimum achievable loss space (contours) and the boundaries defined by set of weights that have complexity lesser than some $c$ intersects on the axes because the boundaries form a diamond shape.<li>$L_2$ does not have the same result since the boundaries of the weight space (with cost less than $c$) is spherical or a circle and so the intersection is along the circumference.</ul><p><strong>Linear classification</strong><ul><li>Lets you create a linear classifier.<li>Output of linear function is passed through a hard threshold that outputs either 0 or 1.a<li>A <strong>decision boundary</strong> is a line or surface that separates the two classes.<li>A linear decision boundary is called a <strong>linear separator</strong>.<li>Data that admit a linear separator are <strong>linearly separable</strong>.<li>The weight-update rule for a linear classifier with a hard threshold pretty much gives us the <strong>perceptron learning rule</strong> which is essentially identical to the update rule for linear regression.<li>For linearly-separable data points, the perceptron learning rule <em>will converge</em> to a perfect linear separator.<li>In general, the perceptron rule <em>may not converge</em> to a stable solution for a <strong>fixed</strong> learning rate $\alpha$. But if we have a decaying learning rate as $O(\frac{1}{t})$ then it will converge to a minimum-error solution when examples are presented in random sequence. Finding a minimum-error solution is also NP-hard.</ul><p><strong>Logistic Regression</strong><ul><li>Lets you create a linear classifier.<li>Output of a linear function can be passed through a threshold function to create a classifier. We use a smooth threshold function here.<li>The process of fitting weights of a model to minimize loss on a data set is called <strong>logistic regression</strong>.<li>We use the logistic function to have a smooth and continuous threshold so that it is differentiable.<li>The output also expresses a &ldquo;belief&rdquo; in the probability of the classification since the value is between 0 and 1.<li>With linearly-separable data, logistic regression is <em>slower</em> to converge but behaves more <em>predictably</em>.<li>It converges far more quickly and reliably with <strong>noisy</strong> and <strong>nonseparable</strong> data.</ul><p><strong>Non-parametric vs Parametric</strong><ul><li><strong>Parametric model</strong>: A learning model that summarizes data with a set of parameters of fixed-size (independent of the number of training examples) is called a <strong>parametric model</strong>.<li><strong>Non-parametric model</strong>: A learning model that cannot be characterized by a bounded set of parameters; it retains information about the training examples and uses <em>all</em> of them to predict the next example. An example is SVM, which retains this through the support vectors.</ul><p><strong>SVM</strong><ul><li><strong>Soft margin classifier</strong>: Allows examples to fall on the wrong side, but assigns them a penalty proportional to the distance required to move them to the correct side.<li><strong>Kernel trick</strong>: Plugging kernels into the optimal-solution equation for SVM (it is a quadratic programming problem), lets us efficiently find optimal linear-separators in feature spaces with billions of (or, in some cases, infinitely many) dimensions. In a lower dimensional space, these separators can appear arbitrarily wiggly. So you simply replace the dot product by this kernel function, which is applied to pairs of input data to evaluate dot products in some corresponding feature space. Using this we can find linear separates in the higher-dimensional feature space $F(\mathbf{x})$.<li><strong>Maximum margin separator</strong>: A decision boundary with the largest possible distance to example points. This helps them generalize well.<li><strong>What a kernel does</strong>: It lets us know how we can weight each example. A kernel function looks like a bump. For example, a parabola with intercepts on the x axis at -5 and 5, centered at 0. The weight is the highest at the center and 0 at -5 and 5. A kernal function should be symmetric around 0 and have a maximum at 0. The area must be bounded as we go to positive or negative infinity. Research suggests the shape doesn&rsquo;t matter much, but the width does matter. If the width is too wide, we will get underfitting and if they are too narrow we&rsquo;ll get overfitting.<li><strong>Support vectors</strong>: The points closest to the decision boundary that &ldquo;hold up&rdquo; the separating hyperplane. The weights here are zero for every other data point except for the support vectors. Hence SVM retain some of the advantages of parametric models since they don&rsquo;t retain <em>all</em> training examples (just the ones that matter most).</ul><h1 id=probability-theory>Probability Theory</h1><p>Agents need to handle <strong>uncertainty</strong>, whether due to partial observability, nondeterminism, or a combination of the two. We can use probability theory for this.<p>The set of all possible worlds is the <strong>sample space</strong>. $\Omega$ is used to refer to the sample space, and $\omega$ refers to the elements of the space.<p>A fully specified <strong>probability model</strong> associates a numerical probability $P(\omega)$ with each possible world. The total probability of the set of possible worlds is 1:<p>$$
0 \le P(\omega) \le 1 \forall\ \omega\text{ and }\sum\limits_{\omega \in \Omega}\omega = 1
$$<p>Probabilistic assertions and queries are not usually about particular possible worlds, but sets of them. For example, we might be interested in the cases where two dice add up to 11. In probability theory, these sets are called <strong>events</strong>. In AI, the sets are always described by <strong>propositions</strong> in a formal language. The probability associated with a proposition is defined to be the sum of pobabilities of the worlds in which it holds:<p>$$
\text{For any proposition }\Phi\text{, }P(\Phi) = \sum\limits_{\omega \in \Phi}P(\omega)
$$<p>Probabilities such as $P(Total = 11)$ and $P(doubles)$ are called <strong>unconditional</strong> or <strong>prior probabilities</strong>. In other cases we are interested in the <strong>conditional</strong> or <strong>posterior</strong> probability. For example $P(doubles\ |\ Die_1 = 5)$. Conditional probabilities are defined in terms of unconditional probabilities as follows:<p><strong>Definition of Conditional Probability</strong>:
$$
P(a\ |\ b) = \dfrac{P(a \land b)}{P(b)}
$$<p><strong>Conditional Probability in the form of the Product Rule</strong>
$$
P(a \land b) = P(a\ |\ b)P(b)
$$<p>Variables in probability theory are called <strong>random variables</strong> and their names begin with an uppercase letter. Every random variable has a <strong>domain</strong> - the set of possible values it can take on. For example, a Boolean random variable can take the values $\{true, false\}$. The proposition that doubles are rolled can be written as $Doubles = true$.<p>By convention, propositions of the form $A = true$ are simply abbreviated as $a$ whereas $A = false$ is abbreviated as $\lnot a$.<p>Variables can have infinite domains as well. For any variable with an ordered domain, inequalities are allowed as well such as $NumberOfAtomsInUniverse \le 10^{70}$.<p>Sometimes we will want to talk about the probabilities of all the possible values of a random variable. We could write:<ul><li>$P(Weather = sunny) = 0.6$<li>$P(Weather = rain) = 0.1$<li>$P(Weather = cloudy) = 0.29$<li>$P(Weather = snow) = 0.01$</ul><p>But as an abbreviation, we can have:<p>$$
\mathbf{P}(Weather) = \langle 0.6, 0.1, 0.29, 0.01 \rangle
$$<p>The bold $\mathbf{P}$ indicates that the result is a vector of numbers, where we assume a pre-defined ordering on the domain of $Weather$. We say that the $\mathbf{P}$ statement defines a <strong>probability distribution</strong> for the random variable $Weather$. The $\mathbf{P}$ notation is also used for conditional distributions: $\mathbf{P}(X\ |\ Y)$ gives the values of $P(X = x_i\ |\ Y = y_i)$ for each possible $i, j$ pair.<p>For continuous variables, it is not possible to write out the entire distribution as a vector, because there are infinitely many values. Instead, we can define the probability that a random variable takes on some value $x$ as a parameterizing function of $x$. For example:<p>$$
P(NoonTemp = x) = Uniform_{[18C, 26C]}(x)
$$<p>expresses the belief that the temperature at noon is distributed uniformly between 18 and 26. We call this function a <strong>probability density function</strong>.<p>Saying that the probability density is uniform from 18C to 26C means that there is a 100% chance that the temperature will fall somewhere in that 8C-wide region and a 50% chance that it will fall in any 4C-wide region, and so on. (pg 487)<p>In addition to distributions on single variables, we need notation for distributions on multiple cariables. Commas are used for this. For example, $\mathbf{P}(Weather, Cavity)$ denotes the probabilities of all combinations of the values of $Weather$ and $Cavity$ (essentially the cross product of both sets). This is called the <strong>joint probability distribution</strong>. We can mix variables with and without values; $\mathbf{P}(sunny, Cavity)$ would be a two-element vector giving the probabilities of a sunny day with a cavity and a sunny day with no cavity. The $\mathbf{P}$ notation makes certain expressions much more concise that they otherwise might be. For example, the product rules for all possible values of $Weather$ and $Cavity$ can be written as a single equation:<p>$$
\mathbf{P}(Weather, Cavity) = \mathbf{P}(Weather\ |\ Cavity)\mathbf{P}(Cavity)
$$<p>This is much more concise than writing 4 x 2 = 8 equations.<p>As a degenerate case, $\mathbf{P}(sunny, cavity)$ has no variables and thus is a one-element vector that is the probability of a sunny day with a cavity, which could also be written as $P(sunny, cavity)$ or $P(sunny \land cavity)$. We will sometimes use $\mathbf{P}$ notation to derive results about invidividual $P$ values, and when we say $\mathbf{P}(sunny) = 0.6$, it is really an abbreviation for &ldquo;$\mathbf{P}(sunny)$ is the one-element vector $\langle 0.6\rangle$, which means that $P(sunny) = 0.6$.&rdquo;<p>A probability model is completely determined by the joint distribution for all of the random variables - this is the full joint probability distribution. For example, if the random variables are $Cavity$, $Toothache$, and $Weather$, then the full joint distribution is given by $\mathbf{P}(Cavity, Toothache, Weather)$. This would be a table with 2 x 2 x 4 = 16 entries because we look at every combination (again, just the cross product of the three sets).<h2 id=probability-axioms-and-their-reasonableness>Probability axioms and their reasonableness</h2><p><strong>Inclusion-exclusion principle</strong>
$$
P(a \lor b) = P(a) + P(b) - P(a \land b)
$$<h2 id=inference-using-full-joint-distributions>Inference Using Full Joint Distributions</h2><p>The method of <strong>probabilistic inference</strong> is the computation of posterior probabilities for query propositions given observed evidence.<p>A particularly common task is to extract the distribution over some subset of variables or a single variable. For example, adding the entries (pg 492 fig 13.3) in the first row gives the unconditional or <strong>marginal probability</strong> of $cavity$:<p>$$
P(cavity) = 0.108 + 0.012 + 0.072 + 0.008 = 0.2
$$<p>This process is called <strong>marginalization</strong> or <strong>summing out</strong>, because we sum up the probabilities for each possible value of the other variables, thereby taking them out of the equation. We can write the following general marginalization rule for any sets of variables $\mathbf{Y}$ and $\mathbf{Z}$:<p>$$
\mathbf{P}(\mathbf{Y}) = \sum\limits_{\mathbf{z} \in \mathbf{Z}}\mathbf{P}(\mathbf{Y}, \mathbf{z})
$$<p>In our example, we basically did:<p>$$
\mathbf{P}(Cavity) = \sum\limits_{\mathbf{z} \in \{Catch, Toothache\}}\mathbf{P}(Cavity, \mathbf{z})
$$<p>A variant of this rule involves conditional probabilities instead of joint probabilities, using the product rule:<p>$$
\mathbf{P}(Y) = \sum\limits_{\mathbf{z} \in \mathbf{Z}}\mathbf{P}(\mathbf{Y}\ |\ \mathbf{z})P(\mathbf{z})
$$<p>This rule is called <strong>conditioning</strong>. Marginalization and conditioning turn out to be useful rules for all kinds of derivations involving probability expressions.<p>Conditional probabilities can be found by first using the equation:<p>$$
P(a\ |\ b) = \dfrac{P(a \land b)}{P(b)}
$$<p>For $\mathbf{P}(Cavity\ |\ toothache)$, we basically have $P(cavity\ |\ toothache) + P(\lnot cavity\ |\ toothache)$. Both will have as denominator $P(toothache)$, which remains constant no matter what value of $Cavity$ we calculate. This means it ends up being a <strong>normalization</strong> constant for the distribution $\mathbf{P}(Cavity\ |\ toothache)$, ensuring that it adds up to 1. We will use $\alpha$ to denote such constants. Hence we can write:<p>$$
\mathbf{P}(Cavity\ |\ toothache) = \alpha\mathbf{P}(Cavity, toothache) = \alpha[\mathbf{P}(Cavity, toothache, catch) + \mathbf{P}(Cavity, toothache, \lnot catch)]
$$<p>We end up with:<p>$$
\alpha[\langle P(cavity, toothache, catch), P(\lnot cavity, toothache, catch)\rangle + \langle P(cavity, toothache, \lnot catch), P(\lnot cavity, toothache, \lnot catch)\rangle]
$$
$$
= \alpha[\langle 0.108, 0.016\rangle + \langle 0.012, 0.064\rangle] = \alpha\langle 0.12, 0.08 \rangle = \langle 0.6, 0.6 \rangle
$$<p>Here, even if we don&rsquo;t know $P(toothache)$, we can forget about $\frac{1}{P(toothache)}$ and just add up the values getting $0.12$ and $0.08$. The relative proprtions are right and so we can nromalize by dividing each of them by 0.12 + 0.08, which gives us the correct probabilities.<p>We can now extract a general inference procedure. If we have a query with a single variable $X$ (like $Cavity$), let $\mathbf{E}$ be the list of evidence variables (just $Toothache$ in the example), and let $\mathbf{e}$ be the observed values for them, and let $\mathbf{Y}$ be the remaining unobserved variables (just $Catch$ in the example). If the query is $\mathbf{P}(X\ |\ \mathbf{e})$, we can evaluate it as:<p>$$
\mathbf{P}(X\ |\ \mathbf{e}) = \alpha\mathbf{P}(X, \mathbf{e}) = \alpha\sum\limits_{y}\mathbf{P}(X, \mathbf{e}, \mathbf{y})
$$<p>where the summation is over all possible $\mathbf{y}$s (i.e., all possible combinations of values of the unobserved variables $\mathbf{Y}$). Notice that $X$, $\mathbf{E}$, and $\mathbf{Y}$ constitute the complete set of variables for the domain and so $\mathbf{P}(X, \mathbf{e}, \mathbf{y})$ is simply a subset of probabilities from the full joint distribution.<p>Given a full joint distribution, we can answer probabilistic queries with this equation but it does not scale well. Consider a domain with $n$ boolean variables. It needs an input table of size $O(2^n)$ and takes $O(2^n)$ time to process the table.<h2 id=independence>Independence</h2><p>What if we add another variable, $Weather$, to the toothache table? The full joint distribution is now $\mathbf{P}(Toothache, Catch, Cavity, Weather)$. What relationship can we infer? For example, how are $P(toothache, catch, cavity, cloudy)$ and $P(toothache, catch, cavity)$ related? Using the product rule:<p>$$
P(toothache, catch, cavity, cloudy) = P(cloudy\ |\ toothache, catch, cavity)P(toothache, catch, cavity)
$$<p>The weather has no bearing on the dental variables, so we can say:<p>$$
P(cloudy\ |\ toothache, catch, cavity) = P(cloudy)
$$<p>From this, we can deduce:<p>$$
P(toothache, catch, cavity, cloudy) = P(cloudy)P(toothache, catch, cavity)
$$<p>A similar equation exists for every entry in $\mathbf{P}(Toothache, Catch, Cavity, Weather)$ and therefore we can write the general equation:<p>$$
\mathbf{P}(Toothache, Catch, Cavity, Weather) = \mathbf{P}(Toothache, Catch, Cavity)\mathbf{P}(Weather)
$$<p>This property is called <strong>independence</strong> (also <strong>marginal independence</strong> and <strong>absolute independence</strong>). In particular, the weather is independent of one&rsquo;s dental problems. Independence between propositions $a$ and $b$ can be written as:<p>$$
P(a\ |\ b) = P(a) \text{ or } P(b\ |\ a) = P(b) \text{ or } P(a \land b) = P(a)P(b)
$$<p>Independence between variables $X$ and $Y$ can be written as follows:<p>$$
\mathbf{P}(X\ |\ Y) = \mathbf{P}(X)\text{ or }\mathbf{P}(Y\ |\ X) = \mathbf{P}(Y)\text{ or }\mathbf{P}(X, Y) = \mathbf{P}(X)\mathbf{P}(Y)
$$<h1 id=bayes-rule-and-its-use>Bayes&rsquo; Rule and its Use</h1><p>Bayes rule is:<p>$$
P(b\ |\ a) = \dfrac{P(a\ |\ b)P(b)}{P(a)}
$$<p>In the more general case, for multivalued variables:<p>$$
\mathbf{P}(Y\ |\ X) = \dfrac{\mathbf{P}(X\ |\ Y)\mathbf{P}(Y)}{\mathbf{P}(X)}
$$<p>We also have a more general version conditionalized on some background evidence $\mathbf{e}$:<p>$$
\mathbf{P}(Y\ |\ X, \mathbf{e}) = \dfrac{\mathbf{P}(X\ |\ Y, \mathbf{e})\mathbf{P}(Y\ |\ \mathbf{e})}{\mathbf{P}(X\ |\ \mathbf{e})}
$$<h2 id=applying-bayes-rule-the-simple-case>Applying Bayes&rsquo; rule: The simple case</h2><p>Often we perceive as evidence the <em>effect</em> of some unknown <em>cause</em> and we would like to determine that cause. (example: a stiff neck (effect) is caused by meningitis (cause)) In that case, Bayes&rsquo; rule becomes:<p>$$
P(cause\ |\ effect) = \dfrac{P(effect\ |\ cause)P(cause)}{P(effect)}
$$<p>The conditional probability $P(effect\ |\ cause)$ quantifies the relationship in the <strong>causal</strong> direction, whereas $P(cause\ |\ effect)$ describes the <strong>diagnostic</strong> direction. In medical diagnosis, we often have conditional probabilities on causal relationships. For example, the doctor knows $P(symptoms\ |\ disease)$ and want to derive a diagnosis $P(disease\ |\ symptoms)$.<p>For example, what if the doctor wants to know if the disease is meningitis given some symptoms?<p>Let&rsquo;s say:<ul><li>$P(\text{stiff neck}\ |\ \text{meningitis}) = 0.7$<li>$P(\text{meningitis}) = <sup>1</sup>&frasl;<sub>50000</sub>$<li>$P(\text{stiff neck}) = 0.01$</ul><p>Then $P(m\ |\ s) = \dfrac{P(s\ |\ m)P(m)}{P(s)} = \dfrac{0.7 \times (<sup>1</sup>&frasl;<sub>50000</sub>)}{0.01} = 0.0014$<p>Recall that we can avoid accessing the prior probability of the evidence ($P(s)$ here) by instead computing a posterior probability for each value of the query variable (here $m$ and $\lnot m$) and then normalizing the results. We can apply the same process using Bayes rule:<p>$$
\mathbf{P}(M\ |\ s) = \alpha\langle\mathbf{P}(s\ |\ m)P(m), P(s\ |\ \lnot m)P(\lnot m)\rangle
$$<p>To use this approach, we need to estimate $P(s\ |\ \lnot m)$ instead of $P(s)$. This information may be easier to obtain than $P(s)$ or it may be harder. The idea is that we have an alternate representation that we can use depending on how difficult it is to get some information. The general form of the Bayes&rsquo; rule with normalization is:<p>$$
\mathbf{P}(Y\ |\ X) = \alpha\mathbf{P}(X\ |\ Y)\mathbf{P}(Y)
$$<h2 id=using-bayes-rule-combining-evidence>Using Bayes&rsquo; rule: Combining evidence</h2><p>What if we want to answer questions based on multiple pieces of evidence?<p>For example, what if we wnat to answer $\mathbf{P}(Cavity\ |\ toothache \land catch)$? We know that this approach does not scale up to large numbers of variables. We can try using Bayes&rsquo; rule to reformulate it:<p>$$
\mathbf{P}(Cavity\ |\ toothache \land catch) = \alpha\mathbf{P}(toothache \land catch\ |\ Cavity)\mathbf{P}(Cavity)
$$<p>For this to work we need conditional probabilities of $toothcahe$ AND $catch$ for each value of $Cavity$. However, this does not scale up. So what do we do? We can refine the earlier notion of <strong>independence</strong> to get the notion of <strong>conditional independence</strong>.<p>For example, it would be nice if $Toothache$ and $Catch$ were independent. They are not, because if the probe catches in the tooth, then it is likely that the tooth has cavity, and the cavity causes a toothache. However, these variables <em>are</em> independent <em>given the presence or absence of a cavity</em>. Each is directly caused by a cavity, but neither has a direct effect on the other (toothache is caused by the nerves signalling pain, and a catch is caused by the dentist&rsquo;s tool catching on the tooth).<p>Hence, we can say:<p>$$
\mathbf{P}(toothache \land catch\ |\ Cavity) = \mathbf{P}(toothache\ |\ Cavity)\mathbf{P}(catch\ |\ Cavity)
$$<p>This equation expresses the <strong>conditional independence</strong> of $toothache$ and $catch$ given $Cavity$. This means we can plug it into the earlier equation)<p>$$
\mathbf{P}(Cavity\ |\ toothache \land catch) = \alpha\mathbf{P}(toothache\ |\ Cavity)\mathbf{P}(catch\ |\ Cavity)\mathbf{P}(Cavity)
$$<p>The general definition of <strong>conditional independence</strong> of two variables $X$ and $Y$ given a third variable $Z$, is<p>$$
\mathbf{P}(X, Y\ |\ Z) = \mathbf{P}(X\ |\ Z)\mathbf{P}(Y\ |\ Z)
$$<p>Hence:<p>$$
\mathbf{P}(Tootache, Catch\ |\ Cavity) = \mathbf{P}(Toothache\ |\ Cavity)\mathbf{P}(Catch\ |\ Cavity)
$$<p>As with absolute independence, we can have the equivalent forms:<p>$$
\mathbf{P}(X\ |\ Y, Z) = \mathbf{P}(X\ |\ Z)\text{ and }\mathbf{P}(Y\ |\ X, Z) = \mathbf{P}(Y\ |\ Z)
$$<p>This means you can decompose the full joint distribution into smaller pieces.<p>For example:<p>$$
\mathbf{P}(Toothache, Catch, Cavity) = \mathbf{P}(Toothache, Catch\ |\ Cavity)\mathbf{P}(Cavity)\text{ (product rule)}
$$
$$
= \mathbf{P}(Toothache\ |\ Cavity)\mathbf{P}(Catch\ |\ Cavity)\mathbf{P}(Cavity)
$$<p>The idea is that for $n$ symptoms that are all conditionally independent given $Cavity$, the size of the representation grows as $O(n)$ instead of $O(2^n)$.<p>Conditional independence assertions can allow probabilistic systems to scale up; morever, they are much more commonly available than absolute independence assertions. (pg 499 for why this helps - more details). The decomposition of large probabilistic domains into weakly connected subsets through conditional independence is one of the most important developments in the recent history of AI. The dentistry example illustrates a commonly occurring pattern in which a single cause directly influences a number of effects, all of which are conditionally independent, given the cause. The full joint distribution can be written as:<p>$$
\mathbf{P}(Cause, Effect_1, \dotso, Effect_N) = \mathbf{P}(Cause)\prod_i\mathbf{P}(Effect_i\ |\ Cause)
$$<p>Such a probability distribution is called a <strong>naive Bayes</strong> model. This is sometimes called a <strong>Bayesian classifier</strong>, which has prompted true Bayesians to call it the <strong>idiot Bayes</strong> model.<p>The Bayes classifier is the function that assigns a class label $\hat{y} = C_k$ for some $k$ as follows:<p>$$
\hat{y} = \underset{k \in \{1, \dotso, K\}}{\operatorname{argmax}}P(C_k)\prod_{i = 1}^nP(x_i\ |\ C_k)
$$<p><img src=http://upload.wikimedia.org/math/3/5/e/35e94f179a666c4b5892a11de1b3b29e.png alt="Bayes classifier"><h1 id=homework-question>Homework Question</h1><p>We know in general that the definition of conditional probability is as follows:<p>$$
\mathbf{P}(A | B) = \frac{\mathbf{P}(A, B)}{\mathbf{P}(B)}
$$<p>We have to show that the statement of conditional independence:<p>$$
\mathbf{P}(X, Y | Z) = \mathbf{P}(X | Z)\mathbf{P}(Y | Z)
$$<p>is equivalent to the statements:<p>$\mathbf{P}(X | Y, Z) = \mathbf{P}(X | Z)$ and $\mathbf{P}(Y | X, Z) = \mathbf{P}(Y | Z)$<p>Let us first try to show that $\mathbf{P}(X | Y, Z) = \mathbf{P}(X | Z)$. We can do this by starting with the statement of conditional independence and then expanding $\mathbf{P}(X, Y | Z)$ and $\mathbf{P}(Y | Z)$ using the definition of conditional probability:<p>$$
\mathbf{P}(X, Y | Z) = \mathbf{P}(X | Z)\mathbf{P}(Y | Z)
$$
$$
\frac{\mathbf{P}(X, Y, Z)}{\mathbf{P}(Z)} = \frac{\mathbf{P}(X | Z)\mathbf{P}(Y, Z)}{\mathbf{P}(Z)}
$$
$$
\mathbf{P}(X, Y, Z) = \mathbf{P}(X | Z)\mathbf{P}(Y, Z)
$$<p>We now need to get a good representation for $\mathbf{P}(X, Y, Z)$. We can do this by starting with $\mathbf{P}(X | Y, Z)$ and then expanding it by using the definition of conditional probability:<p>$$
\mathbf{P}(X | Y, Z) = \frac{\mathbf{P}(X, Y, Z)}{\mathbf{P}(Y, Z)}
$$
$$
\implies\mathbf{P}(X, Y, Z) = \mathbf{P}(X | Y, Z)\mathbf{P}(Y, Z)
$$<p>Substituting this back into our original equation, we get:<p>$$
\mathbf{P}(X, Y, Z) = \mathbf{P}(X | Z)\mathbf{P}(Y, Z)
$$
$$
\mathbf{P}(X | Y, Z)\mathbf{P}(Y, Z) = \mathbf{P}(X | Z)\mathbf{P}(Y, Z)
$$
$$
\mathbf{P}(X | Y, Z) = \mathbf{P}(X | Z)
$$<p>This shows that the statement of conditional independence $\mathbf{P}(X, Y | Z) = \mathbf{P}(X | Z)\mathbf{P}(Y | Z)$ is equivalent to the statement $\mathbf{P}(X | Y, Z) = \mathbf{P}(X | Z)$, since we were able to derive the latter from the former.<p>Now we need to show that the statement of conditional independence is also equivalent to the statement $\mathbf{P}(Y | X, Z) = \mathbf{P}(Y | Z)$. We can use a similar approach as before. However, this time we will expand $\mathbf{P}(X | Z)$ instead:<p>$$
\mathbf{P}(X, Y | Z) = \mathbf{P}(X | Z)\mathbf{P}(Y | Z)
$$
$$
\frac{\mathbf{P}(X, Y, Z)}{\mathbf{P}(Z)} = \frac{\mathbf{P}(X, Z)\mathbf{P}(Y | Z)}{\mathbf{P}(Z)}
$$
$$
\mathbf{P}(X, Y, Z) = \mathbf{P}(X, Z)\mathbf{P}(Y | Z)
$$<p>Again, we need to find a good representation for $\mathbf{P}(X, Y, Z)$. This time we can get it by expanding $\mathbf{P}(Y | X, Z)$:<p>$$
\mathbf{P}(Y | X, Z) = \frac{\mathbf{P}(X, Y, Z)}{\mathbf{P}(X, Z)}
$$
$$
\implies\mathbf{P}(X, Y, Z) = \mathbf{P}(Y | X, Z)\mathbf{P}(X, Z)
$$<p>Substituting this back into our equation, we get:<p>$$
\mathbf{P}(X, Y, Z) = \mathbf{P}(X, Z)\mathbf{P}(Y | Z)
$$
$$
\mathbf{P}(Y | X, Z)\mathbf{P}(X, Z) = \mathbf{P}(X, Z)\mathbf{P}(Y | Z)
$$
$$
\mathbf{P}(Y | X, Z) = \mathbf{P}(Y | Z)
$$<p>Hence, we were also able to derive $\mathbf{P}(Y | X, Z) = \mathbf{P}(Y | Z)$ by starting with the statement of conditional independence. Therefore, we can now say that the statement of conditional independence:<p>$$
\mathbf{P}(X, Y | Z) = \mathbf{P}(X | Z)\mathbf{P}(Y | Z)
$$<p>is equivalent to the statements:<p>$\mathbf{P}(X | Y, Z) = \mathbf{P}(X | Z)$ and $\mathbf{P}(Y | X, Z) = \mathbf{P}(Y | Z)$<h1 id=probability-identities>Probability Identities</h1><p>Identities you can use.<h2 id=inclusion-exclusion-principle>Inclusion-exclusion principle</h2><p>$$
P(a \lor b) = P(a) + P(b) - P(a \land b)
$$<h2 id=conditional-probability>Conditional Probability</h2><p>$$
P(a\ |\ b) = \dfrac{P(a \land b)}{P(b)}\text{ using random-variable instances (represents a particular value)}
$$<p>$$
\mathbf{P}(A\ |\ B) = \dfrac{\mathbf{P}(A, B)}{\mathbf{P}(B)}\text{ using random-variables (represents an entire domain of values)}
$$<h2 id=conditional-probability-as-product-rule>Conditional Probability as Product Rule</h2><p>$$
P(a \land b) = P(a\ |\ b)P(b)\text{ using random-variable instances (represents a particular value)}
$$<p>$$
\mathbf{P}(A, B) = \mathbf{P}(A\ |\ B)\mathbf{P}(B)\text{ using random-variables (represents an entire domain of values)}
$$<h2 id=marginalization>Marginalization</h2><p>This is how you compute the marginal probabilities (in general). $\mathbf{Y}$ and $\mathbf{Z}$ are <em>sets</em> of random-variables. For example, $\mathbf{Z} = \{Catch, Toothache\}$. A $\mathbf{z} \in \mathbf{Z}$ is actually a combination of variable-values. That is $\mathbf{z} \in z_1 \times z_2 \times \dotso \times z_{\left\lvert Z \right\rvert}$, which means that $\mathbf{z}$ is an element in the set of cross products of the values of all variables in $\mathbf{Z}$.<p>$$
\mathbf{P}(\mathbf{Y}) = \sum\limits_{\mathbf{z} \in \mathbf{Z}}\mathbf{P}(\mathbf{Y}, \mathbf{z})
$$<h2 id=conditioning>Conditioning</h2><p>Just use the product rule to get:<p>$$
\mathbf{P}(\mathbf{Y}) = \sum\limits_{\mathbf{z}}\mathbf{P}(\mathbf{Y}\ |\ \mathbf{z})P(\mathbf{z})
$$<h2 id=independence-1>Independence</h2><p>Also known as <strong>marginal independence</strong> and <strong>absolute independence</strong>.<p>$$
P(a\ |\ b) = P(a) \text{ or } P(b\ |\ a) = P(b) \text{ or } P(a \land b) = P(a)P(b)
$$<p>For random variables instead of values (or propositions):<p>$$
\mathbf{P}(X\ |\ Y) = \mathbf{P}(X) \text{ or } \mathbf{P}(Y\ |\ X) = \mathbf{P}(Y) \text{ or } \mathbf{P}(X \land Y) = \mathbf{P}(X)\mathbf{P}(Y)
$$<h2 id=bayes-rule>Bayes Rule</h2><p>Bayes rule is:<p>$$
P(b\ |\ a) = \dfrac{P(a\ |\ b)P(b)}{P(a)}
$$<p>In the more general case, for multivalued variables:<p>$$
\mathbf{P}(Y\ |\ X) = \dfrac{\mathbf{P}(X\ |\ Y)\mathbf{P}(Y)}{\mathbf{P}(X)}
$$<p>We also have a more general version conditionalized on some background evidence $\mathbf{e}$:<p>$$
\mathbf{P}(Y\ |\ X, \mathbf{e}) = \dfrac{\mathbf{P}(X\ |\ Y, \mathbf{e})\mathbf{P}(Y\ |\ \mathbf{e})}{\mathbf{P}(X\ |\ \mathbf{e})}
$$<h2 id=conditional-independence>Conditional Independence</h2><p>This is when two variables are independent given a third variable.<p>$$
\mathbf{P}(X, Y\ |\ Z) = \mathbf{P}(X\ |\ Z)\mathbf{P}(Y\ |\ Z)
$$<p>As with absolute independence, we have the equivalent forms:<p>$$
\mathbf{P}(X\ |\ Y, Z) = \mathbf{P}(X\ |\ Z)\text{ and }\mathbf{P}(Y\ |\ X, Z) = \mathbf{P}(Y\ |\ Z)
$$<h1 id=game-theory>Game Theory</h1><h2 id=game-theory-terms>Game Theory: Terms</h2><ul><li><strong>Agent Design</strong>: Game theory can analyze the agent&rsquo;s decisions and compute the expected utility for each decision (while assuming that the other agents are acting optimally according to game theory).<li><strong>Mechanism Design</strong>: When an environment is inhabited by many agents, it might be possible to define the rules of the environment (i.e., the game that the agents must play) so that the collective good of all agents is maximized when each agent adopts the game-theoretic solution that maximizes its own utility (make it so that there is a dominant strategy).<li><strong>Strategic Form</strong> or <strong>Normal Form</strong>: A matrix representation that shows the outcome of every combination of pure-strategies of players one and two.<li><strong>Pure strategy</strong>: A deterministic policy; a strategy played with a probability of 1.<li><strong>Mixed strategy</strong>: A probability distribution over all pure-strategies. Each pure-strategy has an assigned probability.<li><strong>Strategy profile</strong>: Assignment of a strategy to <em>each</em> player.<li><strong>Dominant strategy</strong>: A strategy $s$ that dominates $s&rsquo;$ for every choice of strategies by the other player.<li><strong>Strongly dominant strategy</strong>: A strategy $s$ <strong>strongly dominates</strong> $s&rsquo;$ if the outcome for $s$ is better for the player than the outcome for $s&rsquo;$ for every choice of strategies by the other player.<li><strong>Weakly dominant strategy</strong>: A strategy $s&rsquo;$ <strong>weakly dominates</strong> $s&rsquo;$ if the outcome for $s$ is better on <em>at least one</em> strategy profile (i.e., one combination of p1 and p2 pure-strategies) and no worse on any other.<li><strong>Pareto optimal</strong>: An outcome <strong>pareto optimal</strong> if there is <em>no other outcome</em> that all players would prefer. The outcome is such that there is <em>no other outcome</em> that makes every player <em>at least as well off</em> and at least one player <em>strictly better off</em>, that is, a <strong>pareto optimal</strong> outcome cannot be improved upon <em>without hurting at least one player</em>. A <strong>Nash Equilibrium</strong> is not necessarily <strong>pareto optimal</strong> because player&rsquo;s payoffs can be increased.<li><strong>Pareto dominated</strong>: An outcome is <strong>pareto dominated</strong> by <em>another</em> outcome, if all players would prefer the <em>other</em> outcome.<li><strong>Dominant strategy equilibrium</strong>: When each player has a dominant strategy, the combination of those strategies is called a <strong>dominant strategy equilibrium</strong>.<li><strong>Equilibrium strategy profile</strong>: In general, a strategy profile forms an <strong>equilibrium</strong> if <em>no</em> player can benefit by switching strategies, given that every other player sticks with the same strategy (i.e., assume there is some strategy-profile $P = \langle p_1, p_2 \rangle$ that is an equilibrium. If player 2 cannot do better by switching to any other strategy while player 1 sticks with $p_1$, and if the same situation applies if you switch the places of players 1 and 2, then the strategy profile is an <strong>equilibrium</strong>).<li><strong>Repeated games</strong>: Games with multiple move (the simplest kind of multiple-move game). Players face the same choice repeatedly.<li><strong>Sequential games</strong>: Games with a sequence of turns that need not all be the same.<li><strong>Extensive form</strong>: A game tree representing a sequential game.<li><strong>Stochastic games</strong>: Games that have an element of chance (like backgammon). A distinguished player called $chance$ is added, and it can take random actions. $Chance&rsquo;s$ &ldquo;strategy&rdquo; is part of the definition of the game and is specified as a probability distribution over actions.<li><strong>Perfect recall</strong>: Players always remember their <em>own</em> previous actions.<li><strong>Belief states</strong>: In a partially observable game, we don&rsquo;t know what the other player might play. Hence the game tree is created over the space of <strong>belief states</strong>, i.e., different situations: I have an Ace and the other guy has a King; I have an Ace and the other guy also has an Ace. These are also called <strong>information sets</strong>.<li><strong>Ascending bid</strong> or <strong>English auction</strong>: Auctioneer sets a reserve bid $b_min$. If someone bids that amount, then the auctioneer asks for $b_min + d$ and continues. The auction ends when no one is willing to bid anymore, after which the last player wins the bid.<li><strong>Efficient auction</strong>: An auction is efficient if the goods go to the agent who values them most. We can also consider revenue to the auctioneer as well (discourage collusion).<li><strong>Strategy-proof mechanism</strong>: A mechanism where agents have a dominant strategy. Hence, they are incentivized to play that.<li><strong>Truth-revealing</strong>, <strong>truthful</strong>, or <strong>incentive-compatible strategy</strong>: A strategy that reveals information. For example, a bidder who reveals his true value by playing this strategy.<li><strong>Revelation principle</strong>: Any mechanism can be transformed into an equivalent truth-revealing mechanism.<li><strong>Sealed-bid auction</strong>: Ascending bid is not quite truth revealing because the winning bidder only reveals that $v_i \ge b_o + d$ ($b_o$ is the highest bid among all other bidders). It can also discourage competition which is a disadvantage from the point of view of the seller. For example, if everyone things one bidder is definitely going to win anyway, then no one will participate, which means that the bidder wins with just $b_{min} + d$. Hence in this auction, each bidder makes a single bid and communicates it ot the auctioneer without the other bidders seeing it. Here, there is no single dominant-strategy. If your value is $v_i$, and you think the maximum of all other bids is going to be $b_o$, then you should bid $b_o + \epsilon$ for some small $\epsilon$ if lesser than $v_i$. So the bid depends on the estimation of other bids which requires agents to do more work. Also, an agent with the highest $v_i$ may not win the auction. This is more competitive, however and so the auctioneer has an advantage.<li><strong>Sealed-bid second-price auction</strong> or <strong>Vickrey auction</strong>: The winner pays the price of the <em>second-highest</em> bid $b_o$ rather than paying his own bid. This eliminates all the complex deliberations required for <strong>first-price sealed-bid</strong> acutions, because the dominant strategy is just to bid $v_i$, and the mechanism is truth-revealing. The utility for an agent $i$, $u_i$ is $(v_i - b_o)$ if $b_i &gt; b_o$. That is, the utility is the difference between his true value and the second-higest bid if his actual bid is greater than the second-highest bid. However, if his bid is <em>not</em> greater, then his utility is just 0. Hence playing $b_i = v_i$ is the <strong>dominant strategy</strong> because when $(v_i - b_o)$ is positive, any bid that wins the auction is optimal, and playing $v_i$ wins the auction for sure. When it is negative, <em>any</em> bid that loses the auction is optimal, and bidding $v_i$ loses the auction as well. Since he will never bid anything greater that $v_i$, playing $v_i$ is a <strong>dominant strategy</strong>.<li><strong>Vickrey-Clarke-Groves</strong> or <strong>VCG</strong> mechanism: A way to allocate goods efficiently while maximizing global utility. The center asks each agent to tell it how much it values this free gift. Everyone has an incentive to lie and report a high value. However, the trick is that each agent pays a tax equivalent to the <em>loss</em> in global utility that occurs because of the agent&rsquo;s presence in the game (i.e., the agent is a winner).<ul><li>The center asks each interested agent to report its value $b_i$ for receiving the good. That is, $b_i$ is how much the agent claims he values the good.<li>The center allocates the goods to a subset of agents, $A$. $b_i(A)$ is the outcome to the $i^{th}$ agent under this particular allocation $A$. If $i \in A$ (i.e., $i$ is a winner) the outcome is $b_i$, and otherwise it is zero. The center also chooses $A$ such that it maximizes the total reported utility $B = \sum\nolimits_ib_i(A)$.<li>For each $i$, the center calculates the sum of the reported utilities of <em>all</em> winners, <em>except</em> $i$. Hence $B_{-i} = \sum\nolimits_{j \ne i}b_j(A)$. For each $i$, the center also calculates the allocation that would maximize the total global utility assuming $i$ was not in the game. This is $W_{-i}$.<li>Each agent pays a tax equal to $W_{-i} - B_{-i}$.</ul></ul><h2 id=game-theory-statements>Game Theory: Statements</h2><p><strong>Regarding equilibria</strong><ul><li>John Nash proved that <em>every game has at least one equilibrium</em>. The general concept of equilibrium is now called <strong>Nash Equilibrium</strong>.<li>If a game does not have <strong>pure-strategy Nash equilibria</strong>, then it has an <strong>equilibrium in mixed strategies</strong>, which is also a <strong>Nash Equilibrium</strong>.<li>The value/outcome of a zero-sum game with <strong>mixed-strategies</strong> is within bounds obtained by the outcome for the minimax pure-strategy and the maximin pure strategy: $U(minimax) \le U(mixed) \le U(maximin)$ or $U_{p_1, p_2} \le U \le U_{p_2, p_1}$.<li>Every two-player zero-sum game has a maximin equilibrium when you allow mixed-strategies. In general, every two-player zero-sum game has a (maximin) solution in either pure or mixed strategies.</ul><p><strong>Regarding strategies</strong><ul><li>A dominant strategy is a Nash Equilibrium.<li>Some games have Nash equilibria, but <em>no</em> dominant strategies.<li>If one player plays a pure strategy, the second player might as well choose a pure strategy. This is because the linear combination (expected payoff) using mixed-strategies can never be better than playing the pure-strategy with the highest payoff.</ul><h2 id=repeated-prisoner-s-dilemma>Repeated Prisoner&rsquo;s Dilemma</h2><p>Suppose there are going to be 100 rounds of the game. Round 100 will not be a repeated game, so both can play the dominant strategy (testify). But once round 100 is determined, round 99 cannot have any effect on subsequent rounds, and so both players can just choose (testify, testify), and so on. We can get different solutions by saying that there is a chance that they will meet again. If we assume there is $.99$ probability that they will meet again.<p><strong>Perpetual Punishment</strong><p>This strategy strats off with players playing (refuse) until one person switches to (testify) after which both players will play (testify). If everyone keeps playing (refuse), the outcome is:<p>$$
\sum\limits_{t = 0}^{\infty}0.99^t \cdot (-1) = -100
$$<p>If a player deviates and chooses (testify) on the first round:<p>$$
0 + \sum\limits_{t = 1}^{\infty}0.99^t \cdot (-5) = -495
$$<p>He gets a payoff of 0 in the first round because he chose (testify), but thereafter he receives -5.</p><a class=readmore href=/2017/12/15/cs540-fr.html title="CS540 Final Review">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-23 propositional logic</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/11/1/cs540-23.html title="November 1, 2017">November 1, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p>$\alpha$ : 1. L ==&gt; (M $\lor$ W $\lor$ F)
2. $\lnot$L 1.</p><a class=readmore href=/2017/11/1/cs540-23.html title="CS540-23 propositional logic">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-21 PCA</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/27/cs540-21.html title="October 27, 2017">October 27, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=principal-component-analysis>Principal Component Analysis</h2><p>Input X1 = (
x11,
x12,
x13,
&hellip;,
x1d
), X2, X3, &hellip; Xn $\in R^d$<h4 id=step-1-center-the-data>Step 1. Center the data</h4><p>$$\mu = \frac{1}{n}\sum_{i=0}^n {X_i}$$ = (<p>$$\frac{1}{n}\sum_{i=0}^n{X_i1}$$<p>&hellip;<p>$$\frac{1}{n}\sum_{i=0}^n{X_id}$$<p>)<p>$$X_i &lt;== X_i - \mu$$<h4 id=step-2-form-the-sample-covariance-matrix>Step 2. Form the sample covariance matrix</h4><p>$$S _{(d \times d)} = \frac {1}{n-1} \sum _{i=1}^n X_i(d \times 1)X_i^T(1 \times d)$$ (outer product)<h4 id=step-3-eigen-decompose-s>Step 3. Eigen-decompose S</h4><p>$$S<em>{(d \times d)} = U</em>{(d \times d)} \Lambda {(d \times d)} U^T_{(d \times d)}$$<p>$U = [u_1, u_2, &hellip;, i_d]$<p>$U_i^Tu_j = 0, i \ne j$<p>$u_i^Tu_i = 1$<p>U: orthonormal matrix<p>$$\Lambda = <a href="lambda_1 ge lambda_2 ge ... ge lambda_d ge 0">\lambda_1 &hellip; \lambda_d</a>$$<h4 id=step-4>Step 4.</h4><p>$$
X_i = ( \
u_1^T X_i \
u_2^T X_i \
) \
X_i \in R^2, i=1..n
$$<h2 id=d-2>d=2</h2><p>$$ || W || = 1$$
Length of projection $$ W^TX<em>i $$
Spread = variance $$\frac{1}{1-n} \sum</em>{i=1}^n (W^TX_i)^2$$<p>optimization
$$\max<em>{W \in R^d} \frac{1}{1-n} \sum</em>{i=1}^n (W^TX<em>i)^2 \ (S.T. ||W|| = 1)$$
variable: $W \in R^d$<br>objective: $\frac{1}{1-n} \sum</em>{i=1}^n (W^TX_i)^2$
constraint: $S.T. ||W|| = 1 \to W^TW=1$<p>$$obj = \frac{1}{1-n} \sum_{i=1}^n W^TX_iX<em>i^TW = W^TSW$$
$$\max</em>{w}: W_TSW + \lambda(1-W^TW)$$</p><a class=readmore href=/2017/10/27/cs540-21.html title="CS540-21 PCA">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-20 Dimention Reduction</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/25/cs540-20.html title="October 25, 2017">October 25, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p>Cosine Sinilarity(x,y) = cos($\theta$)<p>vocab: w1, w2, &hellip; , wd<h3 id=bag-of-words-bow>Bag of Words (BOW)</h3><p>doc = (
C1,
C2,
&hellip;,
Cd
)<p>C1: count of w1</p><a class=readmore href=/2017/10/25/cs540-20.html title="CS540-20 Dimention Reduction">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-14 Cache Coding</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/24/cs354-14.html title="October 24, 2017">October 24, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p><object data=/media/post/CS354/outlineL14-W8t.pdf type=application/pdf width=100% height=800px><p><b>Portus PDF Fallback</b>: This browser does not support PDF preview. Please download the PDF to view it: <a href=/media/post/CS354/outlineL14-W8t.pdf>Download PDF</a>.</object></p><a class=readmore href=/2017/10/24/cs354-14.html title="CS354-14 Cache Coding">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-19 Linear Algebra</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/23/cs540-19.html title="October 23, 2017">October 23, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=linear-algebra>Linear Algebra</h2><p>Norm<p>$$||x|| = \sqrt{X^t \times X}$$<p>Inner Product<p>$$X^tY = \sum_{i=1}^d {x_iy_i}$$<p>Euclidean Distance<p>$$||X-Y|| = \lt X,Y \gt = \sqrt{\sum_{i=1}^d{(X_i - Y_i)^2}}$$<h2 id=word-embedding>Word Embedding</h2><ul><li>WordVec<li>GloVe</ul><h2 id=angle>Angle</h2><a class=readmore href=/2017/10/23/cs540-19.html title="CS540-19 Linear Algebra">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-13 Cache Practise</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/19/cs354-13.html title="October 19, 2017">October 19, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p><object data=/media/post/CS354/outlineL13-W7r.pdf type=application/pdf width=100% height=800px><p><b>Portus PDF Fallback</b>: This browser does not support PDF preview. Please download the PDF to view it: <a href=/media/post/CS354/outlineL13-W7r.pdf>Download PDF</a>.</object></p><a class=readmore href=/2017/10/19/cs354-13.html title="CS354-13 Cache Practise">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-18 Probability</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/18/cs540-18.html title="October 18, 2017">October 18, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=probabilistic-inference>Probabilistic Inference</h2><p>P(claim|evidence)<h2 id=ex-envolop>Ex. Envolop</h2><p>[B B], [B R]<p>P(E=e1) = P(E=e2) = <sup>1</sup>&frasl;<sub>2</sub><ol><li>Envolop 1<li>P(C=B|E=e1) = 1<li><p>P(C=R|E=e1) = 0<li><p>Envolop 2<li><p>P(C=B|E=e2) = <sup>1</sup>&frasl;<sub>2</sub><li><p>P(C=R|E=e2) = <sup>1</sup>&frasl;<sub>2</sub></ol><h2 id=bayes-rule>Bayes Rule</h2><p>P(a,b) = P(b,a)<br>--&gt;<br>P(b|a)P(a) = P(a|b)P(b)<br>--&gt;<br>P(b|a) = P(a|b)P(b)/P(a)<h4 id=p-e-e2-c-b-p-b-e2-p-e2-p-b>P(E=e2|C=B) = P(B|e2)P(e2)/P(B)</h4><h2 id=marginalization>Marginalization</h2><p>P(a,b) = P(A=a, B=b)<br>P(B=b) = sigma_a&rsquo; P(A=a, B=b)<h4 id=p-c-b>P(C=B)</h4><p>==&gt;<br>P(E=e1, C=B) + P(E=e2, C=B)<br>==&gt;<br>P(C=B|E=e2)P(E=e2)</p><a class=readmore href=/2017/10/18/cs540-18.html title="CS540-18 Probability">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-12 Cache Design</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/17/cs354-12.html title="October 17, 2017">October 17, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p><object data=/media/post/CS354/outlineL12-W7t.pdf type=application/pdf width=100% height=800px><p><b>Portus PDF Fallback</b>: This browser does not support PDF preview. Please download the PDF to view it: <a href=/media/post/CS354/outlineL12-W7t.pdf>Download PDF</a>.</object></p><a class=readmore href=/2017/10/17/cs354-12.html title="CS354-12 Cache Design">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-17 NLP Demo</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/16/cs540-17.html title="October 16, 2017">October 16, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h4 id=outcome-space>Outcome space</h4><p>{H, T}<br>{1,2,3,&hellip;,6}<br>{Sunny, Cloudy, &hellip;}<p>A $\in$ outcome space<br>P(A=a)<br>P(A $\in$ subset of outcome)<p>P(A=a) $\in$ [0,1]<br>$\forall a \in outcome \sum P(A=a)=1, a \in outcome$<p>$P(A \in S \lor A \in T) = P(A \in S) + P(A \in T) - P(A \in S \land T)$
(S, T subset of outcome)<h2 id=joint-probability>Joint Probability</h2><h4 id=flip-two-coins>Flip two coins</h4><p>A first flip<br>B second flip<p>P(A=H, B=T) = $<sup>1</sup>&frasl;<sub>4</sub>$<p>A first flip<br>B die roll<p>P(A=H, B $\in$ even) = <sup>1</sup>&frasl;<sub>4</sub><h4 id=words>Words</h4><p>W1 P(W1 = &ldquo;the&rdquo;)<br>W2 word of the loaction after W1<p>P(W1 = &ldquo;the&rdquo;, W2 = &ldquo;AI&rdquo;)<br>P(W1 = &ldquo;the&rdquo;, W2 = &ldquo;the&rdquo;)<h2 id=conditional-probability>Conditional Probability</h2><p>P(B=a | A=a) = $\frac{P(A=a, B=b)}{P(A=a)}$<p>$\frac{num \space of \space bigram(the, w2)}{num \space of \space unigram(the)}$<p>P(w2 = &ldquo;AI&rdquo; | W1 = &ldquo;the&rdquo;) &gt;&gt; P(W1=the|W2=the)<h4 id=p-a-b-p-a-p-b-a>P(A, B) = P(A) * P(B|A)</h4><h2 id=chain-rule>Chain Rule</h2><blockquote><p>P(c|a,b)
P(a,b|c)
P(a,b|c,d)</blockquote><p>P(a,b,c) = P(a) * P(b,c|a) = P(a)*P(b|a)*P(c|a,b)<p>P(b,c,|a) = p(b|a)*P(c|b,a)<h4 id=approximation>Approximation</h4><ul><li>P(x1,x2,x3, &hellip; ,xn) $\approx$ P(x1)<em>P(x2)</em>&hellip;*P(xn)<li>P(x1,x2,x3, &hellip; ,xn) $\approx$ P(x1)<em>P(x2|x1)</em>&hellip;*P(xn|xn-1)</ul><a class=readmore href=/2017/10/16/cs540-17.html title="CS540-17 NLP Demo">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-16 Probability</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/13/cs540-16.html title="October 13, 2017">October 13, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=prob-distribution>Prob. distribution</h2><p>d = # face dimensionality<br>p = (p1, p2, p3 &hellip; pd)<br>all should sum to 1<h4 id=multinormial>Multinormial</h4><p>Given $x_1, x_2, &hellip;, x_n \in {1,2,3,&hellip;,d}$<br>Maximun likelihood estimate
$$\hat{P_i} = \frac{number \space of \space times \space i \space appear \space in \space data}{n}$$<ul><li>$(\hat{p_1}, &hellip;, \hat{p_n})$<li>Random number $\mathcal{R}$ uniforming in $[0,1]$<li>R falls into $p_i$<li>output word type i</ul><h4 id=bigram-with-history>Bigram ()With history)</h4><p>$$\hat{p}(w_2=i|w_1=&ldquo;we&rdquo;) = \frac{\text{number of (we, word type i)}}{number \space of\space we}$$</p><a class=readmore href=/2017/10/13/cs540-16.html title="CS540-16 Probability">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-11 Locality</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/12/cs354-11.html title="October 12, 2017">October 12, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p><object data=/media/post/CS354/outlineL11-W6r.pdf type=application/pdf width=100% height=800px><p><b>Portus PDF Fallback</b>: This browser does not support PDF preview. Please download the PDF to view it: <a href=/media/post/CS354/outlineL11-W6r.pdf>Download PDF</a>.</object></p><a class=readmore href=/2017/10/12/cs354-11.html title="CS354-11 Locality">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-16 NLP Demo</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/11/cs540-16.html title="October 11, 2017">October 11, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p>Corpus --&gt; tokenization --&gt; case folding --&gt; stemming<p>log(count) &lt;--&gt; log(rank)<p>$$
x + y = 9
log&reg; + log&copy; = 9
C = e^9 * frac{r}_{1}
\uparrow
$$<h3 id=zipf-s-law>Zipf&rsquo;s law</h3><h3 id=probability>Probability</h3><p>P(X) X: outcome
$$
1 \ge P(x_i) \ge 0
\sum_i=1^d p(x_i) = 1<p>Unigram</p><a class=readmore href=/2017/10/11/cs540-16.html title="CS540-16 NLP Demo">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-10 Free Block</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/10/cs354-10.html title="October 10, 2017">October 10, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=free-block-too-much-or-too-little>Free Block - Too much or too little</h2><h4 id=what-happens-if-free-block-chosen-is-bigger-than-the-request>What happens if free block chosen is bigger than the request?</h4><ul><li>Use entire block<ul><li>+ thruput: fastest, simplest code<li>- Mem Util: worse, more internal fragmentation</ul><li>Split into 2 small blocks<ul><li>First is allocated and rest in new free blk<li>+ Mem Util: better, Constant time to split<li>- Thruput: not much slower but time for splitting is needed</ul></ul><h4 id=heap-allocation-run-4>Heap Allocation Run 4</h4><p>using Splitting on given Free List<p><img src=/media/post/2017-10-10-CS354-10/ar04-orig.png alt=ar04><blockquote><p>Given the heap above and using NF, what address is ptr assigned?If there is a new free block, what is the size in bytes and address?</blockquote><pre><code class=language-c>// 1.
ptr = malloc(2 * sizeof(int));
// 2.
ptr = malloc(sizeof(char));
// 3.
ptr = malloc(8 * sizeof(int));
</code></pre><p><img src=/media/post/2017-10-10-CS354-10/CS354-LecPic_7.jpg alt=ar04-ans><ol><li>0x_20, 8 bytes 0x_2c<li>0x_30, no new free block<li>We need more heap</ol><h4 id=what-happens-if-no-free-block-is-large-enough-to-satisfy-the-request>What happens if no free block is large enough to satisfy the request?</h4><ol><li>Coalesce free block<li>Ask the kernel for more heap memory<li>Return null</ol><h2 id=coalescing-free-blocks>Coalescing Free Blocks</h2><h4 id=heap-allocation-run-5>Heap Allocation Run 5</h4><p>with given Free List<p><img src=/media/post/2017-10-10-CS354-10/ar05-orig.png alt=ar05-orig><blockquote><p>What’s the problem resulting from the following heap operations using FF?</blockquote><pre><code class=language-c>free(p9); p9 = NULL;
free(p1); p1 = NULL;
p1 = malloc(4 * sizeof(int));
</code></pre><h4 id=problem-false-external-fragmentation>Problem: False [external] Fragmentation</h4><p>Heap cannot satisfy a request despite having a large enough contiguous free area in the heap<h4 id=solution-coalesce-adjacent-free-block>Solution: Coalesce Adjacent Free block</h4><h6 id=immediate-when-the-allocator-block-is-free-merge-with-any-free-neighbours>Immediate: When the allocator block is free, merge with any free neighbours</h6><h6 id=delayed-merge-adjacent-blocks-only-when-a-request-for-a-block-larger-than-free-blocks-occurs>Delayed: Merge adjacent blocks only when a request for a block larger than free blocks occurs</h6><p><img src=/media/post/2017-10-10-CS354-10/CS354-LecPic_8.jpg alt=ar05-ans><blockquote><p>Given the original heap above, what is the size in bytes of the freed heap block?</blockquote><pre><code class=language-c>free(p7); p7 = NULL;
</code></pre><p>16<p>Given a pointer to a payload, how do you find its block header?<blockquote><p>ptr - 4 // scaled to 1</blockquote><p>Given a pointer to a payload, how do you find the block header of the next block?<blockquote><p>ptr - 4 bytes + size in bytes</blockquote><p>Given the modified heap above, what is the size in bytes of the freed heap blockwhen immediate coalescing is used?<pre><code class=language-c>free(p3); p3 = NULL;
free(p1); p1 = NULL;
</code></pre><blockquote><p>24<br>32</blockquote><p>Given a pointer to a payload, how do you find the block header of the previous block?<blockquote><p>Could search for free block before the one just freed out but that is slow dependent on number of blocks in the heap before freed blocks</blockquote><h2 id=footers>Footers</h2><p><img src=/media/post/2017-10-10-CS354-10/footer.png alt=footer><h4 id=heap-free-block-layout-with-header-and-footer>Heap Free Block Layout with Header and Footer</h4><p><strong>Footer (AKA Boundary Tag)</strong>: the last word in free block and a copy of the header<p>p Bit = 0 means previous block is free, p=1 means that block is allocated<p>Minimum size is 8 bytes<h6 id=why-don-t-allocated-blocks-need-footers>Why don’t allocated blocks need footers?</h6><blockquote><p>Allocated block are not coalesced</blockquote><p>Given a pointer to a payload, how do you get to the header of a previous block that’s free?<blockquote><ol><li>ptr - 4 bytes gets to curr block header<li>if p bit is 0 then pre - 8 gets to previous block footer<li>ptr - 4 - previous block size gets to previous block header</ol></blockquote><h4 id=heap-allocation-run-6>Heap Allocation Run 6</h4><p>with given Free List using Immediate Coalescing and Free Block Footers<p><img src=/media/post/2017-10-10-CS354-10/ar06-orig.png alt=ar06><blockquote><p>Given the heap above, what is the size in bytes of the freed heap block?
<code>free(p1);</code></blockquote><p><strong>32 bytes</strong><blockquote><p>Given the modified heap above, what is the size in bytes of the freed heap block?
<code>free(p4);</code></blockquote><p><strong>24 bytes</strong><p><img src=/media/post/2017-10-10-CS354-10/CS354-LecPic_9.jpg alt=ar06-ans><blockquote><p>Is coalescing done in a fixed number of steps (constant time)or is it dependent on the number of blocks (linear time)</blockquote><h2 id=explicit-free-list>Explicit Free List</h2><p>The allocator only tracks the free blocks<p>&ldquo;Doubly linked list&rdquo;<p>Free Block Links<br><strong>pred</strong> address of previous free block<br><strong>succ</strong> address of next free block<p>Pic<p>Minimum block size is 16 bytes<br>$\downarrow$<br>this increases internal fragmentation</p><a class=readmore href=/2017/10/10/cs354-10.html title="CS354-10 Free Block">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-15 NLP Demo</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/9/cs540-15.html title="October 9, 2017">October 9, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=corpus>Corpus</h2><p>p1. Corpora<p>Word token<p>word type</p><a class=readmore href=/2017/10/9/cs540-15.html title="CS540-15 NLP Demo">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-14 Alpha-Beta Pruning</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/6/cs540-14.html title="October 6, 2017">October 6, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=alpha-beta-pruning>Alpha-Beta Pruning</h2><p>aloha: max&rsquo;s alternative<br>beta: min&rsquo;s alternativeo<ul><li>Initialization<ul><li>alpha: -INF, beta: INF</ul><li>keep two bounds along the path<ul><li>alpha: the best Max can do<li>beta: the best Min can do</ul></ul><pre><code>function Max-Value(s,α,β)
inputs:
    s: current state in game, Max about to play
    α: best score (highest) for Max along path to s
    β: best score (lowest) for Min along path to s
output:min(β , best-score (for Max) available from s)
    if ( s is a terminal state )
        then return ( terminal value of s )
    else for each s’ in Succ(s)
        α := max( α , Min-value(s’,α,β))
        if ( α ≥ β ) then return β   /* alpha pruning */
    return α

function Min-Value(s,α,β)
output:max(α , best-score (for Min) available from s )
    if ( s is a terminal state )
        then return ( terminal value of s)
    else for each s’ in Succs(s)
        β := min( β , Max-value(s’,α,β))
        if (α ≥ β ) then return α   /* beta pruning */
    return β
</code></pre><a class=readmore href=/2017/10/6/cs540-14.html title="CS540-14 Alpha-Beta Pruning">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-09 Allocator Design</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/5/cs354-09.html title="October 5, 2017">October 5, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=allocator-design>Allocator Design</h2><h4 id=goals>Goals</h4><p><strong>throughput</strong>: ops/sec<br>ops are malloc/calloc/realloc/free<ul><li>free&rsquo;s work is independent of number of heap block<li>malloc&rsquo;s work is linearly dependent on the number of heap blocks</ul><p><strong>memory utilization</strong><br>maximize memory usable by programs
(memory requested)/(heap allocated)<p>The above two typically increasing one decrease the other<h4 id=requirement>Requirement</h4><p>List the requirements of a heap allocator<ol><li>must handle an arbitrary sequence of requests<li>provide an immediate response<li>don&rsquo;t move/change allocated blocks<li>allocated memory in the heap segment<li>follow system alignment requirements</ol><h4 id=design-considerations>Design Considerations</h4><ul><li>free block organization<li>placement policies<li>spliting free blocks to create a better fit<li>coalescing free blocks to form a larger block</ul><h2 id=simple-view-of-heap>Simple view of heap</h2><h4 id=linear-memory-layout>Linear memory layout</h4><p><strong>double word alignment</strong>: block size must be multiples of 8<h4 id=heap-allocation-run-1>Heap Allocation Run 1</h4><blockquote><p>Update the diagram to show the following heap allocations</blockquote><pre><code class=language-c>p1 = malloc(2 * sizeof(int));
p2 = malloc(3 * sizeof(char));
p3 = malloc(4 * sizeof(int));
p4 = malloc(5 * sizeof(int));
</code></pre><p><img src=/media/post/2017-10-05-CS354-09/heap-allocation-run-1-1.jpg alt=heap-1-1><blockquote><p>What happens with the following heap operations</blockquote><pre><code class=language-c>free(p1); p1 = NULL;
free(p3); p3 = NULL;
p5 = malloc(6 * sizeof(int));
</code></pre><p>given that only the memory diagrammed above is the heap,
this allocation p5 fails.<p><img src=/media/post/2017-10-05-CS354-09/heap-allocation-run-1-2.jpg alt=heap-1-2><h6 id=internal-fragmentation>Internal Fragmentation</h6><p>Memory inside a heap block that is overhead (e.g. padding)<h6 id=external-fragmentation>External Fragmentation</h6><p>When there is enough free memory in the heap but it has been divided
into smaller, non-contiguous blocks<h2 id=free-block-organization>Free Block Organization</h2><p>How does allocator know which parts of the heap are free and which are allocated?<blockquote><p>Simple view doesn&rsquo;t state status of a block</blockquote><p>How does allocator know how big each block is?<blockquote><p>Simple view doesn&rsquo;t track block size</blockquote><h4 id=explicit-free-list>Explicit Free List</h4><p>A structure used to store references to just the free blocks<p><img src=/media/post/2017-10-05-CS354-09/explicit-free-list.jpg alt=efl><ul><li>negative space is needed for the free list<li>allocation is linearly dependent on only the number of free blocks</ul><h4 id=implicit-free-list>Implicit Free list</h4><p>No separate structure, instead status and size info is stored
as part of each block<ul><li>negative allocation is linearly dependent on the number of <strong>used and free</strong> blocks<li>allocator code is simpler</ul><h2 id=implicit-free-list-1>Implicit Free List</h2><p><strong>each block maintain a header with blocks status and size</strong><h4 id=basic-heap-block-layout>Basic Heap Block Layout</h4><p><img src=/media/post/2017-10-05-CS354-09/bhbl.jpg alt=bhbl><ul><li>min size if 8, 4 header + 4 payload<li>a=1 means allocated, 0 means free<li><sup>8</sup>&frasl;<sub>1</sub> = 8 byte block that&rsquo;s allocated<li><sup>64</sup>&frasl;<sub>1</sub> = 64 byte block that&rsquo;s allocated<li><sup>16</sup>&frasl;<sub>0</sub> = 16 bytes block that&rsquo;s free</ul><h4 id=heap-allocation-run-2-with-block-headers>Heap Allocation Run 2 with Block Headers</h4><blockquote><p>Update the diagram to show the following heap allocations:</blockquote><pre><code class=language-c>p1 = malloc(2 * sizeof(int));
p2 = malloc(3 * sizeof(char));
p3 = malloc(4 * sizeof(int));
p4 = malloc(5 * sizeof(int));
</code></pre><p><img src=/media/post/2017-10-05-CS354-09/har2.jpg alt=har2><blockquote><p>Why does it make sense that Java doesn’t allow primitives on the heap?</blockquote><h2 id=placement-policy>Placement Policy</h2><h4 id=placement-policies>Placement Policies</h4><p>Are algorithms used to search the heap for a free block to
satisfy the request<h6 id=first-fit-ff>First Fit (FF)</h6><p>Search from begin for the first free block that&rsquo;s big enough<p>Thruput: larger blk req must step through smaller blk<h6 id=next-fit-nf>Next Fit (NF)</h6><p>Search from block last allocated for the first free
block that is big enough<ul><li>Thruput: on average, faster than FF<br><li>mem util: typycally</ul><h6 id=best-fit-bf>Best Fit (BF)</h6><p>Search all free blocks for the one that is closest to requested size<ul><li>Mem Util: best of 3<br><li><p>Thruput: slowest of 3<h4 id=heap-allocation-run-3>Heap Allocation Run 3</h4><p>using a Placement Policy on the given Free List</ul><blockquote><p>Given the original heap above, what address is ptr assigned?</blockquote><pre><code class=language-c>ptr = malloc(2 * sizeof(int)); //FF? BF?
ptr = malloc(7 * sizeof(char)); //FF? BF?
ptr = malloc(1 * sizeof(int)); //FF? BF?
ptr = malloc(5 * sizeof(int)); //FF? BF?
</code></pre><ol><li>0x_10 | 0x_10<li>0x_10 | 0x_10<li>0x_10 | 0x_30 or 0x_40<li>0x_10 null | 0x_10 null</ol><pre><code class=language-c>ptr = malloc(2 * sizeof(char));  //0x_20?              0x_38?
ptr = malloc(3 * sizeof(int));   //0x_08?              0x_20?

</code></pre><a class=readmore href=/2017/10/5/cs354-09.html title="CS354-09 Allocator Design">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-13 Minimax Algorithm</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/4/cs540-13.html title="October 4, 2017">October 4, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=minimax-algorithm>Minimax Algorithm</h2><pre><code>function Max-Value(s)
inputs:
    s:current state in game, Max about to play
output: best-score (for Max) available from s
    if (s is a terminal state)
    then return (terminal value of s)
    else
        a = -INF
        for each s' in Succ(s)
            a = max(a, Min-Value(s'))
    return a

function Min-Value(s)
output: best-score(for Min) available from s
    if (s is a terminal state)
    then return (terminal value of s)
    else
        b = INF
        for each s' in Succ(s)
            b = min(b, Max-Value(s'))
    return b
</code></pre><h4 id=alpha-beta-motivation>Alpha-beta Motivation</h4><ul><li>Alpha pruning<li>Beta pruning</ul><a class=readmore href=/2017/10/4/cs540-13.html title="CS540-13 Minimax Algorithm">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-08 The Heap</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/3/cs354-08.html title="October 3, 2017">October 3, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=the-heap>The heap</h2><h4 id=what-the-heap-is>What? The heap is</h4><ul><li>a segment of a process&rsquo;s variables used for dynamically allocated memory<li>a collection of various-sized memory blocks that are managed by an allocator<li><strong>dynamically allocated memory</strong>: is allocated during runtime to satisfy the uncertain memory needs</ul><h6 id=block>block</h6><p>A continuous chunk of memory containing a payload and overhead<h6 id=payload>payload</h6><p>Part of block usable by the process requesting heap memory<h6 id=overhead>overhead</h6><p>Part of the block used by the allocator to manage the heap&rsquo;s structure<h6 id=allocator>allocator</h6><p>Code that allocates and frees blocks as requested by a process<h4 id=allocator-approach>Allocator Approach</h4><ul><li>Implicit: Java<ul><li>new automatically determines block size<li>garbage collector recycles for us</ul><li>Explicit: C<ul><li>malloc: we say how many bytes we needed<li>free is required when we&rsquo;re done using block</ul></ul><h2 id=c-s-heap-allocator-stdlib-h>C&rsquo;s Heap Allocator (<code>stdlib.h</code>)</h2><h4 id=what-s-in-the-stdlib-h>What&rsquo;s in the <code>stdlib.h</code>?</h4><p>A collection of about 25 commonly used C functions<ul><li>conversion (atof, atoi, atol)<li>execution flows (abort, exit)<li>math (abs)<li>searching (bsearch)<li>sorting (qsort)<li>random numbers (rand, srand(seed))<li>allocator functions</ul><h4 id=allocator-functions>Allocator Functions</h4><h6 id=void-malloc-size-t-size><code>void *malloc(size_t size)</code></h6><p>Allocates and returns generic ptr to block of heap memory of size bytes, or return null if allocation fails, memory double word aligned in Unix<h6 id=void-calloc-size-t-nitems-size-t-size><code>void *calloc(size_t nItems, size_t size)</code></h6><p>Allocates, <strong>clears to 0</strong>, and returns a block of heap memory of nItems*size bytes,<h6 id=void-realloc-void-ptr-size-t-size><code>void *realloc(void *ptr, size_t size)</code></h6><p>Reallocates to size bytes a previously allocated block of heap memory pointed to by ptr.<pre><code class=language-c>if (ptr == null) return malloc(size);
else if (size == 0) {
    free(ptr);
    return null;
}
else // attempt to resize a block
</code></pre><h6 id=void-free-void-ptr><code>void free(void *ptr)</code></h6><p>Frees the heap memory pointed to by ptr<ul><li>If ptr is null, free does nothing.<li>Undefined:<ul><li>free freed heap memory<li>free stack memory</ul></ul><p>void return type means it can&rsquo;t tell us if there was a problem<p>If malloc/calloc/realloc returns null, exit program with an error message.<h2 id=posix-brk-unistd-h>Posix <code>brk</code> (<code>unistd.h</code>)</h2><h4 id=what-is-unistd-h>What is <code>unistd.h</code></h4><p>Header file to access posix (<strong>Portable OS Interface</strong>)<p>IEEE CS Standard for maintaining compatibility between Unix OS&rsquo;s<h4 id=diy-heap-via-posix-calls>DIY Heap via Posix Calls</h4><h6 id=brk>Brk</h6><p>&ldquo;Program Break&rdquo;, a pointer to the top of the heap<pre><code class=language-c>int brk(void *addr)
</code></pre><p>sets the top of the heap to specified address<br>returns 0 if success, else -1 and sets errno<br>OS clears to 0 new heap pages<pre><code class=language-c>void *sbrk(intptr_t incr)
</code></pre><p>Attempts to change the program&rsquo;s top of heap by <code>incr</code> bytes<br>returns old brk if success, else -1 and sets errno<h6 id=errno-h><code>errno.h</code></h6><p>header file to get the error number<pre><code class=language-c>printf(&quot;Error %s&quot;, strerror(errno));
</code></pre><p><strong>Use malloc/calloc/realloc instead since they will allocate sufficient heap space</strong></p><a class=readmore href=/2017/10/3/cs354-08.html title="CS354-08 The Heap">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-12 Game Playing</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/10/2/cs540-12.html title="October 2, 2017">October 2, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=game-playing>Game Playing</h2><ul><li>2-player<li>Discrete<li>Finite<li>Deterministic<li>Perfect information<li>Zero sum</ul><blockquote><p>2-Nim, Chess, GO</blockquote><p>TODO:<pre><code class=language-mermaid>graph TD;
    0(2,2) -&gt; 1(1,2);
    0(2,2) -&gt; 1(0,2);
    1(1,2) -&gt; 2(0,2);
    1(1,2) -&gt; 2(1,1);
    1(1,2) -&gt; 2(0,1);
    2(0,2) -&gt; 3(0,0);
    2(0,2) -&gt; 3(0,1);
    2(1,1) -&gt; 3(0,1);
    2(0,1) -&gt; 3(0,0);
    1(0,2) -&gt; 2(0,0);
    1(0,2) -&gt; 2(0,1);
    2(0,1) -&gt; 3(0,0);
</code></pre><a class=readmore href=/2017/10/2/cs540-12.html title="CS540-12 Game Playing">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-11 Simulated Annealing</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/29/cs540-11.html title="September 29, 2017">September 29, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=local-search>Local Search</h2><ul><li>Hill Climbing<li>Simulated Annealing<li>Genetic Algorithm</ul><p>State Space + successor G = (V,E)<br>Score f(s), $\forall s \in V $<h4 id=hill-climbing-t-s>Hill Climbing (T, S)</h4><ol><li>at state s, pick a (random) successor t<li>move to t<ul><li>with probability p if f(t) $\le$ f(s)<li>definitely, if f(t) &gt; f(s)</ul></ol><h5 id=p-e-frac-f-s-f-t-t>$p=e^{-\frac{f(s)-f(t)}{T}}$</h5><h4 id=simulated-annealing>Simulated Annealing</h4><ol><li>Randomly choose state s<li>T = big<li>s $\leftarrow$ HC(T, s)<li>Reduce T</ol><p>T: <strong>temperature</strong><br>Reduce T: <strong>cooling scheme</strong> (e.g. T $\leftarrow$ a*T, 0 &lt; a &lt; 1)<p>$T_t = b*c^t$<h2 id=genetic-algorithm>Genetic Algorithm</h2><p>State representation (DNA)<ul><li>Start with population of random states<li>treat f score f(s) as the fitness score of the individual<li>select 100 pairs according to probability and fitness<li>Exchange part of DNA to generate two children<li>remove the original 100 population<li>mutation: very small probability to change a DNA at specific position with another random one</ul><a class=readmore href=/2017/9/29/cs540-11.html title="CS540-11 Simulated Annealing">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-07 Standard IO</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/28/cs354-07.html title="September 28, 2017">September 28, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=standard-and-string-io-in-stdio-h-library>Standard and String IO in stdio.h library</h2><h4 id=standard-io>Standard IO</h4><h6 id=standard-output-console>Standard Output (console)</h6><pre><code class=language-c>putchar, puts
int printf(const char *format, ...)
</code></pre><ul><li>format string, format specifiers<li>matching vars<li>returns the number of chars written of negative if error</ul><h6 id=standard-input-console>Standard Input (console)</h6><pre><code class=language-c>getchar, gets
int scanf(const char *format, ...)
</code></pre><ul><li>gets uses EOLN termination<li>format string with info for input format and format specifiers<li>each format specifier must have the <strong>address</strong> of a matching vars<li>returns the number of inputs that have been successfully matched</ul><h6 id=standard-error-console>Standard Error (console)</h6><pre><code class=language-c>void perror(const char *str)
</code></pre><h4 id=string-io>String IO</h4><pre><code class=language-c>int sprintf(char *str, const char *format, ...)
// sends formatted output to specified string
int sscanf(const char *str, const char *format, ...)
// gets formatted input from specified string
</code></pre><h2 id=file-i-o-in-stdio-h-library>File I/O in stdio.h Library</h2><h4 id=standard-i-o-redirection>Standard I/O Redirection</h4><pre><code>% a.out &lt; input file &gt; output file
</code></pre><p>note with output redirection, errors printed with perror still appears
on the console terminal<h4 id=file-io>File IO</h4><h6 id=file-output>File Output</h6><pre><code class=language-c>fputc/putc, fputs
int fprintf(FILE *stream, const char *format, ...)
</code></pre><h6 id=file-input>File Input</h6><pre><code class=language-c>fgetc/getc, ungetc, fgets
int fscanf(FILE *stream, const char *format, ...)
</code></pre><p>returns the number of inputs or EOF if error or End Of File<h4 id=file-pointers-descriptors>File Pointers/Descriptors</h4><p><code>stdin, stdout, stderr</code><pre><code class=language-c>printf(&quot;Hello&quot;);
// Equivalent to
fprintf(stdout, &quot;Hello&quot;);
</code></pre><h6 id=open-closing>Open/Closing</h6><pre><code class=language-c>FILE *fopen(const char *filename, const char *mode)
</code></pre><p>returns a pointer to the open file or NULL if it can&rsquo;t access<pre><code class=language-c>int fclose(FILE *stream)
</code></pre><p>returns 0, or EOF if there&rsquo;s an error<h2 id=copying-text-files>Copying Text Files</h2><pre><code class=language-c>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt; // for exit()

int main(int argc, char *argv[]) {
    if (argc != 3) {
        fprintf(stderr, &quot;Usage: copy inputfile outputfile\n&quot;);
        exit(1);
    }


    FILE *ifp, *ofp;
    ifp = fopen(argv[1], &quot;r&quot;);
    if (ifp == NULL) {
        fprintf(stderr, &quot;Can't open input file %s!\n&quot;, argv[1]);
        exit(1);
    }


    ofp = fopen(argv[2], &quot;w&quot;);
    if (ofp == NULL) {
        fprintf(stderr, &quot;Can't open output file %s!\n&quot;, argv[2]);
        exit(1);
    }


    const int bufsize = 257; // lines assumed to be 256 chars or less
    char buffer[bufsize];

    while (fgets(buffer, bufsize, ifp) != NULL)
        fputs(buffer, ofp);

    fclose(ifp);
    fclose(ofp);


    return 0;
}
</code></pre><h2 id=what-do-i-live>What do I live?</h2><pre><code class=language-c>#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
int gus = 14;
int guy;
int madison(int pam) {
    static int max = 0;
    int meg[] = {22,44,88};
    int *mel = &amp;pam;
    max = gus--;
    return max + meg[1] + *mel;
}
int *austin(int *pat){
    static int amy = 33;
    int *ari = malloc(sizeof(int)*44);
    gus--;
    *ari = *pat;
    return ari;
}
int main(int argc, char *argv[]) {
    int vic[] = {33,66,99};
    int *wes = malloc(sizeof(int));
    *wes = 55;
    guy = 66;
    free(wes);
    wes = vic;
    wes[1] = madison(guy);
    wes = austin(&amp;gus);
    free(wes);
    printf(&quot;Where do I live?&quot;);
    return 0;
}
</code></pre><ul><li><strong>Arrays, structs, pointers can live in any memory area (2,3,4,5)</strong><li><p><strong>Pointer variables can store any address</strong><li><p>vic[]: 5<li><p>{33, 66, 99}: 1<li><p>wes, ari: 5<li><p>malloc* : 4<li><p>assignment: 1<li><p>pam: 5<li><p>meg: 5<li><p>gus: 2<li><p>guy: 3<li><p>return value: 5<li><p>max: 3</ul><h2 id=c-abstract-memory-model>C Abstract Memory Model</h2><h4 id=1-code-segment-aka-text-rodata>1. CODE Segment (AKA .text &amp; .rodata)</h4><p>contains: <strong>Program instructions and literals</strong><ul><li>typically is read only and sharable</ul><h4 id=2-initialized-data-segment-aka-data>2. Initialized DATA Segment (AKA .data)</h4><p>What: Memory allocation with a specified size, type, and non-zero initialization.
Lifetime: entire program executiton<p>Contains: <strong>extern, global and static local variables that are non-zero initialization</strong><h4 id=3-uninitialized-data-segment-aka-bss>3. Uninitialized DATA Segment (AKA .bss)</h4><p>What: Memory allocation with a specified size, type and uninitialized or initialized to zero.
Lifetime: entire program executiton<p>Contains: <strong>extern, global and static local variables that are uninitialized or initialized to zero</strong><h4 id=4-heap-aka-free-store>4. HEAP (AKA Free Store)</h4><p>What: Memory allocation requested during execution.<p>Lifetime: programmer managed using malloc/free<p>Contains: <strong>Dynamic memory allocations</strong><h4 id=5-stack-aka-call-stack-auto-store>5. STACK (AKA Call Stack, Auto Store)</h4><p>What: Memory automatically allocated and freed as functions are executed.<p>Lifetime: from definition till end of its scope<p>Contains: <strong>Stack frames with non-static local and param variables</strong><p>LIFO: Last in, first out<p>Stack Frame: AKA activation record, contains set values for a call of some - later<h2 id=virtual-address-space-for-a-process-ia-32-linux>Virtual Address Space for a Process (IA-32/Linux)</h2><h4 id=32-bit-processor-32-bit-addresses>32-bit Processor = 32-bit Addresses</h4><h4 id=byte-addressable>byte addressable</h4><p>each address accesses 1 bytemax addressable bytes: 2^32 = 4,294,967,296 = 4GB<h4 id=virtual-address-space>Virtual address space</h4><p>illusion by OS that each process has it&rsquo;s own memory sandbox<h4 id=process>Process</h4><p>Running program<h4 id=virtual-address>Virtual Address</h4><p>Simulated address generated by processes<h4 id=physical-address>Physical Address</h4><p>Actual address used to access the machine&rsquo;s memory<h4 id=page-table>Page table</h4><p>OS use this to map from virtual addresses to physical addresses<h2 id=linux-processes-and-address-spaces>Linux: Processes and Address Spaces</h2><h4 id=program-size>Program Size</h4><pre><code>%gcc -m32 myProg.c
%size a.out   
text    data     bss     dec     hex filename   
1029     276       4    1309     51d a.out
</code></pre><h4 id=virtual-address-space-maps>Virtual Address Space Maps</h4><pre><code>%cat /proc/&lt;pid_of_process&gt;/maps
%cat /proc/self/maps
</code></pre><h4 id=process-and-job-control>Process and Job control</h4><p>Linux is a multitasking OS where you can concurrently run multiple processes<ul><li><strong>ps</strong>: lists a snapshot of your running processes<li><strong>jobs</strong>: list a snapshot of the processes you&rsquo;ve started from the command line<li><strong>&amp;</strong>: Puts a process in the background<li><strong>Ctrl-z</strong>: Suspend the running foreground process<li><strong>bg</strong>: Puts a suspended process in the background<li><strong>fg</strong>: Brings the process to the foreground<li><strong>Ctrl-c</strong>: Stops a running foreground process<li><strong>kill</strong>: Stops a process that is running in background</ul><a class=readmore href=/2017/9/28/cs354-07.html title="CS354-07 Standard IO">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-10 Hill Climbing</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/27/cs540-10.html title="September 27, 2017">September 27, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=hill-climbing-greedy>Hill Climbing (greedy)</h2><ul><li>Random restart, keep tracking best local optimum<li>inspect a single neighbor t (current state s)<ul><li>if f(t)&gt;f(s) move to t s $\leftarrow$ t<li>else do nothing</ul></ul><h2 id=sth>sth</h2><ul><li>if f(t) &gt; f(s) s $\leftarrow t$<li>else [with probability p $\in$ (0,1), s $\leftarrow$ t; otherwise, do nothing]<ul><li>$\uparrow$ pseudo code for else<li>if (random() $\le$ p) s $\leftarrow$ t;<li>else no op</ul></ul><a class=readmore href=/2017/9/27/cs540-10.html title="CS540-10 Hill Climbing">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-06 Structures &amp; Cmdline Arguments</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/26/cs354-06.html title="September 26, 2017">September 26, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=command-line-arguments>Command Line Arguments</h2><h4 id=what>What?</h4><p>Consider the Linux command: <code>%gcc myprog.c -Wall -m32 -std=gnu99 -o myprog</code><p><strong>Info entered at the command prompt.</strong>
Note CLAs start with the prog name and then are followed by prog args.<h4 id=why>Why?</h4><p>Enables info to be passed to the program before it begins execution.<h4 id=how>How?</h4><p>Consider the code:<pre><code class=language-c>int main(int argc, char *argv[]) {
    for (int i = 0; i &lt; argc; i++)
    printf(&quot;%s\n&quot;, argv[i]);
}
</code></pre><ul><li>argc: arg count of whitespace separated CLAs<li>argv: arg vector, which is an array of char pointers</ul><blockquote><p>Assume the program above is invoked with &ldquo;a.out eleven 22 -33.3&rdquo;
Draw the memory diagram for argv.<p>Now show what is output by the program:<pre><code>a.out
eleven
22
-33.3
</code></pre></blockquote><h2 id=meet-structure>Meet Structure</h2><h4 id=what-a-structure-defines>What? A structure defines:</h4><p>a new data type that is a composite of typically related data of difference kind<h4 id=why-1>Why?</h4><p>Enables organizing data into a single module.<h4 id=how-1>How?</h4><pre><code class=language-c>struct &lt;struct-name&gt; {
    &lt;data-declarations&gt;;
} &lt;optional-list-of-variables&gt;;
// ; end the new type declaration
</code></pre><blockquote><p>Declare a structure representing a date having a integer month, day of month, and year.</blockquote><pre><code class=language-c>struct date { // struct is a name space identified with &quot;struct&quot;
    int month;
    int mday;
    int year;
};
</code></pre><blockquote><p>Create a variable containing today&rsquo;s date</blockquote><pre><code class=language-c>struct Date today;
today.month = 9;
today.mday = 26;
today.year = 2017;
</code></pre><p>dot operator(.): member access operator<h4 id=typedef>Typedef</h4><h6 id=what-1>What?</h6><p>Defines a new type name in the global name space<h6 id=why-2>Why?</h6><p>Easier, less cluttered<blockquote><p>Update the code above to use typedef</blockquote><pre><code class=language-c>typedef struct {
    int month;
    int mday;
    int year;
} Date;
Date today;
// ...
</code></pre><h2 id=nested-structures-and-array-of-structures>Nested Structures and Array of Structures</h2><h4 id=nested-structures>Nested Structures</h4><blockquote><p>Add a Date struct, named caught, to the structure code below.<pre><code class=language-c>typedef struct { ... } Date; //from previous page
typedef struct {
    char name[12];
    char type[12];
    float weight;
    Date caught;
} Pokemon;
</code></pre></blockquote><h6 id=structures-can-contain>Structures can contain</h6><ul><li>Structures<li>arrays</ul><p>Nested as deeply as we wish<blockquote><p>Show how a pokemon is laid out in the memory diagram</blockquote><h4 id=array-of-structures>Array of structures</h4><blockquote><p>Statically allocate an array, named pokedex, and initialize it with two pokemon.</blockquote><pre><code class=language-c>Pokemon pokedex[2] =
{ { &quot;Abra&quot;, &quot;Psych&quot;, 43.0, { 1, 21, 2017 } },
  { &quot;Oddish&quot;, &quot;bro..&quot;, 11.9, { 7, 23, 2017 } } };
</code></pre><h6 id=arrays-can-have>Arrays can have</h6><p>structures for their elements<h2 id=structures-and-functions>Structures and functions</h2><blockquote><p>Complete the function below that displays the a Date structure.</blockquote><pre><code class=language-c>void printDate (Date date) {
    printf(&quot;%d/%d/%d&quot;, date.month, date.mday, date.year);
}
</code></pre><h6 id=structures-are-passed>Structures are passed</h6><p><strong>by value</strong> which copies the entire structure<h4 id=consider-the-additional-code>Consider the additional code:</h4><pre><code class=language-c>//assume Date, Pokemon, printDate code included here
void printPm(Pokemon pm) {
    printf(&quot;\nPokemon Name : %s&quot;,pm.name);
    printf(&quot;\nPokemon Type : %s&quot;,pm.type);
    printf(&quot;\nPokemon Weight : %f&quot;,pm.weight);
    printf(&quot;\nPokemon Caught on : &quot;, printDate(pm.caught));
    printf(&quot;\n&quot;);
}
int main(void) {
    Pokemon pm1 = {&quot;Abra&quot;,&quot;Psychic&quot;,30,{1,21,2017}};
    printPm(pm1);
}
</code></pre><blockquote><p>Complete the function below so that it displays pokedex.</blockquote><pre><code class=language-c>void printDex(Pokemon dex[], int size) {
    for (int i = 0; i &lt; size; i++)
        printPM(dex[i]);
}
</code></pre><h6 id=arrays-are-passed>Arrays are passed</h6><p><strong>by value but only their address is copied</strong>, the elements are not copied.<h2 id=pointer-to-structures>Pointer to Structures</h2><h4 id=why-3>Why？</h4><ul><li>Enables heap allocation<li>Avoids copying overhead from pass-by-value<li>Allows functions to change their structure args<li>Enables creating linked data structure</ul><h4 id=how-2>How?</h4><blockquote><p>Declare a pointer to a Pokemon</blockquote><pre><code class=language-c>Pokemon* pmptr;
</code></pre><blockquote><p>Dynamically allocate space for a Pokemon</blockquote><pre><code class=language-c>pmptr = malloc(sizeof(Pokemon));
</code></pre><blockquote><p>Assign a weight to the Pokemon</blockquote><pre><code class=language-c>(*pmptr).height = 43.0; // Don't use
pmptr -&gt; height = 43; // Use this
</code></pre><blockquote><p>Assign a name and type to the pokemon</blockquote><pre><code class=language-c>strcpy(pmptr-&gt;name, &quot;asdf&quot;);
strcpy(pmptr-&gt;type, &quot;asdf&quot;);
</code></pre><blockquote><p>Assign a caught date to the Pokemon</blockquote><pre><code class=language-c>pmptr-&gt;caught.month = 9;
pmptr-&gt;caught.mday = 28;
pmptr-&gt;caught.year = 2017;
</code></pre><blockquote><p>Deallocate the Pokemon&rsquo;s memory</blockquote><pre><code class=language-c>free(pmptr);
// pmptr = NULL;
</code></pre><blockquote><p>Update printPm to efficiently pass and print a Pokemon.</blockquote><pre><code class=language-c>void printPm(Pokemon *pm) {
    printf(&quot;\nPokemon Name : %s&quot;,pm-&gt;name);
    printf(&quot;\nPokemon Type : %s&quot;,pm-&gt;type);
    printf(&quot;\nPokemon Weight : %f&quot;,pm-&gt;weight);
    printf(&quot;\nPokemon Caught on : &quot;);
    printDate(pm-&gt;caught);
    printf(&quot;\n&quot;);
}
int main(void) {
    Pokemon pm1 = {&quot;Abra&quot;,&quot;Psychic&quot;,30,{1,21,2017}};
    printPm(&amp;pm1);
}
</code></pre><a class=readmore href=/2017/9/26/cs354-06.html title="CS354-06 Structures &amp; Cmdline Arguments">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-09 Hill Climbing</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/25/cs540-09.html title="September 25, 2017">September 25, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=score-function-f>Score Function f</h2><h4 id=f-s-forall-s>f(s) $\forall s$</h4><ul><li>f(s) = number of queens attacking each other<li><strong>f(s) = 8 - # of queens attacking</strong></ul><h2 id=hill-climbing>Hill Climbing</h2><h4 id=hill-climbing-s>Hill Climbing (s)</h4><ul><li>randomly generate s<li>check f(s) (number of queens not attacking)<li>t = succ(s)<li>check f(t), pick the max<li>if $(f(s) &lt; max(f(t)))$ then $s \leftarrow argmax(f) , t \in succ(s)$<li><em>(argmax returns the arg with max function value)</em><li>else return s</ul><p>Local Maximum vs Global Maximum<p><strong>Hill Climbing may be stuck at Local Maximum</strong><h4 id=hill-climbing-with-random-restarts>Hill Climbing with Random Restarts</h4><blockquote><p>e.g. TSP</blockquote><a class=readmore href=/2017/9/25/cs540-09.html title="CS540-09 Hill Climbing">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-08 A* Search</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/22/cs540-08.html title="September 22, 2017">September 22, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=a-search>A* Search</h2><h4 id=proof-a-finds-the-optimal-path>Proof A* finds the optimal path</h4><p>Support not, A* ends of G&rsquo;s instead of G<br>n is the node where it diverges from optimal path<br>$f(s) = g(s) + h(s)<ol><li>$f(G&rsquo;) &gt; f(G)$<li>$f(n) \ge f(G&rsquo;)$<li>$f(n) = g(n) + h(n) \le g(n) + h&rsquo;(n) = f(G)$ (h is the heuristic, h&rsquo; is the true future cost)</ol><p>1,2,3 $\to$ $f(G) \gt f(G)$ (contradiction)<h2 id=improvement>Improvement</h2><ol><li>ID A*<li>Beam search (fix max PQ size)</ol><a class=readmore href=/2017/9/22/cs540-08.html title="CS540-08 A* Search">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-05 Array on the heap</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/21/cs354-05.html title="September 21, 2017">September 21, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=1d-arrays-on-the-heap>1D Arrays on the Heap</h2><h4 id=what-memory-segments-used-by-a-program-include>What? Memory segments used by a program include</h4><ul><li>Stack vs Heap<li>static allocation vs dynamic<li>during compile time vs runtime</ul><h4 id=why-heap-memory-enables>Why? Heap memory enables</h4><ul><li>A program to access more mem than what&rsquo;s allocated by the compiler<li>Blocks of memory to be allocated and freed in an arbitrary order</ul><h4 id=how>How?</h4><pre><code class=language-c>#include &quot;stdlib.h&quot;
// for malloc and free

malloc(size_in_mem);
// Preserves a block of heap mem of the specified size

free(pointer);
// Frees the heap mem pointed to by the specified pointer

sizeof(operand);
// operator that returns size in bytes of its operand
</code></pre><ul><li><p>IA32<ul><li>sizeof(double) = 8<li>sizeof(char) = 1<li>sizeof(int) = 4</ul><li><p>Write the cdoe to dynamically allocate an integer array named a having 5 elements<pre><code class=language-c>void someFunction() {
int * a = malloc(5 * sizeof(int));
}
</code></pre><li><p>Draw a memory diagram showing array a</ul><p>Stack: $\block$ a<br>$\downarrow$<br>Heap: |?|?|?|?|?|<ul><li>Write the code that gives the element at indexes 0, 1 and 2 a values of 0, 11 and 22
by using pointer dereferencing, indexing, and address arithmetic respectively.</ul><pre><code class=language-c>*a=0;
a[1]=11;
*(a+2)=22;
</code></pre><ul><li>Write the code that uses a pointer named p to give the element at index 3 a value of 33.</ul><pre><code class=language-c>int* p = a;
*(p+3) = 33;
</code></pre><ul><li>Write the code that frees array a’s heap memory.</ul><pre><code class=language-c>free(a);
a = NULL;
</code></pre><h2 id=pointer-caveats>Pointer Caveats</h2><h4 id=don-t-dereference-uninitialized-or-null-pointers>Don&rsquo;t dereference uninitialized or NULL pointers!</h4><pre><code class=language-c>int *p;
*p = 11;
int *q = NULL;
*q = 11;
</code></pre><h4 id=don-t-dereference-freed-pointers>Don&rsquo;t dereference freed pointers!</h4><pre><code class=language-c>int *p = malloc(sizeof(int));
int *q = p;
. . .
free(p);
. . .
*q = 11;
</code></pre><p><strong>dangling pointer</strong>: is a pointer with an address to heap memory that has been freed<h4 id=watch-out-for-memory-leaks>Watch out for memory leaks!</h4><pre><code class=language-c>int *p = malloc(sizeof(int));
int *q = malloc(sizeof(int));
. . .
p = q;
</code></pre><h4 id=be-careful-with-testing-for-equality>Be careful with testing for equality!</h4><p>assume p and q are pointers<ul><li>p = q compares nothing because it&rsquo;s assignment<li>p == q compares addresses in pointers<li>* p == * q compares values in pointees</ul><h4 id=don-t-return-addresses-of-local-variables>Don&rsquo;t return addresses of local variables!</h4><pre><code class=language-c>int *badcode() {
    int i = 11;
    return &amp;i;
}
int *badMakeIntArray(int size) {
    int a[size];
    return a;
}
</code></pre><h2 id=2d-arrays-on-the-heap>2D Arrays on the Heap</h2><h4 id=java-2d-array-of-arrays>Java 2D “Array of Arrays”</h4><p><code>int[][] m = new int[2][4];</code><h4 id=allocating-on-the-heap-a-2d-array-of-arrays-in-c>Allocating on the Heap a 2D “Array of Arrays” in C</h4><ol><li><p>Make a 2D array pointer named m (declare a pointer to an integer pointer).
<code>int** m;</code><li><p>Assign m an “array of arrays” (allocate of a 1D array of integer pointers of size ROWS).
<code>m = malloc(rows * sizeof(int*))</code><li><p>Assign each element in the “array of arrays” it own row of integers
(for each row allocate a 1D array of integers of size COLS).<pre><code class=language-c>for (int i=0; i&lt;ROWS; i++)
m[i] = malloc(COLS*sizeof(int));
</code></pre></ol><ul><li>What is the contents of m after the code below executes when ROWS is 2 and COLS is 4?</ul><pre><code class=language-c>for (int i = 0; i &lt; ROWS; i++) {
    for (int j = 0; j &lt; COLS; j++)
        m[i][j] = i + j;
</code></pre><ul><li><p>Can the following be used to replace m[i][j]?<p>a.) * ( * (m+i)+j) <strong>Yes</strong><br>b.) * ( * m + COLS * i + j) <strong>No</strong> requires a continuous array<li><p>Write the code to free the heap allocated 2D array?</ul><pre><code class=language-c>for (int i =0; u&lt;ROWS; i++)
    free(m[i]);
free(m)
</code></pre><h4 id=free-the-components-of-your-heap-memory>Free the components of your heap memory</h4><p><strong>In reverse order of their allocation</strong><h2 id=array-caveats>Array Caveats</h2><h4 id=arrays-have-no-bounds-checking>Arrays have no bounds checking!</h4><pre><code class=language-c>int a[5];
for (int i = 0; i &lt; 11; i++)
    a[i] = 0;
</code></pre><h4 id=arrays-cannot-be-return-types>Arrays cannot be return types!</h4><pre><code class=language-c>int[] makeIntArray(int size) {
    return malloc(sizeof(int) * size);
}
</code></pre><h4 id=not-all-2d-arrays-are-alike>Not all 2D arrays are alike!</h4><ul><li>What is the layout for 2D arrays on the stack?<li>What is the layout for 2D arrays on the heap?</ul><h4 id=an-array-argument-must-match-its-parameter>An array argument must match its parameter!</h4><h4 id=statically-allocated-arrays-require-all-but-their-first-dimension-specified>Statically allocated arrays require all but their first dimension specified!</h4><pre><code class=language-c>int a[2][4] = {{1,2,3,4},{5,6,7,8}};
printIntArray(a);
</code></pre><ul><li>Which of the following are type compatible with a ?</ul><pre><code class=language-c>void printIntArray(int a[2][4]) { . . . } // Y
void printIntArray(int a[8][4]) { . . . } // Y
void printIntArray(int a[][4]) { . . . } // Y
void printIntArray(int a[4][8]) { . . . } // N Col too big
void printIntArray(int a[][]) { . . . } // N Incomplete type
void printIntArray(int (*a)[4]) { . . . } // Y
void printIntArray(int **a) { . . . } // N
</code></pre><a class=readmore href=/2017/9/21/cs354-05.html title="CS354-05 Array on the heap">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-07 A* Search</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/20/cs540-07.html title="September 20, 2017">September 20, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><a class=readmore href=/2017/9/20/cs540-07.html title="CS540-07 A* Search">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-04 C String &amp; Arrays</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/19/cs354-04.html title="September 19, 2017">September 19, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=today>Today</h2><ul><li>C String<li>2D array and pointers</ul><h2 id=c-strings>C Strings</h2><h4 id=what-a-string-is>What? A String is</h4><ul><li>A sequence of chars whose end is marked with a null char (<code>\0</code>).<li>Represented as a one dimensional array of chars</ul><h4 id=how>How?</h4><ul><li>Declare a string variable, named str1 and initialize it with &ldquo;CS 354&rdquo;</ul><pre><code class=language-c>void someFunction() {
    char str1[11] = &quot;CS 354&quot;;
    // Copy the string literal into str1's mem
}
</code></pre><ul><li>Complete the memory diagram of str 1</ul><table><thead><tr><th align=center>C<th align=center>S<th align=center><th align=center>3<th align=center>5<th align=center>4<th align=center>\0<th align=center>?<th align=center>?<th align=center>?<th align=center>?<tbody></table><hr><h6 id=the-identifier-of-a-string>The identifier of a string</h6><p>Correspond to address of elem 0.<ul><li>Declare a string pointer, named sptr1, and initialize it with &ldquo;CS 354&rdquo;</ul><pre><code class=language-c>char *sptr1 = &quot;CS 354&quot;;
</code></pre><ul><li>Draw a memory diagram for the code.</ul><p><em>sptr1</em><br>$\downarrow$<table><thead><tr><th align=center>C<th align=center>S<th align=center><th align=center>3<th align=center>5<th align=center>4<th align=center>\0<tbody></table><hr><h4 id=string-caveats>STRING CAVEATS!</h4><ul><li>Assume both code fragments below are added to somefunction above,
what happens when the code is attempted to be compiled and run?</ul><ol><li><code>str1 = &quot;folderol&quot;;</code><ul><li>Compiler Error</ul><li><code>sptr1 = &quot;mumpsimus&quot;;</code><ul><li>No problems</ul></ol><p><strong>Only string pointer <em>variables</em> can be <em>assigned</em> a new string&rsquo;s address</strong><h2 id=string-h>String.h</h2><h4 id=what-string-h-is>What ? string.h is</h4><ul><li>A collection of useful functionsthat manipulate c strings</ul><p><code>int strlen(const char *str)</code><br>Returns the length of string <strong>str</strong> up to but <em>not including the null character</em>.<p><code>char *strcpy(char *dest, const char *src)</code><br>Copies the string pointed to by <strong>src</strong> to the memory pointed to by <strong>dest</strong>
and terminates with the null character.<p><code>char *strcat(char *dest, const char *src)</code><br>Appends the string pointed to by <strong>src</strong> to the end of the string pointed to by <strong>dest</strong>
and terminates with the null character.<p><code>int strcmp(const char *str1, const char *str2)</code><br>Compares the string pointed to by str1 to the string pointed to by str2.<br>Like java,<ul><li>negative if str1 comes before str2<li>0 if same<li>positive if str1 comes after str2</ul><blockquote><p><strong>dest</strong> must be large enough to hold result string or buffer overflow</blockquote><h4 id=more-string-caveats>MORE STRING CAVEATS!</h4><ul><li>Assume each code fragments below is added to somefunction on the prior page,
what happens when the code is attempted to be compiled and run?</ul><ol><li><code>strcpy(str1, &quot;formication&quot;);</code><ul><li>buffer overflow, null char written out of the array</ul><li><code>strcpy(sptr1, &quot;vomitory&quot;);</code><ul><li>Compiles, may be the same as $\uparrow$ or segfault if sptr&rsquo;s memory is read only</ul></ol><p><strong>Use strcpy (or strncpy) to copy a string into <em>writable</em> memory of another string</strong><h2 id=recall-2d-array>Recall 2D array</h2><pre><code class=language-c>void someFunction(){
    int m[2][4] = {{0,1,2,3},{4,5,6,7}};
}
</code></pre><p>m:<table><thead><tr><th align=center>0<th align=center>1<th align=center>2<th align=center>3<tbody><tr><td align=center>4<td align=center>5<td align=center>6<td align=center>7</table><hr><ul><li>What is output by this code fragment?</ul><pre><code class=language-c>for (int i = 0; i &lt; 2; i++) {
    for (int j = 0; j &lt; 4; j++) {
        printf(&quot;%i&quot;, m[i][j]);
    }
    printf(&quot;\n&quot;);
}
</code></pre><pre><code>0123
4567
</code></pre><p><strong>Memory is a sequence of bytes</strong><p><strong>Statically allocated 2D arrays are allocated as a continuous block of memory in role major order</strong><p>layout is row by row<h2 id=2d-array-and-pointers>2D array and pointers</h2><h4 id=address-arithmetic-for-statically-allocated-2d-arrays>Address Arithmetic for Statically Allocated 2D Arrays</h4><ul><li>Which of the following are equivalent to m[i][j]?</ul><p>a. (*(m+i))[j]<br>- addr arith to row i and index to col j<br>b. *(m[i]+j)<br>- index to row i, addr arith to col j<br>c. *( *(m+i)+j)<br>- full addr arith<h6 id=m-i-j-m-i-j>m[i][j] = * ( * (m+i) + j )</h6><ol><li>Compute row i address<ul><li>start at array&rsquo;s address<li>add offset to row i; auto scaled by the row size</ul><li>dereference the address to access row i<li>compute element j address in row i<ul><li>add offset to col j in row i; auto scaled by elem size</ul><li>dereference the address to access element in row i column j</ol><h6 id=m-0-0-m>m[0][0] = * * m</h6><ul><li>What type is * * m? <strong>[int]</strong><li>What type is * m? <strong>[int *]</strong> = <strong>m[0]</strong> = addr of start of row<li>What type is m? <strong>[int * *]</strong> for a statically allocated array 2D<li>* m is the addr of row 0 and * * m is also the addr of row 0</ul><h6 id=m-i-j>m[i][j]</h6><p>= * ( * m + cols * i + j )<br><em>scaling required for row size</em><p>For 2d array in row-major order</p><a class=readmore href=/2017/9/19/cs354-04.html title="CS354-04 C String &amp; Arrays">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-06 A* Search</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/18/cs540-06.html title="September 18, 2017">September 18, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h4 id=edge-cost-c-s-s>Edge cost c(s, s&rsquo;)</h4><h4 id=path-cost>Path cost</h4><ul><li>best path from init to s: g(s)<li>best path from s to goal: h(s)</ul><pre><code class=language-mermaid>graph LR;
    init --&gt; s1;
    init --&gt; s3;
    init --&gt; s5;
    s1 --&gt; s2;
    s3 --&gt; s4;
    s5 --&gt; s6;
    s2 --3--&gt; s7;
    s4 --2--&gt; s7;
    s6 --1--&gt; s7;
    s7 --&gt; goal;
</code></pre><p>g(s2) = g(s4) = g(s6) = 2<br>h(s2) = 4<br>h(s4) = 3<br>h(s6) = 2<p><strong>h(): Heuristic function</strong> --&gt; informed search<p>Priority queue: put g(s)+h(s) instead of g(s)<blockquote><p>Ex.</blockquote><table><thead><tr><th align=center>8<th align=center>1<th align=center>3<tbody><tr><td align=center>2<td align=center><td align=center>4<tr><td align=center>5<td align=center>6<td align=center>7</table><p>goal:<table><thead><tr><th align=center>1<th align=center>2<th align=center>3<tbody><tr><td align=center>4<td align=center>5<td align=center>6<tr><td align=center>7<td align=center>8<td align=center></table><p>Lower bound for h(s): sum of manhattan distance for all the tiles to get to goal place<p>$$H(s)= \sum_{t=1}^8 {| r_1^t - r_1^t| + |C_1^t - C_2^t|}$$<h2 id=a-search-uparrow>A* Search $\uparrow$</h2><ul><li>g(s) + h(s) in priority queue<li>$\forall h(s)$ must be a lower bound of the true h (h is admissible)<li>fail if h is not admissible</ul><a class=readmore href=/2017/9/18/cs540-06.html title="CS540-06 A* Search">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-05 Path Checking DFS</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/15/cs540-05.html title="September 15, 2017">September 15, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=dfs-in-graph-with-loops>DFS in graph with loops</h2><ol><li>Method 1: keep CLOSED/visited, memory X<li>Method 2: Path checking DFS</ol><h2 id=path-checking-dfs>Path checking DFS</h2><pre><code class=language-mermaid>graph TD;
    1 --&gt; 2;
    1 --&gt; 3;
    2 --&gt; 4;
    2 --&gt; 5;
    3 --&gt; 6;
    3 --&gt; 7;
</code></pre><p>Primary stack &amp; Path list<table><thead><tr><th align=center>primary<tbody><tr><td align=center>7_3<tr><td align=center>6_3<tr><td align=center>2_1</table><table><thead><tr><th align=center>Path<tbody><tr><td align=center>1_0<tr><td align=center>3_1<tr><td align=center>6_3</table><h4 id=path-checking-dfs-interative-deepening>(Path-checking DFS)Interative Deepening</h4><p>Recommended method.<h1 id=informed-search>Informed Search</h1><p>G=(V,E)<p>Edge Cost = cost(S, S&rsquo;)<br>Cost from initial state to S<br>(1, 3, 2) = cost(1,3) + cost(3,2)<ul><li><strong>g(s)</strong>: the cheapest path cost from init to S<li><strong>h(s)</strong>: the cheapest path cost from S to a goal</ul><p>g(s)+h(s): cheapest path from initial to goal through s</p><a class=readmore href=/2017/9/15/cs540-05.html title="CS540-05 Path Checking DFS">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-03 Pointers</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/14/cs354-03.html title="September 14, 2017">September 14, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=today>Today</h2><ul><li>Practive Pointer Basics<li>Recall 1D arrays<li>1D arrays and pointers<li>Parameters and Pointers</ul><h2 id=practice-pointer-basics>Practice Pointer Basics</h2><pre><code class=language-c>void someFunction() {
    int* ptr1, ptr2;
}
</code></pre><h5 id=how-many-pointer-variables-are-declared-in-the-code-above>How many pointer variables are declared in the code above?</h5><p><strong>1</strong><ul><li>ptr1 is a pointer<li>ptr2 is an int var</ul><pre><code class=language-c>int *ptr1, *ptr2;
</code></pre><h5 id=what-address-is-stored-in-ptr1>What address is stored in ptr1?</h5><p>DANGER: it&rsquo;s uninitialized.
It may appear to be 0.
But it&rsquo;s actually whatever bit pattern was in the memory.<h5 id=write-the-code-to-initialize-ptr1-with-address-0>Write the code to initialize ptr1 with address 0</h5><pre><code class=language-c>ptr1 = 0; // or ptr1 = 0x0;
// or
ptr1 = NULL; // must include one of c's standard libraries
</code></pre><h5 id=given-i-below-write-the-code-to-make-ptr1-point-at-i>Given i below, write the code to make ptr1 point at i</h5><pre><code class=language-c>int i = 22;
ptr1 = &amp;i;
</code></pre><h5 id=write-the-code-to-display-ptr1-s-pointee-s-value>Write the code to display ptr1&rsquo;s pointee&rsquo;s value</h5><pre><code class=language-c>printf(&quot;%d\n&quot;, *ptr1);
</code></pre><h5 id=what-happens-if-the-code-above-executes-when-ptr1-is-null>What happens if the code above executes when ptr1 is NULL</h5><p>Segmentation Fault (Core Dumped)<h5 id=write-the-code-to-display-ptr1-s-value>Write the code to display ptr1&rsquo;s value</h5><pre><code class=language-c>printf(&quot;%p\n&quot;, ptr1);
</code></pre><p>It is usually not useful to know a pointer&rsquo;s exact value,
which is why we draw arrows instead.<h5 id=what-does-the-code-below-do>What does the code below do?</h5><pre><code class=language-c>int **q = $ptr1;
</code></pre><p>it creates a pointer to a pointer to an integer.<h2 id=recall-1d-arrays>Recall 1D arrays</h2><h4 id=what>What?</h4><p>An array variable is<ul><li>A compound unit of storage having parts called elements<li>it&rsquo;s accessed using its identifier and indexing to get to a particular element<li><strong>stack</strong> allocated as a continuous, fixed-size block of memory</ul><h4 id=why>Why?</h4><ul><li>For storing a collection of data of the same type with fast access to its elements.<li>Because it&rsquo;s much easier to declare individual variables</ul><h4 id=how>How?</h4><pre><code class=language-c>void someFunction() {
    int a[5];
}
</code></pre><h6 id=how-many-integer-elements-have-been-allocated-memory>How many integer elements have been allocated memory?</h6><p><strong>5 stack allocated elements</strong><h6 id=write-the-code-that-gives-the-element-at-index-1-a-value-of-11>Write the code that gives the element at index 1 a value of 11</h6><pre><code class=language-c>a[1] = 11;
</code></pre><h6 id=draw-a-memory-diagram-showing-array-a>Draw a memory diagram showing array a</h6><table><thead><tr><th align=center>0<th align=center>1<th align=center>2<th align=center>3<th align=center>4<tbody><tr><td align=center>?<td align=center>11<td align=center>?<td align=center>?<td align=center>?</table><p>DANGER: elements are undefined/uninitialized<h4 id=the-name-of-an-array>The name of an array</h4><p><strong>Correspond to the address of element 0 in C.</strong><h2 id=1d-array-and-pointers>1D Array and Pointers</h2><h4 id=given>Given:</h4><pre><code class=language-c>void someFunction() {
    int a[5];
}
</code></pre><p><strong>Basic Memory Diagram</strong><table><thead><tr><th align=center>0<th align=center>1<th align=center>2<th align=center>3<th align=center>4<tbody><tr><td align=center>?<td align=center>?<td align=center>?<td align=center>?<td align=center>?</table><p><strong>Linear Memory Diagram</strong><table><thead><tr><th align=center>LMD<tbody><tr><td align=center><tr><td align=center><tr><td align=center><tr><td align=center>&hellip;<tr><td align=center>Lower Addr</table><h4 id=address-arithmetic>Address Arithmetic</h4><h6 id=a-i-equiv-a-i>a[i] $\equiv$ *(a+i)</h6><ol><li><p>compute address<ul><li>starts at array name (addr of elem 0)<li>add the offset to elem <em>i</em>, which is automatically scaled for the elem&rsquo;s size<li>0x_20 + 2*4 = 0x_20 + 8 = 0x_28</ul><table><thead><tr><th align=center>LMD<tbody><tr><td align=center>00<tr><td align=center>00<tr><td align=center>00<tr><td align=center>16</table><li><p>dereference computed address to access the element&rsquo;s value<ul><li>a[i] = *(a+i);<li>or to assign it a value<li>Note ()&rsquo;s name are required</ul></ol><ul><li>Write the code that uses address arithmetic to give the element at index 2 a value of 22</ul><pre><code class=language-c>*(a+2) = 22;
</code></pre><ul><li>Write the code that gives the element at index 0 a value of 0</ul><pre><code class=language-c>*a = 0;
</code></pre><h4 id=using-a-pointer>Using a pointer</h4><ul><li>Write the code to create a pointer p having the address of element 0 in array a</ul><pre><code class=language-c>int *p = a;
</code></pre><ul><li>Now write the code that uses p to give the element in a at index 3 a value of 33</ul><pre><code class=language-c>*(p+3) = 33;
</code></pre><p><strong>Pointers and Arrays are strongly related in C</strong><h2 id=parameters-and-pointers>Parameters and Pointers</h2><h4 id=what-is-output-by-the-code-below>What is output by the code below?</h4><pre><code class=language-c>void f(int pv1, int *pv2, int *pv3, int pv4[]) {
    int lv = pv1 + *pv2 + *pv3 + pv4[0];
    pv1 = 11;
    *pv2 = 22;
    *pv3 = 33;
    pv4[0] = lv;
    pv4[1] = 44;
}
int main(void) {
    int lv1 = 1, lv2 = 2;
    int *lv3;
    int lv4[] = {4,5,6};
    lv3 = lv4 + 2;
    f(lv1, &amp;lv2, lv3, lv4);
    printf(&quot;%i,%i,%i\n&quot;,lv1,lv2,*lv3);
    printf(&quot;%i,%i,%i\n&quot;,lv4[0],lv4[1],lv4[2]);
    return 0;
}
</code></pre><p>lv: 1, 2, 6, [4,5,6]<br>f(1, ?, 6, [4, 5, 6])<br>lv = 1 + 2 + 6 + 4<p>1 22 33<h4 id=call-stack-tracing>Call Stack Tracing</h4><ul><li>Manually tracing code with functions in a manner that mimics the machine<li>Each function gets a box when it is called: activation record / stack frame<li>Top box on the stack is executing and those below are suspended waiting for their call-ee to return</ul><h4 id=pass-call-by-value>Pass/Call-by-value</h4><ul><li>Arg is copied into its corresponding param<li>used with basic types (scalars) since they&rsquo;re fast to copy</ul><h4 id=changing-the-parameter>Changing the parameter</h4><p>Does not change its args<h4 id=passing-an-address-argument>Passing an address argument</h4><ul><li>The arg is copied into its param which must be a pointer<li>used with complex types to avoid copying<li>used to enable functions to change args&rsquo;s values</ul><h4 id=a-pointer-parameter>A pointer parameter</h4><ul><li><strong>can</strong> change its arg by accessing its pointee with dereferencing.</ul><a class=readmore href=/2017/9/14/cs354-03.html title="CS354-03 Pointers">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-04 Iterative Deepening</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/13/cs540-04.html title="September 13, 2017">September 13, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=iterative-deepening>Iterative Deepening</h2><ul><li>DFS, stop expanding at level 1, $b^0$<li>DFS, stop expanding at level 2, $b^0 + b^1$<li>&hellip;<li>DFS, stop expanding at level b. $b^0 + &hellip; + b^d$</ul><p>Time Complexity: $d*b^0 + (d-1)*b^1 + &hellip; + b^{b-1}$<h2 id=search-table-v2>Search Table V2</h2><table><thead><tr><th align=center><th align=center>Guaranteed to find a goal ?<th align=center>Is it the optimal goal ?<th align=center>Time Complexity<th align=center>Space Complexity<tbody><tr><td align=center>BFS<td align=center>Yes<td align=center>Yes (no if cost not the same)<td align=center>$\sum_{x=1}^d b^{d-1} \sim O(b^d)$<td align=center>$b^d-1 \sim O(b^d)$<tr><td align=center>DFS<td align=center>No<td align=center>No<td align=center>(m finite) $O(b^m)$<td align=center>$m*b \sim O(mb)$<tr><td align=center>UCS<td align=center>Yes<td align=center>Yes<td align=center>$O(b^m)$<td align=center>$O(b^m)$<tr><td align=center>ID<td align=center>Yes<td align=center>Yes<td align=center>$O(b^d)$<td align=center>$O(b*d)$</table><p>So far, every search algorithm is <strong>Uninformed Search</strong><hr><h2 id=graph-is-not-a-tree>Graph is not a tree</h2><pre><code class=language-mermaid>graph TD;
    init --&gt; S;
    S --&gt; init;
    S --&gt; goal;
</code></pre><p>Visited {}, great space cost<p>Path checking DFS</p><a class=readmore href=/2017/9/13/cs540-04.html title="CS540-04 Iterative Deepening">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS252-04</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/13/cs252-04.html title="September 13, 2017">September 13, 2017</a></span>
<span class=category><a href=/category/CS252.html rel="category tag">CS252</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS252.html rel="category tag">#CS252</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><a class=readmore href=/2017/9/13/cs252-04.html title=CS252-04>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS354-02</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/12/cs354-02.html title="September 12, 2017">September 12, 2017</a></span>
<span class=category><a href=/category/CS354.html rel="category tag">CS354</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS354.html rel="category tag">#CS354</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h1 id=c-programming-language>C Programming Language</h1><h4 id=variables-and-functions-must-be-declared-before-they-re-used>Variables and functions must be declared before they&rsquo;re used.</h4><ul><li>It&rsquo;s OK to ignore return value.</ul><h4 id=passing-arguments>Passing Arguments</h4><ul><li>Argument: VALUE that passed in<li>Parameter: VARIABLE where the argument is stored in the called func</ul><p><strong>Pass-by-value</strong> - A copy of arg is stored in the corresonding param<h4 id=return-value>Return Value</h4><p><strong>Return-by-value</strong><h2 id=c-control-flow>C Control Flow</h2><h4 id=sequencing>Sequencing</h4><p>Evaluation<ul><li>Starts in main()<li>Flows top to bottom<li>Does one stmt after another</ul><p>Like java<ul><li>Statement;<li>{ BLOCK }<li>Scope Rules</ul><h4 id=selection>Selection</h4><p>Which value means true? true? 42? -17? 0?<p>No boolean types (true/false)<ul><li>non-zero value means true<li>0 means false</ul><p><em>dangling else</em><h6 id=switch>Switch</h6><p>Like java, but not with strings<h4 id=repetition>Repetition</h4><p>Like java<p>for varieble in for to behave like java, use compiler flags <code>-std=gnu99</code><h2 id=recall-variables>Recall Variables</h2><h4 id=what>What?</h4><p>A scalar variable is AKA primitives<p>A single unit of storage whose contents can change.</p><h4 id=aspects-of-a-variable>Aspects of a variable</h4><ul><li><strong>Identifier</strong>: name associated with var&rsquo;s address<li><strong>value</strong>: Info stored in var&rsquo;s memory<li><strong>address</strong>: Location in memory of the variable<li><strong>type</strong>: how to interpret the pattern of bits in a var&rsquo;s memory, e.g. signed ints are in two&rsquo;s complement.<li><strong>size</strong>: Number of butes needed to store a specific type, e.g. int is 4 bytes<li><strong>endianess</strong>: the ordering of bytes for a type &gt; 4 bytes<ul><li>little endian: we&rsquo;ll use least significant digits are at the lowest address<li>big endian: the opposite</ul></ul><p>Base 10: 11<br>Base 2 : 0000 0000 &hellip; 0000 1011<br>Base 16: 0 0 &hellip; 0 B<br>0x0000000B<h2 id=meet-pointers>Meet pointers</h2><h4 id=what-1>What?</h4><p>A <strong>pointer variable</strong> is<ul><li>a unit of storage whose contents is a memory address of a var, func, device, &hellip;<li>like java references but require they require more syntax and knowledge to use correctly</ul><h4 id=why>Why?</h4><ul><li>for indirect memory access (Thur.)<li>for indirect access to funcs (later)<li>because they are used extensively in c libraries &amp; OS code<li>for access to memory mapped hardware</ul><h4 id=how>How?</h4><pre><code class=language-c>void someFunction()
{
 int i = 11;
 int *ptr = NULL;
 ptr = &amp;i;
 *ptr = 22;
}
</code></pre><p>What&rsquo;s the ptr&rsquo;s<ul><li>Initial value? NULL 0x00000000<li>Address? 0x00000004<li>Type? int*<li>Size? 4 Bytes</ul><p><strong>Pointer</strong>: does the pointing (at address tail), declared using *<br><strong>pointee</strong>: is pointed at (at address head)<br><strong>address of</strong>: &amp; operater, which returns its operands addr<br><strong>dereferencing</strong>: using * operater to access a pointer&rsquo;s pointee indirectly</p><a class=readmore href=/2017/9/12/cs354-02.html title=CS354-02>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-03 Search</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/11/cs540-03.html title="September 11, 2017">September 11, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h1 id=search>Search</h1><h2 id=special-case-of-tree>Special case of TREE</h2><p>b: branch factor (out degree)<br>d: first depth of a goal<br>m: maximum depth of tree (could be infinity)<br>cost = 1 for all edges<blockquote><p>Search Table</blockquote><h2 id=cost-0>Cost &gt; 0</h2><p>Cost(goal) is sum of edges from init to goal<blockquote><p>02.md Precedure</blockquote><p>push Cost(S) into the fringe instead of just S<br>Do it w/ Heap<br>$\downarrow$<br><strong>Uniform cost search</strong><h2 id=search-table-v1>Search Table V1</h2><table><thead><tr><th align=center><th align=center>Guaranteed to find a goal ?<th align=center>Is it the optimal goal ?<th align=center>Time Complexity<th align=center>Space Complexity<tbody><tr><td align=center>BFS<td align=center>Yes<td align=center>Yes (no if cost not the same)<td align=center>$\sum_{x=1}^d b^{d-1} \sim O(b^d)$<td align=center>$b^d-1 \sim O(b^d)$<tr><td align=center>DFS<td align=center>No<td align=center>No<td align=center>(m finite) $O(b^m)$<td align=center>$m*b \sim O(mb)$<tr><td align=center>UCS<td align=center>Yes<td align=center>Yes<td align=center>$O(b^m)$<td align=center>$O(b^m)$</table><a class=readmore href=/2017/9/11/cs540-03.html title="CS540-03 Search">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS252-03 Binary System</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/11/cs252-03.html title="September 11, 2017">September 11, 2017</a></span>
<span class=category><a href=/category/CS252.html rel="category tag">CS252</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS252.html rel="category tag">#CS252</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=computer-is-a-binary-digital-system>Computer is a binary digital system.</h2><p>Digital system: finite number of symbols<br>Binary (base two) system: has two states: 0 and 1<p>Basic unit of information is the binary digit, or <strong>bit</strong><br>Values with more than two states require multiple bits.<ul><li>A collection of <strong>two</strong> bits has <strong>four</strong> possible states: 00, 01, 10, 11.<li>A collection of <strong>three</strong> bit has <strong>eight</strong> possible states.<li>A collection of <strong>n</strong> bits has <strong>$2^n$</strong> possible states.</ul><h2 id=cpu-and-memory-data-bus-address-bus>CPU and Memory - Data Bus, Address Bus</h2><pre><code class=language-mermaid>graph TD;
    Address Bus --&gt; Memory;
    Address Bus --&gt; I/O;
    Memory --&gt; CPU;
    CPU --&gt; Memory;
    Memory --&gt; I/O;
    I/O --&gt; Memory;
    CPU --&gt; Address Bus;
    Data Bus &lt;--&gt; CPU;
    Data Bus &lt;--&gt; Memory;
    Data Bus &lt;--&gt; I/O;
</code></pre><h2 id=what-kinds-of-data-do-we-need-to-represent>What kinds of data do we need to represent?</h2><p>Numbers, Text, Image, Sound, Logical, Instructions &hellip;<h4 id=data-type>Data Type</h4><ul><li><strong>Representation</strong> and <strong>operations</strong> within the computer</ul><h2 id=signed-integers>Signed Integers</h2><p>With n bits, we have 2^n distint values
- Assign about half to positive integers and half to negative
- that leaves two values: one for 0, and one extra<h4 id=positive-integers>Positive Integers</h4><ul><li>Just like unsigned - zero in most significant bit<li>00101 = 5</ul><h4 id=negative-integers>Negative integers</h4><ul><li>sign-magnitute - set top bit to show negative, other bits are the same as unsigned<ul><li>10101 = -5</ul><li>one&rsquo;s complement - flip every bit to represent negative</ul><h2 id=two-s-complement>Two&rsquo;s complement</h2><h4 id=problems-with-sign-magnitube-and-1-s-complement>Problems with sign-magnitube and 1&rsquo;s complement</h4><ul><li>two representation of 0 (+0 and -0)<li>arithmetic circuits are complex</ul><h4 id=two-s-complement-representation-developed-to-make-circuits-easy-for-arithmatic>Two&rsquo;s complement representation developed to make circuits easy for arithmatic.</h4><ul><li>for each positive number X, assign value to its negative such that X + (-X) =0 with normal addition, ignoring carry out</ul><h2 id=two-s-complement-representation>two&rsquo;s complement representation</h2><h4 id=if-number-is-positive-or-zero>If number is positive or zero,</h4><ul><li>normal binary representation, zeroes in upper bit(s)</ul><h4 id=if-number-is-negative>If number is negative,</h4><ul><li>start with positive number<li>flip every bit (take the one&rsquo;s complement)<li>then add one</ul><h2 id=two-s-complement-signed-integers>Two&rsquo;s complement signed integers</h2><h4 id=ms-bit-is-sign-bit-it-has-weight-2-n-1>MS bit is sign bit - it has weight - 2^n-1</h4><h4 id=range-of-an-n-bit-number-2-n-1-through-2-n-1-1>Range of an n-bit number: -2^n-1 through 2^n-1 -1</h4><h2 id=integer-representation-schemes>Integer representation schemes</h2><ul><li>Computers use a fixed number of bits to represent an integer<ul><li>8 bit, 16-bit, 32-bit, 64-bit</ul><li>There are two representation schemes for integers:<ul><li>Unsigned integers: can represent zero and positive integers<li>Signed Integers: can represent zero, positive and negative integers. Three</ul></ul><h2 id=sign-magnitude-representation>Sign-Magnitude representation</h2><ul><li>if the sign bit is 0 --&gt; the number is positive in value.<li>if the sign bit is 1 --&gt; the number is negative in value.</ul><a class=readmore href=/2017/9/11/cs252-03.html title="CS252-03 Binary System">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS252-02 Binary System</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/8/cs252-02.html title="September 8, 2017">September 8, 2017</a></span>
<span class=category><a href=/category/CS252.html rel="category tag">CS252</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS252.html rel="category tag">#CS252</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=computer-system-layers-of-abstraction>Computer System: Layers of Abstraction</h2><ul><li>Application Program<li>Algorithms<li>Language<li>---- Software / Hardware ----<li>Instruction Set Architecture (and I/O interface)<li>Microarchitecture<li>Circuits<li>Devices</ul><h2 id=turing-machine>Turing machine</h2><ul><li>ability to read/write symbols on an infinite &ldquo;tape&rdquo;<li>state transitions, based on current state and symbol</ul><blockquote><p>Every computation can be performed by some Turing machine (Turing&rsquo;s Thesis)</blockquote><h2 id=universal-turing-machine>Universal Turing Machine</h2><a class=readmore href=/2017/9/8/cs252-02.html title="CS252-02 Binary System">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-02 Introduction</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/8/cs540-02.html title="September 8, 2017">September 8, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h3 id=water-jugs>Water jugs</h3><h5 id=goal>Goal</h5><p>Many goal states<ol><li>Choose a single goal state<li>Larger goal set. Goal state as a boolean function</ol><h3 id=eight-puzzle>Eight puzzle</h3><ul><li>Successor functionL maximum of 4 possible move<li>cost: 1 for each move</ul><h2 id=assumption>Assumption</h2><ul><li>G = (V, E)<br><li>Special case: G is a tree, directed<br><li>b regular (b = branching factor or out degree)<br><li>complete tree<br><li>depth d</ul><h2 id=procedure>Procedure</h2><pre><code>S &lt;- init state
fringe/open/frontier F &lt;- S
while (F not empty)
    S &lt;- pop(F)
    if (goal_test(S) is true)
        success, stop
    F &lt;- succ(S)
end
report failure
</code></pre><h3 id=f-is-queue-fifo>F is queue (FIFO)</h3><p><strong>BFS</strong><h3 id=f-is-stack-lifo>F is stack (LIFO)</h3><p><strong>DFS</strong></p><a class=readmore href=/2017/9/8/cs540-02.html title="CS540-02 Introduction">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS540-01 Introduction</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/6/cs540-01.html title="September 6, 2017">September 6, 2017</a></span>
<span class=category><a href=/category/CS540.html rel="category tag">CS540</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS540.html rel="category tag">#CS540</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=introduction>Introduction</h2><ul><li>CS Data Structure<li>Calculas<li>Linear Algebra<li>Probability</ul><h2 id=search>Search</h2><h4 id=farmer-cabbage-sheep-dog>Farmer, Cabbage, Sheep, Dog</h4><table><thead><tr><th align=center>#<th align=center>C<th align=center>S<th align=center>D<th align=center>F<th align=center><tbody><tr><td align=center>0<td align=center>1<td align=center>1<td align=center>1<td align=center>1<td align=center>Initial State<tr><td align=center>1<td align=center>1<td align=center>0<td align=center>1<td align=center>0<td align=center><tr><td align=center>2<td align=center>1<td align=center>0<td align=center>1<td align=center>1<td align=center><tr><td align=center><td align=center><td align=center><td align=center><td align=center><td align=center><tr><td align=center>Goal<td align=center>0<td align=center>0<td align=center>0<td align=center>0<td align=center>Goal State</table><h4 id=graph>Graph</h4><p>graph G = (V,E)<pre><code class=language-mermaid>graph TD;
    1111 --&gt; 1010;
    1010 --&gt; 1011;
    1011 --&gt; 0010;
    1011 --&gt; 1000;
    0010 --&gt; 0111;
    1000 --&gt; 1101;
    0111 --&gt; 0100;
    1101 --&gt; 0100;
    0100 --&gt; 0101;
    0101 --&gt; 0000;
</code></pre><a class=readmore href=/2017/9/6/cs540-01.html title="CS540-01 Introduction">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS252-01 Introduction</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/9/6/cs252-01.html title="September 6, 2017">September 6, 2017</a></span>
<span class=category><a href=/category/CS252.html rel="category tag">CS252</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS252.html rel="category tag">#CS252</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><a class=readmore href=/2017/9/6/cs252-01.html title="CS252-01 Introduction">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-2x</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/5/3/math240-2x.html title="May 3, 2017">May 3, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><a class=readmore href=/2017/5/3/math240-2x.html title=Math240-2x>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-21</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/3/27/math240-21.html title="March 27, 2017">March 27, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=linear-congruences>Linear Congruences</h2><p>$ax \equiv b (mod m)$<br>m integer, $a,b \in Z$<p>a solution is some x that satisfies the equation<p>$x = \frac {a} {b}$<p>there are inverses:<br>a&rsquo; is inverse of a, if $a * a&rsquo; \equiv 1 (mod m)$</p><a class=readmore href=/2017/3/27/math240-21.html title=Math240-21>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-20</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/3/13/math240-20.html title="March 13, 2017">March 13, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><a class=readmore href=/2017/3/13/math240-20.html title=Math240-20>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-18</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/3/8/math240-18.html title="March 8, 2017">March 8, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p>f is $\omega(g)$ if $\exists D, e &gt; 0 S,T, |f(x)| \gt D(g(x)) for x \gt e $</p><a class=readmore href=/2017/3/8/math240-18.html title=Math240-18>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-17</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/3/1/math240-17.html title="March 1, 2017">March 1, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=matrices>Matrices</h2><a class=readmore href=/2017/3/1/math240-17.html title=Math240-17>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-16</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/24/math240-16.html title="February 24, 2017">February 24, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=cardinality>Cardinality</h2><p>How big is a given set<blockquote><p>Ex. Too many people in class, if 500 people sit down in a seat,</blockquote><h4 id=pigeon-hole-principle>Pigeon Hole Principle</h4><a class=readmore href=/2017/2/24/math240-16.html title=Math240-16>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-15</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/22/math240-15.html title="February 22, 2017">February 22, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><a class=readmore href=/2017/2/22/math240-15.html title=Math240-15>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-14</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/17/math240-14.html title="February 17, 2017">February 17, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=naive-set-theory>(Naive) set theory</h2><p>set identities</p><a class=readmore href=/2017/2/17/math240-14.html title=Math240-14>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS559 13</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/16/cs559-13.html title="February 16, 2017">February 16, 2017</a></span>
<span class=category><a href=/category/CS559.html rel="category tag">CS559</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS559.html rel="category tag">#CS559</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=rendering-pipeline>Rendering Pipeline</h2><p>Application --&gt; Command Stream --&gt; Vertex Processing --&gt; Transformation --&gt;
Rasterization --&gt; Fragments --&gt; Fragments Processing --&gt; Blending --&gt;
Framebuffer Image<h2 id=shading>Shading</h2><p>L = Ia(ambient) + Id(diffuse) + Is(specular)<p>Ambient: Constant<h2 id=diffuse-reflection>Diffuse Reflection</h2><p>Lambertian
Matte<h2 id=glsl-opengl-shading-language>GLSL - OpenGL Shading Language</h2><p>Shaders<ul><li>little programs that rest on the GPU<li>convert inputs to outputs<li>do not talk to each other, they only talk via data.</ul><p>C-Style language<pre><code class=language-glsl># VERSION VERSION-NUMBER
# VERSION 330 core

in type input_variable_name
in type input_variable_name

out type output_variable_name

uniform type uniform_variable_name //not essential

void main
{
    // Do some weird graphic staff
    // output processed stuff to output variable
}
</code></pre><a class=readmore href=/2017/2/16/cs559-13.html title="CS559 13">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-13</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/15/math240-13.html title="February 15, 2017">February 15, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=venn-diagrams>Venn Diagrams</h2><p>u: universal set: all object<blockquote><p>A,B are sets</blockquote><h4 id=terminalogy>Terminalogy</h4><p>Def: A is a subset of B($a \subseteq b$) if $\forall x (x \in A \to x \in B)$<h4 id=to-show-a-subseteq-b>To Show A $\subseteq$ B</h4><p>Show that if x belongs to A, then x belongs to B<blockquote><p>Ex. Show that the set of multiples of 4 is a subset of the even numbers<br>Converse: To show $A \subsetneq B$, find a single x $\in$ A not in B</blockquote><hr><blockquote><p>Thm. If S is a set then $\varnothing \subseteq S$ and $S \subseteq S$</blockquote><p><strong>Def: A is a proper subset of B $A \subset B$ if $A \subseteq B$ and $\exists x (x \in B \land x \notin A)$</strong><p><strong>Def: A = B if $A \subseteq B$ and $B \subseteq A$</strong><h4 id=size-of-a-set>Size of a set</h4><p><strong>Def: If there are exactly n distinct elements in S, then S is said to be finite with cardinality n, (|s|=n).</strong><br><strong>Def: S is infinite if it is not finite</strong><h4 id=powersets>PowerSets</h4><p>Def: given set S, the power set of S is the set of S is the set of all subsets, denoted P(S)<blockquote><p>Ex. $P(\varnothing) =$ {$\varnothing$}</blockquote><p>If |S| = n, |P(S)| = $2^n$<h4 id=cartisian-product-of-sets>Cartisian Product of sets</h4><p>ordered n tuples (a1, a2, a3, &hellip;, an)<blockquote><p>Ex. (a,b) $\ne$ (B,a) unless a=b</blockquote><p><strong>Def:The cartesian product of A&amp;B is the set</strong><br>$A \times B = $ {(a,b) | $(a \in A) \land (b \in B)$}<p><strong>Def: A subset R $\subseteq$ A $\times$ B is called a relation from A to B</strong><h4 id=notation>Notation</h4><p>$\forall x (x \in S \to P(x))$, write $\forall x \in S P(x)$<p>Given P a predicate with domain D, the truth set of P is {x $\in$ D | P(X)}<blockquote><p>Ex. The set of even numbers {x $\in$ Z | x=2n $\land$ ($n \in Z$)}</blockquote><h4 id=union-set>Union Set</h4><p><strong>Def: Union $A \cup B$ = {x| $x \in A \lor x \in B$}</strong><p><strong>Def: Intersection $A \cap B$ = {x | $x \in A \land x \in B$}</strong><p><strong>Def: Difference $A - B$ = {x | $x in A \land x \notin B$}</strong><p><strong>Def: Complement $\bar A = U-A$</strong><blockquote><p>TODO: Page 130</blockquote><a class=readmore href=/2017/2/15/math240-13.html title=Math240-13>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-12</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/13/math240-12.html title="February 13, 2017">February 13, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=chomp>Chomp</h2><ul><li>Start with an m*n array of cookies<li>two players. Each turn, a player eats as many cookies in a down/right bar as they want.<li>whoever eats the poison cookie loses.</ul><p>Claim 1: player 1 has a winning strategy. If not , player 2 has a winning strategy.<blockquote><p>Ex. for any n &gt;= 2, there is a prime number tetween n and 2n<br>Proof is not constructive</blockquote><h2 id=existence-and-uniqueness>Existence and Uniqueness</h2><blockquote><p>Ex. Claim: If a,b are real numbers a $\ne$ 0, then<br>$\exists$ a unique real number r satisfying ar+b=0<br><strong>Proof: Existence.</strong><br>Let $r=-b/a$<br>This a solution exists.<br><strong>Uniqueness.</strong><br>Imagine r and r&rsquo; are both solution, then
$ar+b = 0 = ar&rsquo; + b$
$r=r&rsquo;$</blockquote><p><br><blockquote><p>Ex. If x and y are distinct positive real numbers, then $\frac {x + y} {2} &gt; \sqrt {xy}$<p>Do some examples.<br>x=1 and y=9 then 5 &gt; 3<p>Imagine true<br>$\frac {x + y} {2} &gt; \sqrt {xy}$<br>$\frac {(x + y)^2} {4} &gt; xy$<br>$(x+y)^2 &gt; 4xy$<br>$x^2 + 2xy + y^2 &gt; 4xy$<br>$x^2 - 2xy + y^2 &gt; 0$<br>$(x-y)^2 &gt; 0$<p>Proof: Since x and y are distinct, x-y $\ne$ 0 and thus<br>$(x-y)^2 &gt; 0$. Expand to get<br>$x^2 -2xy + y^2 &gt;0$ Add 4xy to get<br>$(x+y)^2 &gt; 4xy$<br>&hellip;<br>$\frac {x + y} {2} &gt; \sqrt {xy}$</blockquote><h2 id=set-chadpter-2-1>Set (chadpter 2.1)</h2><h4 id=definition>Definition</h4><p>A set is an unordered collection of object, called elements.<br>We write $a \in A$ for a belongs to A. $a \notin A$ for a does not belong to A.<blockquote><p>Ex. S=set of letters in my last name {e,r,m,a,n}<p>Ex. T = set of even intgers.<br>T={&hellip;,-4,-2,0,2,4,6,8,&hellip;}<br>T={x | x is a even integer}<br>T={x$\in$Z | x is even}</blockquote><ul><li>Z: set of integers<li>R: set of real numbers<li>R+: set of positive real numbers<li>Z+: set of positive integers<li>Q: rational numbers.<li>N: non-negative integers.<li>C: Complex numbers</ul><h4 id=empty-set-varnothing>empty set $\varnothing$</h4><p>Denoted {} or $\varnothing$<p>If S has a finite number of elements, then we write |s| for that number.<blockquote><p>Ex. |{e,r,m,a,n}| = 5</blockquote><a class=readmore href=/2017/2/13/math240-12.html title=Math240-12>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-11</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/10/math240-11.html title="February 10, 2017">February 10, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=exhaustion>Exhaustion</h2><h4 id=proof-by-cases>Proof by cases</h4><blockquote><p>Prove: $(n+1)^3 \ge 3^n$ if n is positive int, with $n \le 4$<br>n=1: $(1+1)^3 = 2^3 = 8 \, , 3^1 = 3$<br>n=2: $(2+1)^3 = 3^3 = 27 \, , 3^2 = 9$<br>n=3: $(3+1)^3 = 4^3 = 64 \, , 3^3 = 27$<br>n=4: $(4+1)^3 = 5^3 = 125 \, , 3^4 = 81$</blockquote><hr><blockquote><p>Ex. Only consecutive positive integer &lt;= 100 that are both powers are 8 and 9<br>Write down all the power and check.</blockquote><hr><blockquote><p>Ex. if n integer $n^2 \ge n$<br>n=0: $0^2 \ge 0$<br>$n \ge 1$: divide both side by n<br>n &lt; 0: &hellip;</blockquote><hr><blockquote><p>Ex. Thm. The final digit of a square is never 2,3,7,8</blockquote><hr><blockquote><p>Ex. Thm. No integer solution to $x^2+3y^2=8$</blockquote><h3 id=symmetry>Symmetry</h3><blockquote><p>Ex. if x,y are integers and both xy and x+y are even
then x and y are both even<br>Proof the contrapositive:
If x or y is odd, xy or x+y is odd.<p>Assume one of x or y is odd.<br>If we do the x odd case, we know how the other case goes.</blockquote><a class=readmore href=/2017/2/10/math240-11.html title=Math240-11>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS559 10</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/9/cs559-10.html title="February 9, 2017">February 9, 2017</a></span>
<span class=category><a href=/category/CS559.html rel="category tag">CS559</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS559.html rel="category tag">#CS559</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p>Orthographic Projection</p><a class=readmore href=/2017/2/9/cs559-10.html title="CS559 10">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-10</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/8/math240-10.html title="February 8, 2017">February 8, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><blockquote><p>Thm. n = ab, then $a \le \sqrt{n} \, or \, b \le \sqrt{n}$<p>Thm. if $n^2$ is odd, then n is odd (last time we proved the converse directly)<br><strong>First Try direct argument</strong><br>Assume $n^2$ is odd, i.e. $n^2=2k+1$, so $n=sqrt{2k+1}$. (Abandoned)<br><strong>2nd try, prove contrapositive.</strong><br>Assume n not odd, i.e. n is even, and show that $n^2$ is even.<br>If n even, n=2k for suck k, so, $n^2 = (2k)^2 = 2*(2k^2)$</blockquote><h4 id=recall>Recall</h4><p>A contradiction is a proposition that is always false.<h4 id=prove-by-contradiction>Prove by contradiction</h4><p>Suppose we want p. Suppose there is a contradiction $q \equiv F$ s.t. $\lnot p \rightarrow q$ is true
. This means $\lnot p$ is false, this means p is true<blockquote><p>Basic example:<br>Thm. if 3n+2 is odd, then n is odd<br><strong>Prove by contradiction</strong><br>Assume the theorem is false. $\lnot (p \rightarrow q) \equiv p \land \lnot q$
which means 3n+2 is odd and n is even. n=2l for some l. Then 3<em>(2l) + 2 is odd.
But 3</em>(2l)+2 = 2(3l+1) is even. This is contradiction.<hr>Thm. $\sqrt{2}$ is irrational. (A number is rational if r = $\frac {a}{b}$ for integers a,b)<br>Proof. Suppose the theorem is false. So $\sqrt {2} = \frac{a}{b}$.<br>Assume (as we may) that $\frac {a}{b}$ is in lowest terms,<br>i.e. a and b have no common factors.<br>$2 = \frac {a^2}{b^2} \rightarrow 2b^2 = a^2$, so $a^2$ is even , so a is even. a = 2c, for some c.<br>$2b^2 = 4c^2 \rightarrow b^2 = 2c^2$, so b is even.<br>So a and b are not the lowest terms.</blockquote><h4 id=brovwer-s-fixed-point-thm><em>Brovwer&rsquo;s fixed point thm</em></h4><h2 id=topology>Topology</h2><h4 id=mention>Mention</h4><p>$p \leftrightarrow q$ p iff q
$p \rightarrow q$ and $q \rightarrow p$<p>The following are equivalent ABBR: TFAE<p>To proof $p \leftrightarrow q \leftrightarrow r$<br>proof $p \to q \to r$<blockquote><p>Ex. Thm. TFAE<ul><li>p: n even<li>q: n-1 odd<li>r: n^2 even</ul><p>$p \to q , q \to r , r \to p$</blockquote><hr><blockquote><p>Ex. Every positive integer is a sum of two squares<br>False: 3 is a counterexample.</blockquote><a class=readmore href=/2017/2/8/math240-10.html title=Math240-10>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-09</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/6/math240-09.html title="February 6, 2017">February 6, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h4 id=rules-of-inference>Rules of inference</h4><p>$$
\rightarrow
$$<p>$$
\begin{array}
p \rightarrow r
r \rightarrow q
\hline
p \rightarrow q$$
\end{array}
$$
p \lor q
\lnot p \lor r
\hline
q \lor r
$$<p>Jasmine is sailing or it&rsquo;s not snowing<blockquote><p>Ex. It is not sunny and it&rsquo;s colder than yesterday<br>We&rsquo;ll go swimming only if it&rsquo;s sunny<br>If we don&rsquo;t go swimming, we&rsquo;ll go canoeing<br>If we canoe, we&rsquo;ll be home by sunset.</blockquote><p>$$
\begin{array}{c|lcr}
n &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \
\hline
1 &amp; 0.24 &amp; 1 &amp; 125 \
2 &amp; -1 &amp; 189 &amp; -8 \
3 &amp; -20 &amp; 2000 &amp; 1+10i
\end{array}
$$<p>$((p \rightarrow q) \land q) \rightarrow p$ <strong>is not a tautology</strong><p>$((p\rightarrow q) \land \lnot p) \rightarrow \lnot q$ <strong>is not a tautology</strong><h4 id=rules-of-quantifiers>Rules of Quantifiers</h4><p>&hellip;<h4 id=some-title>Some title</h4><blockquote><p>Ex. Assume someone in the class hasn&rsquo;t read the book<br>Everyone passed the exam.<br><strong>Conclude:</strong> There is someone in this class who passed the exam and hasn&rsquo;t read the book.<br>C(x): x in the class<br>B(x): x read the book<br>P(X): x passed the exam<br>$\exists x (C(x) \land \lnot B(x))$<br>$C(a) \land \lnot B(a)$<br>$C(a)$<br>$\forall x (C(x) \rightarrow P(a))$</blockquote><a class=readmore href=/2017/2/6/math240-09.html title=Math240-09>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS559 09</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/6/cs559-09.html title="February 6, 2017">February 6, 2017</a></span>
<span class=category><a href=/category/CS559.html rel="category tag">CS559</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS559.html rel="category tag">#CS559</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=4-key-ideas>4 Key Ideas</h2><h4 id=1-coordinate-system>1. Coordinate System</h4><ul><li>Transformations<li><p>Hierarchical Modeling<h4 id=2-homogeneous-coordinate>2. Homogeneous Coordinate</h4><p>(x, y, z) --&gt; (x, y, z, 1)<h4 id=3-viewing-transforms>3. Viewing Transforms</h4><h4 id=4-primitive-base-rendering>4. Primitive-Base Rendering</h4></ul><h2 id=basic-ideas>Basic Ideas</h2><h4 id=coordinate-system>Coordinate System</h4><h4 id=transformation>Transformation</h4><ul><li>Move between different coordinate systems<li>Move object around</ul><h4 id=matrices>Matrices</h4><h2 id=math>Math</h2><h4 id=points-vectors-matrices>Points, Vectors, Matrices</h4><h2 id=dot-product>Dot Product</h2><p>(a1, a2, a3)<br>(b1, b2, b3)<p>a dot b = a1 * b1 + a2 * b2 + a3 * b3<h2 id=cross-product>Cross Product</h2><h2 id=primitive-based-rendering>Primitive-Based Rendering</h2><p>2D Object in image<h2 id=what-does-it-take>What does it take?</h2><ol><li>Put a 3D primitive in world <strong>Modeling</strong><li>Figure out its color <strong>Shading</strong><li>Position relative to the eye <strong>Viewing / Camera Transformation</strong><li>Remove objects behind/off-screen <strong>Clipping</strong><li>Identify what goes on screen <strong>Projection</strong><li>Figure out if something blocks it <strong>Visibility / Occlusion</strong><li>Draw the 2D primitive <strong>Rasterization</strong></ol><h4 id=3d-world-2d-screen>3D world --&gt; 2D screen</h4><ol><li>3D Points map to 2D points<li>3D lines map to 2D lines<li>3D triangles map to 2D triangles</ol><p>curve/ellipse --may not--&gt; 2D equivalent<h2 id=projection>Projection</h2><ol><li>Orthographic<ul><li>Parallel lines remain parallel<li>No difference between objects far away and close by<ul><li>we lose that information</ul></ul><li>Perspective<ul><li>3D parallel lines <strong>do NOT</strong> remain parallel.
The meeting point is called vanishing point.<li>Distorts primitive to show depth information.</ul></ol><a class=readmore href=/2017/2/6/cs559-09.html title="CS559 09">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-08 Nesting</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/3/math240-08.html title="February 3, 2017">February 3, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h4 id=the-order-of-quantifiers>The order of Quantifiers</h4><blockquote><p>Ex. $\forall x \exists y (x+y=0) \rightarrow$ true<p>Ex. $\exists y \forall x (x+y=0) \rightarrow$ false</blockquote><hr><blockquote><p>Ex. $\forall x \forall y (x+y=y+x)$<br>$\equiv \forall y \forall x (x+y=y+x)$</blockquote><h4 id=negating>Negating</h4><p>$\lnot (\forall x \exists y P(x,y))$<br>$\equiv \exists x \lnot (\exists y P(x,y))$<br>$\equiv \exists x \forall y \lnot P(x,y)$<blockquote><p>Every lamp has a switch$\ne$<br>There is a switch for every lamp<hr>Ex. &ldquo;There is a person that has taken a flight on every airline&rdquo;<br>P(w,f): w(person) has flown on f<br>Q(f,a): f is a flight for a(airline)<br>$\exists w \forall a \exists f (P(w,f) \land Q(f,a))$<br><hr>Ex. What is the negation<br>$\lnot (\exists w \forall a \exists f (P(w,f) \land Q(f,a)))$<br>$\equiv \forall w \lnot (\forall a \exists f (P(w,f) \land Q(f,a))$<br>$\equiv \forall w \exists a \forall f \lnot (P(w,f) \land Q(f,a))$<br>$\equiv \forall w \exists a \forall f (\lnot P(w,f) \lor \lnot Q(f,a))$<br><hr>Ex. $\exists w \exists f \forall a (P(w,f) \land Q(f,a))$<hr>Ex. $\lim\limits_{x \to a} f(x) = L$<br>for every $\varepsilon$ there is a $\delta &gt; 0$<br>$\forall \varepsilon &gt; 0 \exists \delta &gt; 0 \, (|x-a| &lt; \delta \rightarrow |f(x)-L|&lt;\varepsilon)$<br>Negation:<br>$\lnot (\forall \varepsilon &gt; 0 \, \exists \delta &gt; 0 \, (|x-a| &lt; \delta \rightarrow |f(x)-L|&lt;\varepsilon))$<br>$\equiv \exists \varepsilon &gt; 0 \, \forall \delta &gt; 0 \lnot (|x-a| &lt; \delta \rightarrow |f(x)-L|&lt; \varepsilon)$<br>$\equiv \exists \varepsilon &gt; 0 \, \forall \delta &gt; 0 (|x-a|&lt;\delta \land \lnot(|f(x)-L|&lt; \varepsilon))$<br><strong>domain does not change when nagating</strong></blockquote><h2 id=arguments>Arguments</h2><h4 id=rules-of-inference>Rules of Inference</h4><blockquote><p>Ex. <strong>A</strong>: If you have a ticket, you can get into the movie.<br><strong>B</strong>: You have a ticket.<br><strong>C</strong>: you can get into the movie</blockquote><p>$((p \rightarrow q) \land p) \rightarrow q$<br>&lt;--&gt;<br>$p \rightarrow q$<br>$p$<br>$\therefore q$<p>$P \lor q$<br>$\lnot p$<br>$\therefore q$<p>$p \rightarrow q$<br>$\lnot p \rightarrow q$<br>$\therefore q$</p><a class=readmore href=/2017/2/3/math240-08.html title="Math240-08 Nesting">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-07 Quantification</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/1/math240-07.html title="February 1, 2017">February 1, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h4 id=terminology>Terminology</h4><p>When a quantifier is used on the variable x , we say that this occurrence of the variable is bound.
An occurrence of a variable that is not bound by a quantifier or set equal to a particular value
is said to be free.<blockquote><p>$\exists x (x+y=1)$</blockquote><p>x is <strong>bound</strong> to this quatifier<br>y is free<blockquote><p>$\forall z (z-y=4)$<br>z bound by $\forall$</blockquote><hr><blockquote><p>$\exists x(P(x) \land Q(x)) \lor \forall x R(x)$<br>the former x have bound by $\exists$<br>the latter x have bound by $\forall$</blockquote><hr><blockquote><p>Ex. $\exists x (x&gt;4 \land x&lt;-10) \lor \forall x (x^2\ge 0)$<br>$\equiv \exists x (x&gt;4 \land x&lt;-10) \lor \forall y (y^2\ge 0$</blockquote><p>The scope of a quantifier is the portion of statement to which
that quantifier applies<p><em>Two statements involving quantifiers and predicates are logically
equivalent if they have the same truth value no matter what predicates
you plug in and no matter what domain.</em><p>Statements involving predicates and quantifiers are logically equivalent if and only if they
have the same truth value no matter which predicates are substituted into these statements
and which domain of discourse is used for the variables in these propositional functions.
We use the notation S ≡ T to indicate that two statements S and T involving predicates and
quantifiers are logically equivalent.<blockquote><p>Ex. $\forall x (P(x) \land Q(x)) \equiv \forall x P(x) \land \forall x Q(x)$</blockquote><p>Two propositions $\scr P$ and $\scr R$ are equivalent<br>means that $\scr P$ is true if and only if $\scr R$ is true<p>So to show that $\scr P$ and $\scr R$ are equivalent<ul><li>$\scr R$ true implies $\scr P$ true<li>$\scr P$ true implies $\scr R$ true</ul><h2 id=negating>Negating</h2><h4 id=lnot-forall-x-p-x-equiv-exists-x-lnot-p-x>$\lnot(\forall x P(x)) \equiv \exists x \lnot P(x)$</h4><blockquote><p>Ex. $\lnot(\forall x (x^2 &gt; x))$<br>$x=\frac{1}{2}$ false</blockquote><hr><h4 id=lnot-exists-x-q-x-equiv-forall-x-lnot-q-x>$\lnot(\exists x Q(x)) \equiv \forall x \lnot Q(x)$</h4><blockquote><p>Ex. $\lnot(\exists x (x^2 = -1)$<br>$\equiv \forall x (x^2 \ne 1)$</blockquote><p>These two equivelances are called <strong>&ldquo;De Morgan&rsquo;s Law for quantifiers&rdquo;</strong><blockquote><p>Ex. $\lnot(\forall x P(x) \rightarrow Q(x))$<br>$\equiv \exists x \lnot(P(x) \rightarrow Q(x))$<br>$\equiv \exists x (P(x) \land \lnot Q(x))$</blockquote><hr><blockquote><p>Lewis Carroll<ol><li>All lions are fierce<li>Some lions don&rsquo;t drink coffee<li>Some fierce creatures don&rsquo;t drink coffee</ol><p>P(x) x is a lion<br>Q(x) x is fierce<br>R(x) x don&rsquo;t drink coffee<br>Domain is all creatures<hr><ol><li>$\forall x (P(x) \rightarrow Q(x)$<li>$\exists x (P(x) \land \lnot Q(x))$<li>$\exists x (Q(x) \land \lnot R(x))$</ol></blockquote><h2 id=nestings>Nestings</h2><blockquote><p>Ex. $\forall x \exists y (x+y=0)$</blockquote><a class=readmore href=/2017/2/1/math240-07.html title="Math240-07 Quantification">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS368-03</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/2/1/cs368-03.html title="February 1, 2017">February 1, 2017</a></span>
<span class=category><a href=/category/CS368.html rel="category tag">CS368</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS368.html rel="category tag">#CS368</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=element-wise-operation>Element-Wise Operation</h2><p>add a dot before operator to make them element-wize<blockquote><p>a = [1,2,3].*[4,5,6] --&gt; a [4, 10, 18]<p>d = cos(a) --&gt; designed to work element-wise(vectorization)</blockquote><h2 id=basic-plotting>Basic Plotting</h2><p>Matlab plots points<p><strong>plot(x, y, spec)</strong><p>optional specification:<br>- color<h2 id=line-style>- line style</h2><a class=readmore href=/2017/2/1/cs368-03.html title=CS368-03>Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-06 Quantification</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/1/30/math240-06.html title="January 30, 2017">January 30, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h4 id=quantify-the-variable>Quantify the variable</h4><ol><li>What are the possibilities for x？<li>How to specify particular possibilities</ol><h4 id=universal-quantification>Universal Quantification</h4><p>The universal quantification of P(x) is the statement
“P(x) for all values of x in the domain.”
The notation ∀xP (x) denotes the universal quantification of P(x).
Here ∀ is called theuniversal quantifier.
We read ∀xP (x) as “for all xP (x)” or “for every xP (x).”
An elementfor which P(x) is false is called a counterexample of ∀xP (x).</p><a class=readmore href=/2017/1/30/math240-06.html title="Math240-06 Quantification">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-05 Logical Equivalences</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/1/27/math240-05.html title="January 27, 2017">January 27, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p><strong>T</strong> tautology <strong>F</strong> contradiction<h2 id=logical-equivalences>Logical Equivalences</h2><h4 id=identity-laws>Identity laws</h4><p>p∧<strong>T</strong> ≡ p<br>p∨<strong>F</strong> ≡ p<h4 id=domination-laws>Domination laws</h4><p>p∨<strong>T</strong> ≡ <strong>T</strong><br>p∧<strong>F</strong> ≡ <strong>F</strong><h4 id=idempotent-laws>Idempotent laws</h4><p>p∨p ≡ p<br>p∧p ≡ p</p><a class=readmore href=/2017/1/27/math240-05.html title="Math240-05 Logical Equivalences">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-04 Propositional Equivalences</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/1/25/math240-04.html title="January 25, 2017">January 25, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=system-specification>System Specification</h2><h4 id=consistency-of-a-collection-of-specifications>Consistency of a collection of specifications.</h4><blockquote><p>X &gt;= 1 and X &lt;= 5<br>X &gt;= 1 and X &lt;= 0</blockquote><p>System specifications should be consistent, that is, they should not contain conflicting
requirements that could be used to derive a contradiction. When specifications are not consistent,
there would be no way to develop a system that satisfies all specifications.</p><a class=readmore href=/2017/1/25/math240-04.html title="Math240-04 Propositional Equivalences">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS559 Transitions</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/1/24/cs559-03.html title="January 24, 2017">January 24, 2017</a></span>
<span class=category><a href=/category/CS559.html rel="category tag">CS559</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS559.html rel="category tag">#CS559</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=translation>Translation</h2><h2 id=scale>Scale</h2><h2 id=rotation>Rotation</h2><a class=readmore href=/2017/1/24/cs559-03.html title="CS559 Transitions">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-03 Logical Operation and Bit string</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/1/23/math240-03.html title="January 23, 2017">January 23, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=biconditional-statement>Biconditional Statement</h2><p>Let p and q be propositions. The biconditional statement p ↔ q is the proposition “p if
and only if q .” The biconditional statement p ↔ q is true when p and q have the same truth
values, and is false otherwise. Biconditional statements are also called bi-implications.<table><thead><tr><th align=center>p<th align=center>q<th align=center>(p → q) ∧ (q → p)<th align=center>p → q<th align=center>q → p<tbody><tr><td align=center>T<td align=center>T<td align=center>T<td align=center>T<td align=center>T<tr><td align=center>T<td align=center>F<td align=center>F<td align=center>F<td align=center>T<tr><td align=center>F<td align=center>T<td align=center>F<td align=center>T<td align=center>F<tr><td align=center>F<td align=center>F<td align=center>T<td align=center>T<td align=center>T</table><a class=readmore href=/2017/1/23/math240-03.html title="Math240-03 Logical Operation and Bit string">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS559 HW#1 Kaleidoscope in Canvas</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/1/21/cs559-hw01-kaleidoscope-in-canvas.html title="January 21, 2017">January 21, 2017</a></span>
<span class=category><a href=/category/CS559.html rel="category tag">CS559</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;</span>
<span class=category>Tags
<a href=/tags/CS559.html rel="category tag">#CS559</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p>This is my first program assignment for CS 559 at UW-Madison.<br>The pattern below is the output generated by Canvas.<br>The Code is pretty much straight forward and is well commented.<p><canvas id=mainCanvas></canvas>
<script src=https://ler0ever.gitlab.io/CS559/Program1/mainCanvas.js></script></p><a class=readmore href=/2017/1/21/cs559-hw01-kaleidoscope-in-canvas.html title="CS559 HW#1 Kaleidoscope in Canvas">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>Math240-02 Propositional Logic and Equivalent</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2017/1/20/math240-02.html title="January 20, 2017">January 20, 2017</a></span>
<span class=category><a href=/category/Math.html rel="category tag">Math</a>;
<a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/Math240.html rel="category tag">Math240</a>;</span>
<span class=category>Tags
<a href=/tags/Math240.html rel="category tag">#Math240</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=propositions>Propositions</h2><p>A proposition is a declarative sentence (that is, a sentence that declares a fact) that is either true or false, but not both. (T || F)<h6 id=ex>Ex:</h6><ol><li>Madison is the capital of Wisconsin --&gt; T<li>1 + 2 = 3 --&gt; T<li>1 + 4 = 9 --&gt; F</ol><h6 id=non-ex>Non Ex:</h6><ol><li>What time is it?<li>Read a book<li>X + 1 = 5<li>X + Y = Z</ol><a class=readmore href=/2017/1/20/math240-02.html title="Math240-02 Propositional Logic and Equivalent">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 23</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/12/15/cs367-introtods23.html title="December 15, 2016">December 15, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=sorting-in-java>Sorting in Java</h2><h4 id=in-hava-util>in hava.util</h4><p>Collection.sort(List)<h2 id=radix-sort>Radix Sort</h2><h4 id=assumption>Assumption</h4><p>number of items: size of collection (binary: 0-1, hex: 0-9, a-f)<p>range of unique digits(RANGE): Radix 10<p>length of item&rsquo;s sequence of digits(LEN):<h4 id=idea>Idea</h4><p>For Each Position from right to left (least significant digit to most significant digit)<ul><li>Items in A<br><li>A.length = N (number of items)<br><li>Items are a sequence of digits<br><li>LEN is # of digits in each item<li>each position is one value of a range of values</ul><h4 id=complexity>Complexity</h4><p>O(N+R) * LEN vs O(NlogN)</p><a class=readmore href=/2016/12/15/cs367-introtods23.html title="CS367-Introduction To Data Structures 23">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 23</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/12/13/cs367-introtods23.html title="December 13, 2016">December 13, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=heap-sort>Heap Sort</h2><h4 id=idea>Idea</h4><ol><li>Insert item from unsorted array into a min heap<li>Remove the min from min heap and add it back into original array (accending order)</ol><h4 id=analysis>Analysis</h4><p>[time]<ol><li>add: N items + logN = O(NlogN)<li>remove: N items + logN = O(NlogN)</ol><p>O(NlogN)<p>[space]<p>naive - 2N memory<h4 id=is-there-an-in-place-algorithm>Is there an in-place algorithm</h4><p>Yes<ol><li>Reheapify unsorted array into a maxheap<li>Remove Max and place into &ldquo;next&rdquo; in sorted part.</ol><h2 id=merge-sort>Merge Sort</h2><h4 id=idea-1>Idea</h4><ol><li>Devide unsorted array in half<li>Sort left half<li>Sort right half<li>Merge sorted halves into sorted whole</ol><h4 id=analysis-1>Analysis</h4><p>[Time]<p>O(NlogN)<p>[Space]<p>Naive way: 2N memory<p>can be a little better but not in-place<h2 id=quick-sort>Quick Sort</h2><h4 id=idea-2>idea</h4><p>Devide unsorted into less than pivot partition and a greater than pivot partition<br>And place pivot value in correct place<ol><li>Pick a pivot value<li>Partition the array based on pivot value<li>Recursively sort left partition<li>Recursively sort right partition</ol><h4 id=analysis-2>Analysis</h4><p>[Time]<p>Best case: if pivot is always middle value and each partition is roughly half<br>O(Log2N)<p>Worst Case: pivot is min or max value
O(N^2)<h4 id=choosing-a-good-pivot>Choosing a Good Pivot</h4><p>Bad Idea: pick the first/last value (sorted is worst case)<p>Good Idea: medium of three (1st, last, (1st+last)/2)<h4 id=quick-sort-with-partitioning>Quick sort with partitioning</h4><ol><li><ul><li>Pick a pivot using medium of three<li>place smallest in first<li>place biggest in last<li>swap(p, last-1)</ul><li>Partition<ul><li>set left to index of next value &gt; p<li>set R to index of next value &lt; p<li>swap left and right values<li>Repeat 2. until R &lt; L</ul><li>Place the pivot value at R+1</ol><h2 id=stable-sorts>Stable Sorts</h2><p>Maintain the relative ordering between different sort<p>Stable sort used on composite sort
first + last<h4 id=stable-sort-we-see>Stable Sort we see</h4><table><thead><tr><th align=center>Sort<th align=center>Stable?<tbody><tr><td align=center>Bubble Sort<td align=center>Yes<tr><td align=center>Insertion Sort<td align=center>Yes<tr><td align=center>Selection Sort<td align=center>No<tr><td align=center>Heap Sort<td align=center>No<tr><td align=center>Merge Sort<td align=center>Yes<tr><td align=center>Quick Sort<td align=center>No</table><a class=readmore href=/2016/12/13/cs367-introtods23.html title="CS367-Introduction To Data Structures 23">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 22</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/12/8/cs367-introtods22.html title="December 8, 2016">December 8, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=quadratic-probing>Quadratic Probing</h2><p>P.S.: Hk, Kk+1^2m Hk + 2^2, &hellip;<h2 id=double-hashing>Double Hashing</h2><p>O.S: Hk. Hk + stepsize, Hk + 2*stepsize, Hk + 3*stepsize &hellip;<br>stepsize = hash2(key)<p>t.s / gcf(ss, ts) = element used<h2 id=collicion-handling-using-buckets>Collicion Handling using Buckets</h2><h4 id=buckets>Buckets</h4><ul><li>Each element of hash table can store multiple items<li>if collision, add item to bucket at that index<li>Typically, the bucket is not sorted (few item in each bucket)</ul><h4 id=array-bucket>Array bucket</h4><p>fixed sized array:
- easy to implement
- fixed sized bucket can overfill
- waste a lot of space<p>variable sized array (arraylist)
- waste a lot of space<h4 id=chained-bucket>Chained Bucket</h4><ul><li>easy to implement<li>does not overfill<li>does not waste space<li>can get long chains</ul><p>bucket grows and shrinks as needed<p>Worst case lookup: all items in same chain O(N)<p>O(1) insert if dups allowed<br>O(N) insert if no dups<p><strong>Prefered for many yses</strong> (in java hashmap)<h4 id=tree-buckets-balanced-search-tree>Tree buckets( balanced search tree)</h4><p>use if you expect a lot of collisions<p>O(log2N): all items in one bucket<p>Java&rsquo;s HashTree<h2 id=java-support-for-hashing>Java support for Hashing</h2><p>hashCode method
• method of Object class
• returns an int
• default hash code is BAD - computed from object’s memory address<p>Guidelines for overriding hashcode:<ul><li>Must be deterministic<li>If class override .equals, if should also override hashcode</ul><h4 id=hashtable-k-v-and-hashmap-k-v-class>Hashtable<k,v> and HashMap<k,v> class</h4><blockquote><p>! CS-367 (F16): L27-6 !</blockquote><p>operations:<ul><li>V get(K key)<li>V put(K key, V value)<li>boolean remove (K key)<li>V remove(K key, V value)</ul><h2 id=treemap-vs-hashmap>TreeMap vs HashMap</h2><table><thead><tr><th align=center><th align=center>TreeMap<th align=center>HashMap<tbody><tr><td align=center>D.S.<td align=center>R.B.T<td align=center>Has table with chained busket<tr><td align=center>Complexity<td align=center>O(logN)<td align=center>O(1) best &amp; avg O(N); all item in same bucket<tr><td align=center>Iterate through keys<td align=center>Ascending Order<td align=center>No particular order<tr><td align=center>Comp of iteration<td align=center>O(N)<td align=center>O(T.S. + N)</table><h2 id=sorting>Sorting</h2><h4 id=problem>Problem</h4><p>Place items in the collection in a particular order<h4 id=solution>Solution</h4><p>Comparison sorts - compare items and move data<p>Items must be comparable<ul><li>Ref type: a.compareTo()<li>Prim: &lt; &lt;= &gt;= &gt;</ul><h4 id=complexity>Complexity</h4><p>Best Comparison sorts are O(NlogN)<h4 id=in-place-sorts>In-Place Sorts</h4><p>Use only one array to store and sort collection<h4 id=basic-in-place-comparison-sorts>Basic In-place Comparison Sorts</h4><p>Idea: devide array into &ldquo;sorted&rdquo; and &ldquo;unsorted&rdquo; part<ul><li>N-1 passes, each pass moves one item from unsorted to sorted</ul><h2 id=bubble-sort>Bubble Sort</h2><h4 id=idea>Idea</h4><p>Bubble the item to its correct position<p>Start all in unsorted part<p>Each pass moves 1 item to the sorted part<pre><code>boolean swapsdone = true;
int passes = A.length-1; 
for (int i = 0; i &lt; passes &amp;&amp; swapsdone; i++) { 
   for (int j = A.length–1; j &gt; i; j--) { 
      if (A[j] &lt; A[j-1]) { 
         swap(A[j], A[j-1]); 
         swapsdone = true;
      } 
   } 
} 
</code></pre><h4 id=analysis>Analysis</h4><h2 id=insertion-sort>Insertion Sort</h2><h4 id=idea-1>Idea</h4><p>Start: first item is sorted<p>Each pass: get next item from unsorted and place in correct position in sorted<h4 id=analysis-1>Analysis</h4><h2 id=selection-sort>Selection Sort</h2><h4 id=idea-2>Idea</h4><p>Start: all items are unsorted<p>Each pass: select the next item for the sorted part and swap into that position</p><a class=readmore href=/2016/12/8/cs367-introtods22.html title="CS367-Introduction To Data Structures 22">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 21</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/12/6/cs367-introtods21.html title="December 6, 2016">December 6, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=ideal-hashing>Ideal Hashing</h2><h4 id=trvial-hash-function>Trvial Hash Function</h4><p>If key is an int, use it as hash index<h4 id=perfect-hash-function>Perfect Hash Function</h4><p>When each key maps to a different hash index<pre><code class=language-java>void insert (K keym D data) { table[hash(key)] = ddata;}
D lookup(K key) {return table[hash(key)];}
void delete(K key) {table[hash(key)] = null;}
</code></pre><h4 id=colusion>Colusion</h4><p>When a different key hashes to the same hash index<h4 id=key-issues>Key issues:</h4><ul><li>must design a good hash Function<li>must choose approporiate table size, TS<li>Must be able to handle collision</ul><h2 id=design-a-hash-function>Design a hash Function</h2><h4 id=good-hash-function>Good Hash Function</h4><ol><li>Must be deterministic<li>Should achive uniformity across the hash table<li>Should minimize collision<li>Should be fast and easy to compute (for computer)</ol><h4 id=java-hash-function-steps>Java Hash Function Steps</h4><ol><li>Generate a hash code<li>compress the hash code into a hash index (java: abs(obj.hashcode())%TS)</ol><h2 id=techniques-in-generating-hash-codes>Techniques in generating hash codes</h2><h4 id=integer-key-90123456789>Integer Key 90123456789</h4><h4 id=extraction>Extraction</h4><ol><li>Devide key into parts<li>use only the part that distinguish items</ol><h4 id=weighting>Weighting</h4><p>Emphasize some parts more than others<h4 id=folding>Folding</h4><p>Combine parts back into an int<h2 id=handling-string-keys>Handling String Keys</h2><h4 id=what-does-java-do>What does Java do?</h4><p>Ci = c is the character at index i of a Strin<br>String s = cat;
hc = &lsquo;c&rsquo;*31^2 + &lsquo;a&rsquo;*31^1 + &rsquo;t&rsquo; * 31^0<h4 id=generalize>Generalize</h4><p>C0 * 31^N-1 + C1 * 31^N-2 &hellip; + CN-2 * 31^1 + CN-1*31^0<br>31 * i = 32 * i - i = i &lt;&lt; 5 - i<h4 id=copmlexity>Copmlexity</h4><p>O(1) - wrt to num items in table<br>O(N) - wrt to length of string<h2 id=handling-double-keys>Handling Double Keys</h2><h4 id=double-ieee-64-bit>Double IEEE 64 bit</h4><p>1st bit: positive | negative<br>2 - ..: exponent<br>.. - 64: mantissa<h4 id=what-java-do>What java do</h4><ul><li>split 64 bit in half(32 and 32)<li>Extract<li>right shift 32 bit<li>exclusive OR<li>right part</ul><h4 id=e-g>e.g.</h4><p>0110 1011<ol><li>0000 0110 (right)<li>0110 1101 exclusive OR<li>right part = 13</ol><h4 id=in-java>In java!</h4><pre><code class=language-java>double key = 121.11;
long bits  = Double.doubleToLongBits(key);
int hascode = (int)(bits ^ (bits &gt;&gt; 32));
</code></pre><h2 id=hoosing-the-table-size>hoosing the table size</h2><p>| Item | TS | # of collisions | Load Factor |
| :---: | :---: | :---: | :---: |
| 100 | 10000 | 0 or 1 | <sup>1</sup>&frasl;<sub>100</sub> |
| 100 | 1000 | 10- | <sup>1</sup>&frasl;<sub>10</sub> |
| 100 | 100 | 30+ | 1 |<p>L.F &lt;= 0.75<h4 id=table-size-and-distribution>Table size and distribution</h4><p>To keey efficiency, choose t.S with extra space<table><thead><tr><th align=center>N<th align=center>TS<th align=center># collision<tbody><tr><td align=center>50<td align=center>60<td align=center>47<tr><td align=center>50<td align=center>50<td align=center>45<tr><td align=center>50<td align=center>37<td align=center>13</table><h2 id=resizing-the-hash-table>Resizing the Hash Table</h2><p>Resize when L.F. &gt;= 0.75<h4 id=naive-expand>Naive Expand</h4><ol><li>Make larger array<li>copy elements</ol><p>Items not likely in correct hash index for new TS<h4 id=rehashing>Rehashing</h4><ol><li>Double the T.S to nearest prime<ul><li>Backup: double T.S + 1</ul><li>Rehash each itme into new hash table</ol><h4 id=complexity>Complexity</h4><p>O(N): wrt to # item in table<br>Pichk good size to start with ( a prime with extra space )<br>L.F around 0.5 after initial data<h2 id=collision-handling-using-open-addressing>Collision Handling using Open Addressing</h2><h4 id=open-addressing>Open Addressing</h4><p>Look for open spot<ul><li>each element in table store at most one item<li>If collision occurs, look for open spot</ul><p>Probe sequence: how will we find open spot<h4 id=linear-probing>Linear Probing</h4><p>Step one by one until we find open element<h4 id=problem>Problem</h4><p>If we remove an item, we must mark as removed<br>So that subsequent looup don&rsquo;t fail<h4 id=downside>Downside</h4><p>Tends to cause clustering</p><a class=readmore href=/2016/12/6/cs367-introtods21.html title="CS367-Introduction To Data Structures 21">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 20</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/12/1/cs367-introtods20.html title="December 1, 2016">December 1, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=application-of-dfs-bfs>Application of DFS/BFS</h2><ul><li>Is it connected ?<li><p>Which vertex are reachable<h4 id=path-detection>Path Detection</h4><li><p>Is there a path from start vertex to another<li><p>What is the path?<li><p>What is the shortest path?<ul><li>Shortest length (unweighted)<li>Least cost (weighted) : least sum of all edges on path</ul></ul><h4 id=cycle-detection>Cycle Detection</h4><ul><li>Is there a cycle from start vertex back to start vertex (exclude simple cycles)<li>Is there a cycle with 3 or more vertices</ul><p>unvisited, visited, in-progress<h2 id=more-graph-terminology>More Graph Terminology</h2><h4 id=weighted-graph>Weighted Graph</h4><p>edges have a cost<br>ex: network<h4 id=complete-graph>Complete Graph</h4><p>There is an edge between each pair of vertices<h4 id=connected-graph>Connected Graph</h4><ul><li>undirected</ul><p>There is a path between each pair of vertices<ul><li>directed<ul><li>Strongly connected. There is a path between each pair of vertices if no respect to directed (not backwards)<li>Weakly connected. There is a path between each pair of vertices if we ignore the direction</ul></ul><h2 id=dijkstra>Dijkstra</h2><ul><li>Fastest single-start, shortest path algorithm for arbitrary graphs with unbounded non-negative edge weights<li>Easiest O(e, vlogv) - use Fibonacci heap<li>Must specify a single start node<li>Directed Graph<li>Unbounded non-negative edge weight<li>Our version -- finds shortest path from to all other vertices<li>uses a predecessor list to reconstruct the shortest path</ul><h4 id=psuedo-code>Psuedo Code</h4><pre><code>
for each vertex V
    initialize V’s visited mark to false
    initialize V's total weight to “infinity”
    initialize V's predecessor to null

set start vertex’s total weight to 0

create new priority queue pq
pq.insert( [start vertex total weight,start vertex] )

while !pq.isEmpty()
    [C’s total weight,C] = pq.removeMin()
    set C’s visited mark to true

    for each unvisited successor S adjacent to C
        if S's total weight can be reduced
        S's total weight = C's total weight + edge weight from C to S
        update S's predecessor to C
        pq.insert( [S’s total weight,S] )
        (if S already in pq we’ll just update S's total weight)
</code></pre><h4 id=key-point-reconstruct-the-path>Key point: reconstruct the path</h4><h2 id=hashing>Hashing</h2><h4 id=goal>Goal</h4><p>Do better than all log(n) lookup, insert, remove<br>We want constant time<h4 id=concept>Concept</h4><p>Simple - if we use an array and we know index<br>Idea - write a function that tells me where to look<br>key --&gt; [Function f(key)] --&gt; location &ldquo;index in the array&rdquo;<h4 id=hash-table>Hash Table</h4><p>Array that shows the collection of item<h4 id=table-size-ts>Table Size (TS)</h4><p>Length of the hash table<h4 id=load-factor-lf>Load Factor (LF)</h4><p>number of items / TS<h4 id=key>Key</h4><p>Unique id for key, value pair<h4 id=hash-function>Hash Function</h4><p>Converts key to the hash index in the hash table<h4 id=hach-index>Hach Index</h4><p>Specifies the item location<h2 id=ideal-hashing>Ideal Hashing</h2><a class=readmore href=/2016/12/1/cs367-introtods20.html title="CS367-Introduction To Data Structures 20">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 19</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/11/29/cs367-introtods19.html title="November 29, 2016">November 29, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=using-edge-representations>Using Edge Representations</h2><h4 id=1-adjacency-list>1. Adjacency List:</h4><pre><code class=language-java>public int degree( Graphnode&lt;T&gt; n) {
    return n.getEdges().size();
}
</code></pre><h4 id=2-adjacency-matrix>2. Adjacency Matrix :</h4><pre><code class=language-java>public int degree(Graphnode&lt;T&gt; n) {
    int i = n.getMapKeyIndex();
    int degree = 0;
    for (int j=0; j&lt;Am[i].length; j++&gt;)
        if (Am[i][j]) degree++;
    reeturn degree;
}
</code></pre><h2 id=comparison-of-edge-representations>Comparison of Edge Representations</h2><h4 id=ease-implmentation>Ease implmentation</h4><p>Both are easy<h4 id=space-memory>Space (memory)</h4><p>AM: O(N^2)<br>AL:<br>- O(N): sparse graph
- O(N^2): comoplete graph<h4 id=time>Time</h4><p>Depends on implementation
<strong>Node&rsquo;s Degree?</strong><br>AM: O(N) - must count edges for the vertex<br>AL: O(1) - access size() if the list<p><strong>Edge exist between two given nodes</strong><br>AM: O(1)<br>AL: O(N) - must look at all vertices in AL<h2 id=searches-and-traversals>Searches and Traversals</h2><h4 id=search>Search</h4><ul><li>Look through collection until we find a match for desired key<li>Must pick a start vertex (if not told, follow 367 Convention)</ul><blockquote><p>367 Convention: Pick next unvisited vertex in increasing numerical or alphabetical order</blockquote><h4 id=traversal>Traversal</h4><ul><li>Visit each vertex exactly once.<li>must specify starting vertex, and then visit vertices that are reachable.</ul><h4 id=path>Path</h4><p>length of a path: # of edges (# of nodes in TREES)<br><strong>Are Cycles allowed or not?</strong><h2 id=depth-first-search-dfs>Depth-First Search(DFS)</h2><ul><li>Assume all vertices are unvisited at start<li>Relies on a stack we&rsquo;ll use call stack and recursion</ul><h4 id=algorithm>Algorithm</h4><p>DFS(V):<ol><li>mark v as visited<li>for each unvisited successor(s) (adjecent to v)<ul><li>DFS(s)</ul></ol><p><strong>Equivalent to a pre-order traversal on a tree</strong><br>Tree: D.A.G. Directed Acyclic Graph<h2 id=breadth-first-search-bfs>Breadth-First Search(BFS)</h2><ul><li>Assume all vertices are unvisited at start<li>Relies on a queue FIFO</ul><h4 id=algorithm-1>Algorithm</h4><ul><li>BFS(V)<ul><li>q = new Queue()<li>mark v as visited<li>q.enqueue(v); // priming the pump<li>while q is not empty<ul><li>c = q.dequeue(); //curent vertex<li>for each unvisited successor s (adjacent to c):<ul><li>mark s as visited<li>q.enqueue(s)</ul></ul></ul></ul><p><strong>Equivalent to level-order traversal on a tree (D.A.G.)</strong><h2 id=topological-ordering>Topological Ordering</h2><h4 id=idea>IDEA</h4><p>Come up with a list such that each vertexin list comes before its successors.<h4 id=interative-algorithm-see-readings-for-recursive-algorithm>Interative Algorithm (see readings for recursive algorithm)</h4><ul><li>num = number of vertices<li>st = new Stack();<li>for each vertices v with no predecessors:<ul><li>mark v as visited<li>st.push(v);</ul><li>while st is not empty:<ul><li>c = st.peak();<li>if all successors of c are visited:<ul><li>st.pop();<li>assign node c the value in node<li>num--</ul><li>else:<ul><li>select an unvisited successor of c (u, with 367 conv)<li>mark u as visited</ul></ul></ul><a class=readmore href=/2016/11/29/cs367-introtods19.html title="CS367-Introduction To Data Structures 19">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 18</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/11/22/cs367-introtods18.html title="November 22, 2016">November 22, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=adts>ADTs</h2><h4 id=linear>Linear</h4><ul><li>predecessors: at most 1 (prev)<li>successor: at most 1 (next)</ul><h4 id=hierarchical>Hierarchical</h4><ul><li>predecessirs: at most 1 (parent)<li>successors: 0 or more - general tree, at most 2 - binary tree</ul><h4 id=graphical>Graphical</h4><ul><li>predecesors: any number<li>successors: any number</ul><p><strong>Vertex | Edges</strong><ul><li>graph represents the relationships between items.<li>edge is relationship between two nodes<li>no clear first/last top/bottom<li>need to efficiently access any item</ul><h2 id=graph-terminology>Graph Terminology</h2><ul><li>Undirected Graph<li>No Dups - edges or vertices<li>Can have a self-edge<li>Degree: # of edges for a vertex<li><p>Path: sequence of connected vertices<li><p>Cycles: pose a key problem in current graph algorithm</ul><h4 id=directed-graph>Directed Graph</h4><ul><li>Source --&gt; Destination<li>&ldquo;In&rdquo; degree: Num of edges into a vertex<li>&ldquo;Out&rdquo; degree: Num of edges out from a vertex<li>Degree: Sum of in degree and out degree<li>Path: Sequence of vertices in the directions of their edges</ul><h4 id=graph>Graph</h4><p><strong>Order</strong>: num of vertices
<strong>Size</strong>: num of edges<h2 id=implementing-graphs>Implementing Graphs</h2><h4 id=graph-adr-ops>Graph ADR Ops</h4><ul><li>Constructor: Empty graph<li>insert(vertex)<li>insert(vertex, vertex, relationship) - edge between two vertices<li>delete(vertex) - must also delete edges into and out of vertex<li>delete(v1, v2) - may or may not also mean delete(v2,v1)<li>lookup(vertex)<li>lookup(v1, v2)<li>isEmpty, size(), orders(), getVertices()</ul><p>BFS &amp; DFS<h4 id=graph-class>Graph Class</h4><pre><code class=language-java>public class Graph&lt;T&gt; {
    private List&lt;GraphNode&lt;T&gt;&gt; vertices;
    // Adjacency Matrix;
}
</code></pre><h4 id=graphnode-class>Graphnode Class</h4><pre><code class=language-java>class Graphnode&lt;T&gt; {
    private T data;
    private int key;
    private boolean visited = false;
    // Adjacency Lists
}
</code></pre><h2 id=representing-edges>Representing Edges</h2><h4 id=adjacency-matrix>Adjacency Matrix</h4><ul><li>Source: row (from) --&gt;<li>Target: Column |<li>i --&gt; j = (i,j) = ajn[i,j]</ul><h2 id=representing-edges-1>Representing Edges</h2><h4 id=adjacency-lists>Adjacency Lists</h4><ul><li>Store the list of successors with each vertex</ul><a class=readmore href=/2016/11/22/cs367-introtods18.html title="CS367-Introduction To Data Structures 18">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 18</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/11/17/cs367-introtods17.html title="November 17, 2016">November 17, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=non-empty-case-2-k-s-parent-p-is-red>Non-Empty Case 2: K&rsquo;s parent P is red</h2><h4 id=fixing-rbt>Fixing RBT</h4><ul><li>Tri-Node Restructuring is done if P&rsquo;s sibling S is null<li>Recoloring is done if P&rsquo;s sibling S is red</ul><p><strong>Tri-Node Restrcturing</strong><br>1. Middle Value becomes &ldquo;root&rdquo;
2. Smallest value becomes left
3. Largest value becomes right<p><strong>Recoloring</strong><br>1. P, S become black
2. G becomes black if not root
3. K stays red<h2 id=cascading-fixes>Cascading Fixes</h2><p>Recoloring is done if P&rsquo;s sibling S is red
&hellip;
Must cascade the fix back up the tree<p>Tri-Node Restrcturing is done if P&rsquo;s sibling S null or black<h2 id=rbt-complexity>RBT Complexity</h2><h4 id=print>Print</h4><p>Linear - same as BST<h4 id=lookup>Lookup</h4><p>O(log2N) - same as BST (in best case)<br>But, wost case of RBT is still O(Log2N)<br>B.C. RBT stays balanced<h4 id=insert>Insert</h4><ul><li>O(LogN) find spot to add new node<li>O(1) link node into RBT<li>O(1000) (Linear) Recolor or TNR<li>O(logN) cascade<br></ul><p><strong>O(Log2N)</strong></p><a class=readmore href=/2016/11/17/cs367-introtods17.html title="CS367-Introduction To Data Structures 18">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 16</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/11/10/cs367-introtods16.html title="November 10, 2016">November 10, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=classifying-binary-tree>Classifying Binary Tree</h2><h4 id=full>Full</h4><ul><li>No missing nodes<li>All leaves at same level<li><h2 id=all-interior-nodes-have-two-children>All interior nodes have two children</h2><ul><li>N = 2^H - 1<li><p>H = log2(N+1)<h4 id=complete>Complete</h4><p>Array-based impl of priority queue</ul><li><p>Full to depth of H-1<li><p>Depth H is filled left to right</ul><h4 id=height-balanced>Height-balanced</h4><p>mostly balanced<ul><li>For each node, the difference between the height of left subtree and right subtree is at most 1</ul><p>Balance Factor(B.F.) : the difference between left and right subtree height.<h4 id=balanced>balanced</h4><p>balanced, not necessarily height balanced<blockquote><p>Red-Black Tree</blockquote><ul><li>Height when N is O(logN)</ul><h2 id=practice>Practice</h2><table><thead><tr><th align=center><th align=center>Full<th align=center>Complete<th align=center>Height Balanced<th align=center>Balanced<tbody><tr><td align=center>A<td align=center>N<td align=center>N<td align=center>N<td align=center>Y<tr><td align=center>B<td align=center>N<td align=center>Y<td align=center>Y<td align=center>Y<tr><td align=center>C<td align=center>Y<td align=center>Y<td align=center>Y<td align=center>Y<tr><td align=center>D<td align=center>N<td align=center>N<td align=center>N<td align=center>N<tr><td align=center>E<td align=center>N<td align=center>N<td align=center>Y<td align=center>Y<tr><td align=center>F<td align=center>N<td align=center>N<td align=center>Y<td align=center>N</table><h2 id=balanced-search-tree>Balanced Search Tree</h2><h4 id=goal>Goal</h4><p>Keep height H ~ log2N where N is number of nodes(keys)<h4 id=idea>Idea</h4><p>Make insert and delete restructure tree when it gets out of balance<h4 id=avl>AVL</h4><p>height-balalnced tree<ol><li>Detect when a node has a B.F &gt;= 2<li>Fix it.<ol><li>Case 1: Right rotate *<li>Case 2: Left rotate *<li>Case 3: require left-right rotate<li>case 4: require right-left rotate</ol></ol><h4 id=btree>BTree</h4><p>(cs564) - relax the &ldquo;binaru&rdquo; structure<ul><li>Detect during insert - when you reach a 4-node<li>if you reach a 4-node, split into 2-nodes</ul><p>2 node can grow to a 3-node<br>3 node can grow to a 4-node
4 node must split<h2 id=red-black-tree>Red-Black Tree</h2><h4 id=rbt>RBT:</h4><p>BST that modified to stay balance<h4 id=red-black-tree-properties>Red-Black tree properties</h4><ul><li>Root property: Root Node is black<li>Red Property: Red nodes have black children<li>Black Property: Every path from root to null children must pass through the same number of black node.</ul><h4 id=red-black-tree-operations>Red-black tree operations</h4><ul><li>print : same as BST<li>lookup: same as BST<li>insert: similar to BST, but with detect &amp; rebalance<li>delete: similar to BST, but with detect &amp; rebalance</ul><h2 id=inserting-into-a-red-black-tree>Inserting into a Red-Black Tree</h2><h4 id=goal-1>Goal:</h4><p>Insert key value into red-black tree and maintain RBT properties - O(logN)<h4 id=if-t-is-empty>If T is empty</h4><p>root is null, k is added as a leaf at root and colored black<h4 id=if-t-is-not-empty>If T is not empty</h4><ul><li>Step down tree as done for BST<li>Add a node containing K as done for BST, and: colored Red<li>restoreRBT properties as needed</ul><h4 id=which-of-the-properties-might-be-violated-as-a-result-of-inserting-a-red-leaf-node>Which of the properties might be violated as a result of inserting a red leaf node</h4><ul><li>root properti: new red leaf will not affect root property<li>black property: new red leaf will not affect black property<li>red property: adding a new red leaf MAY affect red property (<em>occurs if parent of new node is red</em>)</ul><p><strong>Must detect red property violations</strong><h4 id=non-empty-case-1-k-s-parent-p-is-black>Non-Empty Case 1: K&rsquo;s parent P is black</h4><p>No RPV, no fix needed<h2 id=non-empty-case-2-k-s-parent-p-is-red>Non-Empty Case 2: K&rsquo;s parent P is red</h2><h4 id=fixing-rbt>Fixing RBT</h4><ul><li>Tri-Node Restructuring is done if P&rsquo;s sibling S is null<li>Recoloring is done if P&rsquo;s sibling S is red</ul><a class=readmore href=/2016/11/10/cs367-introtods16.html title="CS367-Introduction To Data Structures 16">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 15</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/11/8/cs367-introtods15.html title="November 8, 2016">November 8, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h4 id=bstnode-class>BSTnode Class</h4><pre><code class=language-java>class BSTnode&lt;K&gt; {
    private K key;index
    private BSTnode&lt;K&gt; left, right;
    public BSTnode(K key, BSTnode&lt;K&gt; left, BSTnode&lt;K&gt; right) {
        this.key = key;
        this.left = left;
        this.right = right;
    }
    public K getKey() { return key; }
    public BSTnode&lt;K&gt; getLeft() { return left; }
    public BSTnode&lt;K&gt; getRight() { return right; }
    public void setKey(K newK) { key = newK; }
    public void setLeft(BSTnode&lt;K&gt; newL) { left = newL; }
    public void setRight(BSTnode&lt;K&gt; newR) { right = newR; }
}

</code></pre><h2 id=bst-class>BST Class</h2><pre><code class=language-java>import java.io.*; //for PrintStream
public class BST&lt;K extends Comparable&lt;K&gt;&gt; {
    private BSTnode&lt;K&gt; root;
    public BST() { root = null; }
    public void insert(K key) throws DuplicateException {
        root = insert(root, key);
    }
    public void delete(K key) {
        root = delete(root, key);
    }
    public boolean lookup(K key) {
        return lookup(root, key); //AKA Search in P4
    }
    public void print(PrintStream p) {
        print(root, p);
    }
    //add helpers ...

}
</code></pre><h2 id=implementing-print>Implementing Print</h2><h4 id=complete-the-print-method-based-on-the-recursive-definition> Complete the print method based on the recursive definition</h4><pre><code class=language-java>public void print( PrintStream p ) { print( root, p ); }
private void print( BSTnode&lt;K&gt; n, PrintStream p ) {
    if (n == null) return;
    else {
        print(n.getLeft(), p); //L
        p.println(n.getKey()); //V
        print(n.getRight(), p); //R
    }
}
</code></pre><h2 id=implementing-lookup-and-insert>Implementing <code>Lookup</code> and <code>Insert</code></h2><h4 id=pseudo-code-algorithm-for-lookup>Pseudo-Code Algorithm for lookup</h4><pre><code>private boolean lookup(BSTnode&lt;K&gt; n, K key) {
    if n is null, return false;
    if n's key equals key, return true;
    if n's key is larger than key, return lookup n's left subtree
    else return lookup n's right subtree
}
</code></pre><h4 id=high-level-algorithm-for-insert>High-Level Algorithm for insert</h4><pre><code>private BSTnode&lt;K&gt; insert(BSTnode&lt;K&gt; n, K key) throws DuplicateException {
    Search down tree as in lookup
    if we find a node with same key
        throw DuplicateException
    There will be a point where search ends
    When we reach that point
        we will insert a new leaf node
}
</code></pre><h2 id=inserting-into-a-bst>Inserting into a BST</h2><h4 id=what-is-the-shape-of-a-bst-when-values-are-inserted-in-sorted-order>What is the shape of a BST when values are inserted in sorted order?</h4><p>A Line<h4 id=will-you-get-a-bad-shape-only-if-values-are-inserted-in-sorted-order>Will you get a bad shape only if values are inserted in sorted order?</h4><p>No： 11， 53， 22， 44， 33<p><strong>The order if a BSTree deoebds upon the order of inserts and delete</strong><ul><li>Unbalanced does not give O(logN) complexity<li>Balanced does give O(H) complexity, where h is height of the tree</ul><h2 id=implementing-delete>Implementing <code>delete</code></h2><h4 id=high-level-algorithm>High-Level Algorithm</h4><pre><code>private BSTnode&lt;K&gt; delete( BSTnode&lt;K&gt; n, K key ) {
    Search down the tree as in loopup
    if n is null, return null
    if n's key equals key //FOUND --&gt; must remove
    ..Action depends upon n's left and right children

    //Case 1: n has no children, AKA leaf node.
    nulink n from its parents, by setting appropriate child of p to null

    //Case 2: n has one child
    Delete n by setting appropriate child of parent to n's child
    
    //Case 3: n has two children
    Sol: find a replacement value for n in either left or right subtree
    //(reading -- chooses smallest of right subtree)
    Copy k, v from replacement node
    delete key from that subtree
}
</code></pre><h4 id=how-do-you-delete-50-or-30-from-the-above-tree>How do you delete 50 or 30 from the above tree</h4><ol><li>largest value on left subtree or smallest value on right subtree<li>delete the &ldquo;new&rdquo; key from the correct subtree</ol><p><strong>Key Concept: find a replacement for n&rsquo;s key instead of lots of relinking</strong><p>Delete 30 from the tree above using the inorder predecessor 20<br>Delete 50 from the tree above using the inorder sucessor 60<h2 id=complexity-of-bst-method>Complexity of BST Method</h2><ul><li>N: number of nodes<li>print: O(N)<li>lookup, insert, delete: O(H)</ul><p>H is the height of the tree<br>roughtly O(logN) if tree is Balanced<br>O(N) if tree is not balanced :</p><a class=readmore href=/2016/11/8/cs367-introtods15.html title="CS367-Introduction To Data Structures 15">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 14</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/11/3/cs367-introtods14.html title="November 3, 2016">November 3, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=tower-of-hanoi>Tower of hanoi</h2><h4 id=algorithm>Algorithm</h4><pre><code>solveTowers(count, src, dest, spare){
    if (count == 1) move src to dest;
    else {
        solveTowers(count - 1, src, spare, dest);
        solveTowers(1, src, dest, spare);
        solveTowers(count - 1, spare, dest, src);
    }
}
</code></pre><h4 id=complexity>Complexity</h4><p><strong>Problem Size N is</strong><br>Num of Disks<br><strong>1. Equation</strong><br>T(1) = 1<br>T(N) = T(N-1) + T(1) + T(N-1) = 2T(N-1) + 1<br><strong>2.Table</strong><br>&hellip;<br>2^k - 1
<strong>3. Verify</strong><br>T(N) = 2T(N-1) + 1<br>2^k-1 = 2 * (2^(N-1) -1) + 1<h2 id=picking-lottery-numbers>Picking Lottery Numbers</h2><ul><li>order does not matter<li>duplicates not allowed<li><p>combination matters, not permutation<h4 id=n-choose-k>N Choose K</h4><p>N: Number to choose from
K: Number of items in a combination<h4 id=recursive-definition>Recursive Definition</h4><p>c(n,k) = c(n-1, k-1) + c(n-1, k)<br>with fav # in case<h4 id=base-cases>Base cases</h4><p>c(n, k) = 0 when k &gt; n // no comb solves
c(n, k) = 1 when k = n // one comb - all numbers
c(n, k) = 1 when k = 0 // empty set - no pick</ul><h4 id=recursive-case>Recursive case</h4><p>for c(n-1, k-1), we eventually reach k = 0<br>for (n-1, k) we eventually reach<h4 id=implement-the-c-n-k>Implement the c(n,k)</h4><pre><code>int c(int n, int k) {
    if (k&gt;n) return 0;
    if (k==0 || k==n) return 1;
    return c(n-1, k-1) + c(n-1, k);
}
</code></pre><p>T(N) = n!/k!(n-k)!<h2 id=searching>Searching</h2><p><strong>N - Num of items in a list</strong><h4 id=linear-search>Linear Search</h4><ul><li>search one by one<li>the list can be unsorted</ul><pre><code>boolean LinSearch(ListADT L, int pos, E x) {
    if (pos &gt;= L.size()) return false;
    if (x.equals(L.get(pos))) return true;
    return LinSearch(L, pos+1, x);
}
</code></pre><h4 id=binary-search>Binary search</h4><ul><li>Divide &amp; Conquer<li>Require a sorted list</ul><pre><code>boolean BinSearch(L, first, last, x) {
    if (first &gt; last) return false;
    int center = (first + last)/2.
    if (x.equals(L.get(center))) return true;
    if (x.compareTo(L.get(center))) &lt; 0)
        return BinSearch(L, first, center-1, x);
    else
        return BinSearch(L, center+1, last, x);
}
</code></pre><h2 id=categorising-adt-part-1>Categorising ADT Part 1</h2><ul><li>Linear ADT<ul><li>list<li>stack<li>queue</ul></ul><p>bext.prev relationship, 1 processor, 1 successor<ul><li>Hierarchical<ul><li>Tree</ul></ul><p>parent/child relationship, 1 processor, multiple successor<ul><li>Graphical<ul><li>graph</ul></ul><p>pairwise relationship, 1 or more processors, 1 or more successors<h2 id=general-tree>General Tree</h2><ul><li>each node can store an arbitrary number of child</ul><p>Treenode
data -&gt;
children -&gt; treenode<h2 id=determining-height-of-a-general-tree>Determining Height of a General Tree</h2><h4 id=writing-a-recursive-definition-for-the-height-of-a-general-tree>writing a recursive definition for the height of a general tree</h4><p>height(t) = 0 if t is null<br>height = 1 if t is a leaf<br>height = 1 + max(height of children)<pre><code class=language-java>public int height() {
    
}
private int height(Treenode&lt;T&gt; t) {
    if (t == null) return 0;
    if (t.getChildren().size() == 0) return 1;
    
    // get max height of all children
    int max = 0;
    for (Treenode&lt;T&gt; child: t.getChildren()) {
        int h = height(child);
        IF (h &gt; max) max = h;
    }
    reture 1+max;
}
</code></pre><h2 id=binary-tree>Binary Tree</h2><h4 id=each-node-has-at-most-two-children>Each node has at most two children</h4><blockquote><p>//TODO: Arrange notes</blockquote><p>get/set method<h2 id=tree-traversal>Tree Traversal</h2><h4 id=goal-visit-every-node-in-the-tree-exactly-once>Goal: visit every node in the tree exactly once</h4><ul><li>V - visit -- do something with the nodes data<li>C - children -- traverse the children (left to right)<li>L - Left child -- traverse the left child<li>R - right child -- traverse the right child</ul><h4 id=level-order>Level order</h4><p>Top to bottom
Left to right<h4 id=pre-order>Pre-order</h4><p>VLR (VC)<h4 id=post-order>Post-order</h4><p>LRV (CV)<h4 id=in-order>In-order</h4><p>LVR<h2 id=categorising-adt-part-2>Categorising ADT part 2</h2><p>Position-oriented ADTs: add, remove, lookup by position
List stack, queue
Value-oriented ADTs: operation occur at a specific based on the items value
Sorted List: name determines the position to add remove, lookup
MapADT - key-value pair
* in 367, we only focus on the keys
* we assume that you can get the value from the key<h2 id=binary-search-tree>Binary Search Tree</h2><p>Value-Oriented - Duplicates are not allowed<h4 id=goal>Goal</h4><ul><li>Fast lookup<li>Insert<li>Remove</ul><p><strong>BST are not guaranteed to be ideal shape</strong><p>for each node N having a key value K, L &lt; K for a very node in the left subtree of N, R is greater than K for every node in the right subtree of N</p><a class=readmore href=/2016/11/3/cs367-introtods14.html title="CS367-Introduction To Data Structures 14">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 12</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/10/13/cs367-introtods12.html title="October 13, 2016">October 13, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=stack-adt>Stack ADT</h2><h4 id=operation>Operation</h4><ul><li>push(E item)<li>E pop()<li>E peak()<li>boolean isEmpty()</ul><h4 id=implementing-using-an-array>Implementing using an array</h4><p><strong>Top at index</strong>
Push: O(N)<br>Pop: O(N)<br>Peak: O(1) //must shift<br><strong>Top at numItem</strong><br>Push: O(1) Pop: O(1)<br>Peak O(1) //top = numItem -1<h4 id=implementing-using-a-chain-of-nodes>Implementing using a chain of nodes</h4><p><strong>if top = head</strong><br>Push: O(1)<br>Pop: O(1)<br>Peak: O(1) //top=head<br><strong>top = tail</strong><br>Push: O(1)<br>Pop: O(N)//update tail<br>Peak: O(1)<br><strong>top at end, no tail</strong><br>Push: O(N)<br>Pop: O(N)<br>Peak: O(N)<h2 id=queue-adt>Queue ADT</h2><h4 id=concepts>Concepts</h4><p>Like a checkout line. First in First out<h4 id=operations>Operations</h4><p>(present traditional names)<ul><li>void enqueue(E item) // add<li>E dequeue() // remove<li>boolean isEmpty()</ul><h4 id=implementing-using-a-chain-of-nodes-1>Implementing using a chain of nodes</h4><p><strong>Option 1: front of queue is at head, rear of queue is at tail</strong><ul><li>Enqueue: O(1)<li>Dequeue: O(1)</ul><p><strong>front of queue is at tail, rear of queue is at head</strong><ul><li>Enqueue: O(1)<li>Dequeue: O(N)</ul><h4 id=implementing-using-an-array-1>Implementing using an array</h4><p><strong>Option 1: front of queue is at 0, rear of queue is at n-1</strong><ul><li>Enqueue: O(1)<li>Dequeue: O(N)</ul><p><strong>Option 2: front of queue at numItem -1, rear of queue at 0</strong><ul><li>Enqueue: O(N)<li>Dequeue: O(1)</ul><p><strong>Option 3: front of queue at first item, rear of queue at last item</strong><ul><li>Enqueue: O(1)<li>Dequeue: O(1)</ul><pre><code class=language-java>private int incrementIndex(int i){
    if (i == items.length - 1)
        return 0;
    return i++
}
</code></pre><h2 id=implementing-using-circular-array>Implementing using Circular Array</h2><h4 id=concepts-1>Concepts</h4><p>using free elements at front of array<br>Only expand if all elements used<h4 id=enqueue-e-item>Enqueue (E item)</h4><pre><code class=language-java>if (numItems == items.length)
    expand();
R = incrementIndex(R);
items[R] = item;
numItems ++;
</code></pre><h4 id=e-dequeue>E Dequeue()</h4><pre><code class=language-java>if (numItems &lt;= 0)
    throw new EmptyQueueException();
E temp = items[F];
F = incrementIndex(F);
return temp;
</code></pre><h4 id=expand>Expand()</h4><p>System.<strong>ArrayCopy</strong>(q(source), F(srcPos), newQ(dest), Q(destPos), length)<pre><code class=language-java>
</code></pre><h2 id=end-of-exam-1>End of exam 1</h2><h2 id=tree-terminology>Tree Terminology</h2><blockquote><p>outlineW6R -3</blockquote><ol><li>Root: H<br><li>How many leaves: 4, EFDI<li>3<li>B<li>2<li>G<li>6<li>A, H<li>3<li>5<li>4</ol><h2 id=priority-queue-adt>Priority Queue ADT</h2><p>store items - so they can be remove in order of hiest priority<h4 id=priority>Priority</h4><ul><li>each item is assigned a priority value<li>allow duplicated priority<li>highest priority is smallest (or largest) value (just be clear which it is)</ul><h4 id=concepts-2>Concepts</h4><p>Items are removed in order of their priority<p><strong>Goal</strong>: Fast acces to highest priority<h4 id=operation-1>Operation</h4><ul><li>void insert(comparable item)<li>comparable removeMax()<li>comparable getMax()<li>boolean isEmpty()</ul><a class=readmore href=/2016/10/13/cs367-introtods12.html title="CS367-Introduction To Data Structures 12">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 11</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/10/11/cs367-introtods11.html title="October 11, 2016">October 11, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=number-guessing-game>Number Guessing Game</h2><h6 id=linear-search>Linear Search</h6><ul><li>O(N) W.C.<li>O(1) B.C.<li>O(N/2) A.C.</ul><h6 id=binary-search>Binary Search</h6><p>7 Guess for N &lt; 100<h2 id=complexity-caveats>Complexity Caveats</h2><h4 id=small-problem-size>Small Problem Size</h4><p>Complexity analysis is not a good basic for choosing an algorithm when the problem size is small<br>T(N) = N^2 + 3<br>T(N) = 10 * N + 30<h4 id=same-complexity-or-dominant-operations>Same Complexity (or dominant operations)</h4><ul><li>look at multiplication constant<li>look at lower order terms<li>look at the amount of memory<li>look at expected problem size</ul><h2 id=returning-n-paper-to-n-students>Returning N Paper to N students</h2><h4 id=algorithm-1>Algorithm 1:</h4><p>call out each name,<br>have student come forward &amp; pick up<br>O(2N)<br>Best Case: O(N)<br>Worst Case: O(N)<h4 id=algorithm-2>Algorithm 2:</h4><p>hand pile to first student,<br>student linearly searches through papers &amp; takes hers/his,<br>pass pile to next student who does likewise<br>O(N^2) Quadratic<h4 id=algorithm-3>Algorithm 3:</h4><p>sort the papers alphabetically, O(NlogN)<br>hand pile to first student who does binary search, &lt; NlogN<br>pass to next student who does likewise<br>O(2NlogN) ==&gt; O(NlogN)<h2 id=shadow-array>Shadow Array</h2><ul><li>if items is full, expand shadow<ol><li>create new arr 2x shadow<li>reassign items to shadow (item = shadow)<li>reassign shadow to new array (shadow = newarray)<li>place new item in items<li>place new item in shadow array<li>copy one item from items to shadow</ol></ul><p>All Constant time, O(6)<br>We add to array (w/shadow) is O(1) constant<h2 id=stack-adt>Stack ADT</h2><h4 id=operation>Operation</h4><ul><li>push(E item)<li>E pop()<li>E peak()<li>boolean isEmpty()</ul><a class=readmore href=/2016/10/11/cs367-introtods11.html title="CS367-Introduction To Data Structures 11">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 10</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/10/4/cs367-introtods10.html title="October 4, 2016">October 4, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><table><thead><tr><th align=center>T(N) = n+2<th align=center>f(N) = N<th align=center>C=2, cxf(N)<tbody><tr><td align=center>T(1) = 3<td align=center>f(1) = 1<td align=center>2<tr><td align=center>T(2) = 4<td align=center>f(2) = 2<td align=center>4<tr><td align=center>T(N) = N+2<td align=center>f(N) = N<td align=center>2N</table><p><strong>Choose the smallest growth rate function that is an upper bound</strong><h2 id=complexity-of-java-code>Complexity of Java Code</h2><h4 id=basic-operation>Basic operation</h4><p>O(1) anything that is not a control statement or method call<h4 id=sequence-os-statement-sum-of-comp-of-each-stmt>Sequence os statement //Sum of comp of each stmt</h4><pre><code>statement1;
statement2;
...
statementk;
</code></pre><p>O(K) ==&gt; O(1) CONSTANT<h4 id=if-else>If else</h4><pre><code class=language-java>if (cond) {
    //if sequence of statements
}
else {
    //else sequence of statements
}
</code></pre><p>O<br>=O(Cond) + Max(Gif, Gelse)
=Max(Cond, Gif, Gelse)<p><strong>Pick the block that does most work</strong><h4 id=basic-loop>Basic loop</h4><pre><code class=language-java>for (i = 0; i &lt; j; i++) {
    //sequence of statements
}
</code></pre><p>Repeats J times<br>if O(1) then JxO(1) = O(j)<h4 id=nested-loop>Nested Loop</h4><pre><code class=language-java>for (i = 0; i &lt; N; i++) {
    for (j = 0; j &lt; M; j++) {
        //sequence of statements
    }
}
</code></pre><p>N * M * Gbody = O(N * M * Gbody)<br>When doing analysis, keep vars.<h4 id=loops-with-nested-method-calls-assume-problem-size-based-on-n>Loops with nested method calls (assume problem size based on N)</h4><pre><code class=language-java>for (i = 0; i &lt; N; i++) {
    f1(i); //assume O(1)
}
for (i = 0; i &lt; N; i++) {
    f2(N); //assume O(N)
}
for (i = 0; i &lt; N; i++) { // 
    f3(i); //assume O(i)
}
</code></pre><p><strong>Must unroll loop if O dependes on a loop</strong><h2 id=comparing-listadt-implentation>Comparing ListADT Implentation</h2><p>| | Constructor | add(E) | add(E) at pos | contains(E) | size | isEmpty | get(int) | remove(int) |
| | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--:
| Array | O(1) | O(N) | O(N) | O(N) Linear Search | O(1) | O(1) | O(1) | O(N) |
| Single-Linked List | same | O(1) | O(N) | same | O(1) | O(1) | O(N) W.C. O(N/2) A.C O(1) B.C. | O(N) W.C O(N/2) A.C O(1) B.C |
| Circular SLL | same | same | same | same | O(1) | O(1) | same | same |
| Double Linked List | same | same | same | same | same | same | same | same |
| Circular DLL | same | same | same | same | same | same | same | same |<p>expand the array is O(N) lenear for , O(1) for shadowarray<h2 id=comparing-listadt-implementation>Comparing ListADT IMplementation</h2><h4 id=problem-size>Problem Size</h4><p>Each Element(same for all, ignore for today)
1. Array
- 2N
- Shift items
- Expand array
- naive
- shadow
2. Singly-Linked List
- 2N
3. Circular SLL
- 2N
4. Doubly-Linked List
- 3N
5. Circular DLL
- 3N<p>At first, getting links correct.<br>Draw pictures of chain of node, keep references to curr, prev</p><a class=readmore href=/2016/10/4/cs367-introtods10.html title="CS367-Introduction To Data Structures 10">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 08</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/10/4/cs367-introtods08.html title="October 4, 2016">October 4, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h3 id=java-for-each-loop>Java For-Each loop</h3><ul><li>Work on arrays and Iterable type
```
ListADT<string> list &hellip; //assume list of words
Iterator<string> itr = list.iterator();
while (itr.hasNext())
System.out.println(itr.next());</ul><pre><code>
### Problem - Implementing Multiple Intefaces
```java
public class LinkedList&lt;E&gt; implements ListADT&lt;E&gt;, Iterable&lt;E&gt; { ... }

ListADT&lt;String&gt; list = new LinkedList&lt;String&gt;();
Iterator&lt;String&gt; itr = list.iterator(); //Fails! No iterator in ListADT

Iterable&lt;String&gt; list = new LinkedList&lt;String&gt;(); //ok
Iterator&lt;String&gt; itr = list.iterator(); //ok
list.add(&quot;newitem&quot;); //FAILS list is not ListADT anymore

LinkedList&lt;String&gt; list = new LinkedList&lt;String&gt;(); //ok
Iterator&lt;String&gt; itr = list.iterator(); //ok
// Loss of generality - list is a LinkedLlist, not any other type
</code></pre><h3 id=solution>Solution</h3><pre><code class=language-java>public interface ListADT&lt;E&gt; extends Iterable&lt;E&gt; {
    //List Ops Listed here
    //iterator()
}
</code></pre><h3 id=make-linkedlist-iterable>Make LinkedList Iterable</h3><pre><code class=language-java>public class LinkedList&lt;E&gt; implements ListADT&lt;E&gt; {
    private Listnode&lt;E&gt; head;
    private int numItems;
    public LinkedList() { ... }
    public void add(E item) { ... }
    public E get(int pos) { ... }
    . . .
    //must define b.c. ListADT inherited the method requirement
    public Iterator&lt;E&gt; iterator() {
        return new LinkedListIterator&lt;E&gt;(head); 
        // DIRECT access to List
    }
}
</code></pre><h2 id=double-and-circular-linking>Double and Circular Linking</h2><ol><li>curr.getNext().setPrevious(newnode);<li>newnode.setNext(curr.getNext())<li>newnode.setPrevious(curr)<li>curr.setNext(newnode)</ol><h4 id=circular-singly-linked-chains-of-nodes>Circular Singly-Linked Chains of Nodes</h4><p>Last node next refers to the first node<br>- If First, Last and position do not mastter, just keep a curr referense
- If First, Last and position matter, just keep tail reference (head is same as tail.getNext())<h4 id=circular-doubly-linked-chains-of-nodes>Circular Doubly-Linked Chains of Nodes</h4><ul><li>if Position does not matter, just keep current reference.<li>if Position does matter, keep head reference</ul><h2 id=analyzing-algorithm-efficiency>Analyzing Algorithm Efficiency</h2><h4 id=complexity>Complexity</h4><ol><li>Use to compare algorithms that solve same problem<li>Use to classify the use of resources and how they scale
Resources<ul><li>Time: # of operation executed<li>Space: Amount of memory
Problem<li>Some aspect of the problem that when changed, affects the resources used.</ul></ol><h4 id=if-problem-size-doubles-and-the-number-of-operation>If problem size doubles and the number of operation</h4><ul><li>Stays the same =&gt; Constant<li>Doubles =? Linear<li>Quadruples =&gt; Quadratic</ul><p>Complexity classifications is first-order analysis<br>classify based on 1st(highest) order operation<br>Focus on most significant differences first !<h2 id=exmaple-complexity-analysis-of-giving-a-toast>Exmaple: Complexity Analysis of giving a toast</h2><h4 id=what-is-the-problem-size>What is the problem size?</h4><p>Num of people, n<h4 id=what-is-the-algorithm>What is the algorithm</h4><ol><li>Fill the glasses (one bottle) LINEAR<li>Raise the glasses CONSTANT<li>Clink with everyone QUADRATIC<li>Drink CONSTANT</ol><h4 id=characterize-the-problem>characterize the problem</h4><p>Person 1 clinks with persons 2-N [n-1]
Person 2 clinks with persons 3-N [n-2]
&hellip;
Person n-2 clinks with persons N-1 &amp; N [2]
Person n-1 clinks with person N [1]
Person N clicks with no one [0]<p>sum of Clinks: n/2x(n-1)
n^<sup>2</sup>&frasl;<sub>2</sub>: fist order term
QUADRATIC<p>is not concerned<h2 id=big-o-notation>Big-O notation</h2><h4 id=concept>Concept</h4><p>CONStANT O(1)
LINEAR O(N)
QUADRATIC O(N^2)<h4 id=some-growth-rate-function-in-order-of-complexity>Some growth rate function in order of complexity</h4><p>1, logN, N, NlogN, N^2, N^4, N^k, 2^N, N!, N^N<h4 id=simplifying-equations>Simplifying Equations</h4><p>time(N) =&gt; T(N) = 4N^2 + 3N -11
1. Drop lower order terms
2. Drop multiplication constant<h4 id=formal-definition>Formal Definition</h4><p>A function T(N) is O(F(N))
if for some constant C and some number n
such that T &lt;= F(N) for all N &gt; n</p><a class=readmore href=/2016/10/4/cs367-introtods08.html title="CS367-Introduction To Data Structures 08">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 07</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/9/29/cs367-introtods07.html title="September 29, 2016">September 29, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><blockquote><p>L7 - 3</blockquote><h3 id=making-a-chain-of-nodes>Making a chain of nodes</h3><pre><code class=language-java>Listnode&lt;String&gt; head = new Listnode&lt;String&gt;(&quot;yippie&quot;, new Listnode&lt;String&gt;(&quot;ki&quot;, new Listnode&lt;String&gt;(&quot;yay&quot;)))

Listnode&lt;String&gt; head = new Listnode&lt;String&gt;(&quot;yay&quot;);
head = new Listnode&lt;String&gt;(&quot;ki&quot;, head);
head = new Listnode&lt;String&gt;(&quot;yippie&quot;, head);
</code></pre><h3 id=traversing-a-chain-of-nodes>Traversing a chain of nodes</h3><pre><code class=language-java>Listnode&lt;String&gt; curr = head
while (curr != null) {
    count++;
    curr = curr.getNext();
}
</code></pre><p>When traversing a chain of nodes,use another variable to track<h3 id=adding-a-node-at-the-chain-s-end>Adding a node at the chain&rsquo;s end</h3><pre><code class=language-java>//Traverse the chains of nodes until we reach last node
Listnode&lt;String&gt; curr = head
while (curr.getNext() != null) {
    curr = curr.getNext()
}

// Create new node, and link to last node
curr = new Listnode&lt;String&gt;(&quot;rear&quot;);
</code></pre><h3 id=removing-a-node-from-a-chain>Removing a Node from a chain</h3><pre><code class=language-java>head.getNext().setNext(head.getNext().getNext().getNext());
</code></pre><pre><code class=language-java>Listnode&lt;String&gt; curr = head;
for (int i=0; i&lt;N-1; i++){
    curr = curr.getNext();
}
curr.setNext(cur.getNext().getNext());

</code></pre><a class=readmore href=/2016/9/29/cs367-introtods07.html title="CS367-Introduction To Data Structures 07">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 08</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/9/29/cs367-introtods08.html title="September 29, 2016">September 29, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=java-visibility-modifiers>Java visibility modifiers</h2><p>public <code>public class ArrayList</code><br>private <code>private Object[] items</code><br>protected <code>protected String name</code><br>package <code>class Listnode&lt;E&gt;</code><p>Goal: hide d.s. from other class, but make available to our own class<p>Steps to create a package:
1. Create a folder LinkedList
2. Put in the folder:
- Listnode.java
- LinkedList.java
- LinkedListIterator.java
3. Programmer of classes that need LinkedList
- <code>import LinkedList.*</code><p>CS367 Don&rsquo;t create packages<h2 id=recall-the-list-adt>Recall the List ADT</h2><p>Separate interface from implementation
- allows programmers to choose diff implementation with only minor changes(one line)<p>ListADT
- ensures that different implementation type define the same set of operation.<h2 id=linkedlist-implementing-listadt-using-a-chain-of-nodes>LinkedList - Implementing ListADT using a Chain of Nodes</h2><pre><code class=language-java>public class LinkedList&lt;E&gt; implements ListADT&lt;E&gt; {
    private Listnode&lt;E&gt; head;
    private int numItems;

    public LinkedList() {
        head = null;
        numItem = 0;
    }

    public void add(E item) {
        if (item == null) throw new IllegalArgumentException();
        
        Listnode&lt;E&gt; newnode = new Listnode&lt;E&gt;(item)
        if (null == head)
            head = newnode;
        else{
            Listnode&lt;E&gt; curr = head;
            while(curr.getNext() != null)
                curr = curr.getNext();
            curr.setNext(newnode);
        }
        numItem++;
    }

    public E get(int pos) {
        if (pos &lt; 0 || pos &gt;= numItems)
            throw new IndexOutOfBoundsException();
        
        Listnode&lt;E&gt; curr = head;
        for (int p=0; p&lt;pos; p++)
            curr = curr.getNext();

        return curr.getData();
    }
}
</code></pre><h2 id=header-node>Header Node</h2><ul><li>header node does not store data<li>empty condition head references header node<li>Not the same as head reference</ul><pre><code class=language-java>
public class LinkedList&lt;E&gt; implements ListADT&lt;E&gt; {
    private Listnode&lt;E&gt; head;
    private int numItems;
    public LinkedList() {
        head = new Listnode&lt;E&gt;(null); //~~null;~~
        numItems = 0;
    }
    public void add(E item) {
        if (item == null) throw new IllegalArgumentExceptions();
        Listnode&lt;E&gt; newnode = new Listnode&lt;E&gt;(item);
        //Special Case: empty list
        //if (head == null) {
        //    head = newnode;
        //}
        //General Case: non-empty list
        //else {
            Listnode&lt;E&gt; curr = head;
            while (curr.getNext() != null)
                curr = curr.getNext();
            curr.setNext(newnode);
        //}
        numItem ++;
    }
}
</code></pre><h2 id=tail-reference-no-header-nodes>Tail Reference (no header nodes)</h2><ul><li>reference to the last node</ul><pre><code class=language-java>public class LinkedList&lt;E&gt; implements ListADT&lt;E&gt; {
    private Listnode&lt;E&gt; head;
    private Listnode&lt;E&gt; tail;
    private int numItems;

    public LinkedList() {
        head = null;
        tail = null;
        numItem = 0;
    }

    public void add(E item) {
        if (item == null) throw new IllegalArgumentException();

        Listnode&lt;E&gt; newnode = new Listnode&lt;E&gt;(item)
        if (null == head){
            head = newnode;
            tail = newnode;
        } else {
            //Listnode&lt;E&gt; curr = head;
            //while(curr.getNext() != null)
            //    curr = curr.getNext();
            //curr.setNext(newnode);
            tail.setNext(newnode);
            tail = newnode;
            numItem++;
        }
        numItem++;
    }
}
</code></pre><h2 id=implementing-linkedlistiterator>Implementing LinkedListIterator</h2><ul><li><p>Indirect iterator<ul><li>mylist<li>currPos</ul><li><p>Direct iterator<ul><li>curr</ul></ul><a class=readmore href=/2016/9/29/cs367-introtods08.html title="CS367-Introduction To Data Structures 08">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 06</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/9/22/cs367-introtods06.html title="September 22, 2016">September 22, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><a class=readmore href=/2016/9/22/cs367-introtods06.html title="CS367-Introduction To Data Structures 06">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 05</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/9/20/cs367-introtods05.html title="September 20, 2016">September 20, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p>Java uses indirect iterator
List interface extends Iterable<p>When a problem is detected, an exception is constructed.
When exception is thrown, an exception execution cycle<pre><code class=language-java>throw exceptionObject;
</code></pre><p>swtich to exception execution cycle<pre><code class=language-java>// add to remove method ArrayBag
if (numItems &lt;= 0)
    throw new EmpltyBagException();
</code></pre><h2 id=exception-handling-resolving-a-problem>Exception Handling – Resolving a Problem</h2><ul><li>try block with code that may cayse an exception<li>catch block: one block for type of exception we can handle<li>finally block : always executed of try block is executed</ul><pre><code class=language-java>// FORMAT
try {
    // try block
    code that might cause an exception to be thrown
} catch (ExceptionType1 identifier1) {
    // catch block
    code to handle exception type 1
} catch (ExceptionType2 identifier2) {
    // catch block
    code to handle exception type 2
}
    ... more catch blocks
finally {
    // finally block - optional
    code always executed when try block is entered
}
</code></pre><h4 id=example>Example</h4><pre><code class=language-java>try {
    for (int i=0;i &lt; 1000; i++)
        bag.remove();
} catch (EmptyBagException egg) {
    S.O.P(&quot;remove caused EmpltyBagException&quot;);
}
</code></pre><h3 id=exception-execution>Exception Execution</h3><h5 id=normal-execution>Normal Execution</h5><p>Execute:
- Normal code (not in tru block)
- Code in a try block
- code in a finally block
Skip:
- Code in catch block<h5 id=exception-handling>Exception Handling</h5><p>Skip:
- normal code
- remaining code in try block<p>Execute:
- Code in matching catch clause
- Code in finally block<p>Switch to Normal Exec:
- if exception is caught<h5 id=searching-for-a-matching-catch>Searching for a Matching Catch</h5><ol><li><p>Locally<ul><li>Look in same method/current method</ul><li><p>Remotely<ul><li>if Current method can&rsquo;t handle, look to next method on call stack</ul></ol><h5 id=checking-a-match>Checking a Match</h5><ol><li>Match found<ul><li>execute code in matching catch<li>skip other catch blocks<li>execute code in finally block<li>switch back to normal execution</ul><li>No Match Found<ul><li>Execute in finally block<li>Pass exception object to calling method</ul></ol><blockquote><p>L5 - 5</blockquote><ol><li>main{<br>A{<br>B{after C, after D, B finally, }B<br>After B, }A<br>After A, After E, main finally, }main<br></ol><p>3.
main{<br>A{<br>B{main-green, main-finally,}main<h2 id=throws-clause>Throws clause</h2><p>Checked: Compiler checks that you do something if exception<br>(IOException)<br>a. catch it<br>b. throw it<br>Unchecked: Compiler does not check that you have a plan<br>(Runtime Exception)<br>a. catch it<br>b. throw it<br>c. ignore it, no additional code added</p><a class=readmore href=/2016/9/20/cs367-introtods05.html title="CS367-Introduction To Data Structures 05">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 04</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/9/15/cs367-introtods04.html title="September 15, 2016">September 15, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><h2 id=iterators>Iterators</h2><ol><li>Ask container for an iterator<li>Use the iterator to get to &ldquo;step thru&rdquo; the items in the containers</ol><h2 id=operations>Operations</h2><p>Container<ul><li>operations that returns an iterator</ul><p>Iterator class<ul><li>the object returned by container and used to track the iteration<li>return true if there are remaining items<li>return the next item and advances the iteration</ul><h2 id=iterators-in-java-api>Iterators in Java API</h2><pre><code class=language-java>Iterator&lt;T&gt; iterator() // has method that returns an Items
</code></pre><p>Properties of the Iterator returned<ul><li>at beginning, points to 1st iter container<li>container may be empty<li>iterator() method is defined, returns an iterator with its properties.</ul><h4 id=boolean-hasnext>boolean hasNext()</h4><p>returns true if there are more iterations to process<h4 id=e-next>E next()</h4><p>returns the next item and advances the pointer<h4 id=void-remove>void remove()</h4><p>implementing remove() is beyond scope of CS367<br>requires locks and synchronized code<br>CS367: DISABLE this operation<blockquote><p>L4 - 3</blockquote><pre><code class=language-java>Interator&lt;string&gt; itr = words.iterator();
while (itr.hasNext()) {
    System.out.Print(itr.Next());
}
itr = words.iterator();
while (itr.hasNext()) {
    System.out.Print(itr.Next().length())
}
</code></pre><blockquote><p>L4-4</blockquote><pre><code class=language-java>if (list==null || list.isEmpty())
    return false;
//Do this on your own
</code></pre><h2 id=implementation>Implementation</h2><h4 id=indirect-access>Indirect Access</h4><ul><li>Do not have a reference to a container&rsquo;s internal data structures<li>Do have a reference to the container</ul><ol><li>The iterator has a reference to the container.<li>Call the container&rsquo;s methods to implement <code>hasNext</code> and <code>next()</code></ol><p><strong>SAFER</strong><h4 id=direct-access>Direct Access</h4><ul><li>Does have a reference to internal data structures<li>Does not have a reference to container itself.</ul><ol><li>the container creates an iterator with the date structure (array)<li>iterator<ul><li>can change the data structure (<strong>BUT must not</strong>)<li>can(must) have additional data structures to track progress</ul></ol><p><strong>FASTER</strong> Direct Access<blockquote><p>L4-5</blockquote><pre><code class=language-java>
import java.util.*;
public class SimpleArrayListIterator&lt;E&gt; implements Iterator&lt;E&gt; {
    private SimpleArrayList&lt;E&gt; myList;
    private int currPos;
    public SimpleArrayListIterator(SimpleArrayList&lt;E&gt; list){
        myList = list;
        currPos = 0;
    }

    public boolean hasNext() {
        return (currPos &lt; myList.size());
    }

    public E next() {
        if (!hasNext())
            throw new NoSuchElementException();
        E result = myList.get(currPos);
        currPos ++;
        return result;
    }

    public void remove() throws UnsupportedOperationException {
        throw new UnsupportedOperationException();
    }
</code></pre><blockquote><p>L4-7</blockquote><pre><code class=language-java>
import java.util.*;
public class ArrayBagIterator&lt;E&gt; implements Iterator&lt;E&gt; {
    private E[] items;
    private int numItems;
    private int currPos;

    public ArrayBagIterator(E[] items, int n) {
        this.items = items;
        numItems = n;
        currPos = 0;
    }

    public boolean hasNext() {
        return currPos &lt; numItems;
    }

    public E next() {
        if (currPos &gt;= numItems) throw new NoSuchElementException();
        E result = items[currPos];
        currPos++;
        return result;
    }

    public void remove() {
        throw new UnsupportedOperationException();
    }
}
</code></pre><p><strong>Can&rsquo;t use indirect because the ArrayBag does not has <code>get()</code> method</strong><blockquote><p>L4-8</blockquote><pre><code class=language-java>import java.util.*;
public interface BagADT&lt;E&gt; {
    void add(E item);
    E remove() throws NoSuchElementException;
    boolean isEmpty();
    Iterator&lt;E&gt; iterator();
}
</code></pre><pre><code class=language-java>
import java.util.*;
public class ArrayBag&lt;E&gt; implements BagADT&lt;E&gt; {
    // *** Data members (fields) ***
    private E[] items;
    private int numItems;
    private static final int INIT_SIZE = 100;
    //*** required BagADT methods ***
    public void add(E item) { ... }
    public E remove() throws NoSuchElementException { ... }
    public boolean isEmpty() { ... }


    public Iterable&lt;E&gt; iterator() { //returns iterator
        return new ArrayBagIterator(items, numItems);
    }
}
</code></pre><a class=readmore href=/2016/9/15/cs367-introtods04.html title="CS367-Introduction To Data Structures 04">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 03</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/9/13/cs367-introtods03.html title="September 13, 2016">September 13, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p>List Operations<pre><code class=language-java>void add(E item);
void add(int pos, E item);
//(at end) and (at position pos | insert)
E get(int pos);
E remove(int pos);
// get and remove, true if item exists in list
boolean contains(E item);
int size();
boolean isEmpty();
</code></pre><blockquote><p>L3 -3</blockquote><h3 id=code-tracing>Code Tracing.</h3><ul><li>How can we check<li>Code tracing -- correctly</ul><p><strong>myList [a][b][c][d][e]</strong><table><thead><tr><th align=center>index<th align=center>myList.size<th align=center>Eles<tbody><tr><td align=center>0<td align=center>5<td align=center>(a)bcde<tr><td align=center>1<td align=center>4<td align=center>b&copy;de<tr><td align=center>2<td align=center>3<td align=center>bd(e)<tr><td align=center>3<td align=center>2<td align=center>bd</table><p>Removes items at even indexes<blockquote><p>L3 - 4</blockquote><pre><code class=language-java>for (int i=0; i&lt; myList.size()/2; i++) {
    E tmp = myList[i];
    myList[i] = myList[myList.size()-i] ;
    myList[myList.size()-i] = tmp;

}
</code></pre><p>Implemention of ListADT with generics<pre><code class=language-java>
public class SimpleArrayList&lt;E&gt; implements ListADT&lt;E&gt; {
    private E[] items;
    //the items in the List
    private int numItems;
    //the # of items in the List
    public SimpleArrayList() {
        items = new Object[100];
        numItems = 0
    }
    //*** required ListADT methods ***
    public void add(E item) { ... }
    public void add(int pos, E item) { ... }
    public E remove(int pos) { ... }
    public E get (int pos) { ... }
    public boolean contains (E item) { ... }
    public int size() { ... }
    public boolean isEmpty() { ... }
    //*** additional optional array list methods ***
}

public boolean contains(E item) {
    if (item == null)
        throw new IllegalArgumentException();
    for (int i=0; i&lt;numItems; i++)
        if (items.equals(items[i]))
            return true;
    return false;
}

public void add(E item) {
    // check if null
    if (item == null)
        throw new IllegalArgumentException();
    // check for full array
    if (items.length == numItems)
        expandArray();

    items[numItems] = item;
    numItems++;
}

private void expandArray() {
    // create new array
    E[] a = (E[])new Object(items.length * 2)

    // Copy items from orig to new
    for ( int i = 0; i&lt;numItems; i++) {
        a[i] = items[i]
    }
    //reassign items to new array
    items = a
}
</code></pre><blockquote><p>L3 -8</blockquote><ul><li>Why ?</ul><p>Program 1<br>- What ?<p>ADt ListAPI<br>- How ?<p>Implementation<br>ArrayList<br>SunokeArrayList<h2 id=iterator>iterator</h2><ul><li>Track progress( objects used to step through a collection of items )<li>an abstracion of a pointer<li>sotres a position in a particular collection<li>position of the iterator can change<li>item at the position of the iterator can be accessed.</ul><p>Coding Tip:
seperate iter from the container class</p><a class=readmore href=/2016/9/13/cs367-introtods03.html title="CS367-Introduction To Data Structures 03">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367-Introduction To Data Structures 02</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/9/8/cs367-introtods02.html title="September 8, 2016">September 8, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><pre><code class=language-java>public class ArrayBag
//instance variables
private Object[] items;
private int numItems;

//constructor
public ArrayBag(){
    items = new Object[100];
    numItems = 0;
}
//BagADT methods
public Boolean isEmpty() {
    return ()numItems &lt;1)
}

public void add(Object item){

}
public Object remove() throws NoSuchElementException {
    if (isEmpty()) throw new NoSuchElementException();
    return Items[--numItems];
}
</code></pre><blockquote><p>CS367 Outline 2 P2
To generalize, good for code reuse<p>CS367 Outline 2 P3</blockquote><a class=readmore href=/2016/9/8/cs367-introtods02.html title="CS367-Introduction To Data Structures 02">Read more</a></article></section><hr><section class="post type-post status-publish format-standard hentry category"><article class=clearfix><header class=entry-header><h2 class=entry-title>CS367(Text)-Introduction</h2><div class=entry-meta><span class="by-author vcard author"><span class=fn>By <a href=https://rongyi.blog>L.E.R</a></span></span>
<span class="date updated"><a href=/2016/9/7/cs367-text-introduction.html title="September 7, 2016">September 7, 2016</a></span>
<span class=category><a href=/category/Computer%20Science.html rel="category tag">Computer Science</a>;
<a href=/category/CS367.html rel="category tag">CS367</a>;</span>
<span class=category>Tags
<a href=/tags/CS367.html rel="category tag">#CS367</a></span>
<span class=comments><a href>Comments are closed</a></span></div></header><p><a name=top><h1>Introduction: Abstract Data Types</h1></a><hr><p><h2>Contents</h2><ul><li><a href=#intro>Good Programs Use Abstraction</a><li><a href=#benefits>Benefits of Abstract Data Types</a><li><a href=#adts>Abstract Data Types (ADTs)</a></ul><hr><p><a name=intro><h2>Good Programs Use Abstraction</h2></a>What makes a program <em>good</em>?<ol><li>It works (as specified!).<li>It is easy to understand and modify.<li>It is reasonably efficient.</ol><p>One way to help achieve (2) (which helps with (1)) is to use
<span class=term>abstract data types</span>, or <span class=term>ADTs</span>.
The idea of an ADT is to separate the notions of <span class=term>specification</span>
(what kind of thing we're working with and what operations can
be performed on it) and <span class=term>implementation</span> (how the thing and
its operations are actually implemented).<p><a name=benefits><h2>Benefits of using Abstract Data Types</h2><ul><li>Code is easier to understand (e.g., it is easier to see &ldquo;high-level&rdquo;
steps being performed, not obscured by low-level code).<li>Implementations of ADTs can be changed (e.g., for efficiency)
without requiring changes to the program that uses the ADTs.<li>ADTs can be reused in future programs.</ul><p>Fortunately for us,
object-oriented programming languages (like Java) make it easy for
programmers to use ADTs:
each ADT corresponds to a <span class=term>class</span> (or <span class=term>Java interface</span> - more
on this later) and the operations on the
ADT are the class/interface&rsquo;s <span class=term>public methods</span>.
The user, or client, of the ADT only needs to know about the method
<span class=term>interfaces</span> (the names of the methods, the types of the
parameters, what the methods do, and what, if any, values they return),
not the actual implementation (how the methods are implemented, the private data
members, private methods, etc.).<p><a name=adts><h2>Abstract Data Types</h2></a><p>There are two parts to each ADT:<ol><li>The <span class=term>public</span> or <span class=term>external</span> part, which consists of:<ul><li>the conceptual picture (the user&rsquo;s view of what the object looks
like, how the structure is organized)<li>the conceptual operations (what
the user can do to the ADT)</ul><li>The <span class=term>private</span> or <span class=term>internal</span> part, which consists of:<ul><br><li>the representation (how the structure is actually stored)<li>the implementation of the operations (the actual code)</ul></ol><p>In general, there are many possible operations that could be defined
for each ADT;
however, they often fall into these categories:<ol><li>initialize<li>add data<li>access data<li>remove data</ol><p>In this class, we will study a number of different abstract data types,
different ways to implement them, and different ways to use them.
Our first ADT (coming up in the next set of notes) is the <span class=adt>List</span>.</p><a class=readmore href=/2016/9/7/cs367-text-introduction.html title=CS367(Text)-Introduction>Read more</a></article></section><hr><ul class="default-wp-page clearfix"></ul></div></div><div id=secondary class="widget-area col-md-4" role=complementary><aside id=toolbox class=widget><h4 class=widget-title>About this site</h4><div class=author_bio_message>This is my personal notes website.
You are welcomed to use it.
But do NOT stop taking your own notes!</div></aside><aside id=recent-posts-2 class="widget widget_recent_entries"><h4 class=widget-title>Recent</h4><ul><li><a href=/2019/2/7/cs538-hw1.html>CS 538 Programming Assignment 1</a><li><a href=/2019/1/24/cs538-02.html>CS 538 Lecture 2</a><li><a href=/2019/1/22/cs538-01.html>CS 538 Lecture 1</a><li><a href=/2018/12/22/math541-notes.html>Math 541 Abstract Algebra Lecture Notes</a><li><a href=/2018/12/22/math541-homeworks.html>Math 541 Abstract Algebra Homeworks</a><li><a href=/2018/12/11/aa160-notes.html>Asian American 160 Lecture Notes</a><li><a href=/2018/9/27/aa160-06.html>Asian American 160 Lec 06</a><li><a href=/2018/9/25/aa160-05.html>Asian American 160 Lec 05</a><li><a href=/2018/9/20/aa160-04.html>Asian American 160 Lec 04</a><li><a href=/2018/9/18/aa160-03.html>Asian American 160 Lec 03</a></ul></aside><aside id=categories-2 class="widget widget_categories"><h4 class=widget-title>Categories</h4><ul><li class=cat-item><a href=/category/AA160.html>AA160</a>(7)<li class=cat-item><a href=/category/Artificial%20Intelligence.html>Artificial Intelligence</a>(1)<li class=cat-item><a href=/category/Bio101.html>Bio101</a>(5)<li class=cat-item><a href=/category/Biology.html>Biology</a>(5)<li class=cat-item><a href=/category/CS.html>CS</a>(4)<li class=cat-item><a href=/category/CS252.html>CS252</a>(4)<li class=cat-item><a href=/category/CS354.html>CS354</a>(13)<li class=cat-item><a href=/category/CS367.html>CS367</a>(23)<li class=cat-item><a href=/category/CS368.html>CS368</a>(1)<li class=cat-item><a href=/category/CS537.html>CS537</a>(2)<li class=cat-item><a href=/category/CS538.html>CS538</a>(2)<li class=cat-item><a href=/category/CS540.html>CS540</a>(24)<li class=cat-item><a href=/category/CS559.html>CS559</a>(5)<li class=cat-item><a href=/category/Computer%20Science.html>Computer Science</a>(90)<li class=cat-item><a href=/category/Deep%20Learning.html>Deep Learning</a>(1)<li class=cat-item><a href=/category/ESL118.html>ESL118</a>(2)<li class=cat-item><a href=/category/Econ102.html>Econ102</a>(2)<li class=cat-item><a href=/category/Economics.html>Economics</a>(2)<li class=cat-item><a href=/category/Ethnics.html>Ethnics</a>(7)<li class=cat-item><a href=/category/Go.html>Go</a>(1)<li class=cat-item><a href=/category/Kines100.html>Kines100</a>(3)<li class=cat-item><a href=/category/Kinesiology.html>Kinesiology</a>(3)<li class=cat-item><a href=/category/Math.html>Math</a>(31)<li class=cat-item><a href=/category/Math240.html>Math240</a>(20)<li class=cat-item><a href=/category/Math320.html>Math320</a>(2)<li class=cat-item><a href=/category/Math431.html>Math431</a>(2)<li class=cat-item><a href=/category/Math519.html>Math519</a>(3)<li class=cat-item><a href=/category/Math521.html>Math521</a>(2)<li class=cat-item><a href=/category/Math541.html>Math541</a>(2)<li class=cat-item><a href=/category/NS132.html>NS132</a>(30)<li class=cat-item><a href=/category/Nutritional%20Science.html>Nutritional Science</a>(30)<li class=cat-item><a href=/category/Psych202.html>Psych202</a>(3)<li class=cat-item><a href=/category/Psychology.html>Psychology</a>(3)<li class=cat-item><a href=/category/Test.html>Test</a>(1)<li class=cat-item><a href=/category/Website.html>Website</a>(1)</ul></aside><aside id=tag_cloud-3 class="widget widget_tag_cloud"><h4 class=widget-title>Tags</h4><div class=tagcloud><a href=/tags/AA160.html class="tag-link tag-link-position" style=font-size:8pt>AA160</a>
<a href=/tags/Bio101.html class="tag-link tag-link-position" style=font-size:8pt>Bio101</a>
<a href=/tags/CNNs.html class="tag-link tag-link-position" style=font-size:8pt>CNNs</a>
<a href=/tags/CS252.html class="tag-link tag-link-position" style=font-size:8pt>CS252</a>
<a href=/tags/CS354.html class="tag-link tag-link-position" style=font-size:8pt>CS354</a>
<a href=/tags/CS367.html class="tag-link tag-link-position" style=font-size:8pt>CS367</a>
<a href=/tags/CS368.html class="tag-link tag-link-position" style=font-size:8pt>CS368</a>
<a href=/tags/CS537.html class="tag-link tag-link-position" style=font-size:8pt>CS537</a>
<a href=/tags/CS538.html class="tag-link tag-link-position" style=font-size:8pt>CS538</a>
<a href=/tags/CS540.html class="tag-link tag-link-position" style=font-size:8pt>CS540</a>
<a href=/tags/CS559.html class="tag-link tag-link-position" style=font-size:8pt>CS559</a>
<a href=/tags/ESL118.html class="tag-link tag-link-position" style=font-size:8pt>ESL118</a>
<a href=/tags/Econ102.html class="tag-link tag-link-position" style=font-size:8pt>Econ102</a>
<a href=/tags/Kines100.html class="tag-link tag-link-position" style=font-size:8pt>Kines100</a>
<a href=/tags/Lumos.html class="tag-link tag-link-position" style=font-size:8pt>Lumos</a>
<a href=/tags/Math240.html class="tag-link tag-link-position" style=font-size:8pt>Math240</a>
<a href=/tags/Math320.html class="tag-link tag-link-position" style=font-size:8pt>Math320</a>
<a href=/tags/Math431.html class="tag-link tag-link-position" style=font-size:8pt>Math431</a>
<a href=/tags/Math519.html class="tag-link tag-link-position" style=font-size:8pt>Math519</a>
<a href=/tags/Math521.html class="tag-link tag-link-position" style=font-size:8pt>Math521</a>
<a href=/tags/Math541.html class="tag-link tag-link-position" style=font-size:8pt>Math541</a>
<a href=/tags/NS132.html class="tag-link tag-link-position" style=font-size:8pt>NS132</a>
<a href=/tags/Psych202.html class="tag-link tag-link-position" style=font-size:8pt>Psych202</a>
<a href=/tags/Test.html class="tag-link tag-link-position" style=font-size:8pt>Test</a></div></aside></div></div></div></div><footer id=colophon class=clearfix><div class=wrapper><div id=site-generator class=clearfix><div class=copyright><a href=https://creativecommons.org/licenses/by-nc-nd/4.0/>CC BY-NC-ND 4.0</a> |
<a href=/feed.xml>Feed</a> |
<a href=/sitemap.xml>Sitemap</a></div></div><div id=site-generator class=clearfix><div class=copyright>Copyright © 2016-2018 <a href=https://rongyi.blog/ title="L.E.R Wiz"><span>L.E.R Wiz</span></a> |
Powered by: <a href=/portus.html target=_blank title=Lumos><span>Portus Engine 1.3.7</span></a></div></div></div><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-81922310-2','auto');ga('send','pageview');</script><script async src=//www.google-analytics.com/analytics.js></script><script>_atrk_opts={atrk_acct:"JXtbq1Fx9f207i",domain:"rongyi.blog",dynamic:true};(function(){var as=document.createElement('script');as.type='text/javascript';as.async=true;as.src="https://certify-js.alexametrics.com/atrk.js";var s=document.getElementsByTagName('script')[0];s.parentNode.insertBefore(as,s);})();</script><noscript><img src="https://certify.alexametrics.com/atrk.gif?account=JXtbq1Fx9f207i" style=display:none height=1 width=1 alt></noscript><div class=back-to-top style=display:none><a href=#branding>Back to Top</a></div></footer><div class=search-tool style=position:fixed;top:0;bottom:0;left:0;right:0;opacity:.95;background-color:#111;z-index:9999;display:none><input class="form-control search-content" id=search-content style=position:fixed;top:60px placeholder="文章标题 分类 标签 (L.E.R Wiz 站内搜索)"><div style=position:fixed;top:16px;right:16px><img src=/search/img/close.png id=close-btn></div></div><link rel=stylesheet href=/search/css/search.css><script src=/search/js/bootstrap3-typeahead.min.js></script><script src=/search/js/search.js></script><script src=/js/backtotop.js></script><script src=/js/tinynav.js></script><script src=/js/jquery.cycle.all.min.js></script><script>var cleanretina_slider_value={"transition_effect":"fade","transition_delay":"4000","transition_duration":"1000"};</script><script src=/js/cleanretina-slider-setting.js></script><script src=/js/jquery.fancybox-1.3.4.pack.js></script><script src=/js/cleanretina-custom-fancybox-script.js></script><script src=/js/highlight.pack.js></script><script src=/js/copynotice.js></script><script>hljs.initHighlightingOnLoad();</script>